{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Study the transformer code provided in the [module](https://github.com/DrUzair/NLP/blob/master/Transformer/5%20LM_TransformerBlock_MLP_PosEmb_AddNormOpt.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "''' Look at all previous tokens to generate next\n",
    "    @Author: Uzair Ahmad\n",
    "    2022\n",
    "    +TransformerBlock \n",
    "'''\n",
    "\n",
    "\n",
    "class TransformerBlockLM(nn.Module):\n",
    "    class TransformerBlock(nn.Module):\n",
    "        def __init__(self, head_count, in_size, out_size,device=\"cuda\"):\n",
    "            super().__init__()\n",
    "            self.comm = TransformerBlockLM.MultiHeadAttention(head_count=head_count,\n",
    "                                                              in_size=in_size,\n",
    "                                                              out_size=out_size).to(device)\n",
    "            self.think = TransformerBlockLM.MLP(embed_size=out_size).to(device)\n",
    "\n",
    "        def forward(self, x):\n",
    "            return x + self.think(x + self.comm(x))\n",
    "\n",
    "    class MLP(nn.Module):\n",
    "        # FFNN (embed_size, embed_size*4, embed_size)\n",
    "        def __init__(self, embed_size,device=\"cuda\"):\n",
    "            super().__init__()\n",
    "            self.mlp = nn.Sequential(nn.Linear(embed_size, embed_size * 4).to(device),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(embed_size * 4, embed_size).to(device))\n",
    "            self.layerNorm = nn.LayerNorm(embed_size).to(device)\n",
    "\n",
    "        def forward(self, x):  # think\n",
    "            return self.layerNorm(self.mlp(x))  # paper - after\n",
    "            # return self.mlp(self.layerNorm(x)) # alternate - before\n",
    "\n",
    "    class MultiHeadAttention(nn.Module):\n",
    "        \"\"\"\n",
    "        multiple parallel SA heads (communication among words)\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, head_count, in_size, out_size,device=\"cuda\"):\n",
    "            super().__init__()\n",
    "            self.heads = nn.ModuleList(\n",
    "                TransformerBlockLM.SelfAttentionHead(in_size, out_size // head_count).to(device)\n",
    "                for _ in range(head_count)\n",
    "            )\n",
    "            self.layerNorm = nn.LayerNorm(out_size).to(device)\n",
    "            # self.proj = nn.Linear(out_size, out_size)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # concat over channel/embeddings_size dimension\n",
    "            return self.layerNorm(torch.cat([head(x) for head in self.heads], dim=-1))  # paper - after\n",
    "            # return torch.cat([head(self.layerNorm(x)) for head in self.heads], dim=-1) # alternate - before\n",
    "            # return self.proj(torch.cat([head(x) for head in self.heads], dim=-1))\n",
    "\n",
    "    class SelfAttentionHead(nn.Module):\n",
    "        def __init__(self, in_size, out_size,device=\"cuda\"):\n",
    "            \"\"\"\n",
    "            in_size is embed_size\n",
    "            out_size is head_size\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "            self.head_size = out_size\n",
    "            self.K = nn.Linear(in_size, self.head_size, bias=False).to(device)\n",
    "            self.Q = nn.Linear(in_size, self.head_size, bias=False).to(device)\n",
    "            self.V = nn.Linear(in_size, self.head_size, bias=False).to(device)\n",
    "\n",
    "        def forward(self, x):\n",
    "            keys = self.K(x)\n",
    "            queries = self.Q(x)\n",
    "            # affinities :\n",
    "            # all the queries will dot-product with all the keys\n",
    "            # transpose (swap) second dimension (input_length) with third (head_size)\n",
    "            keys_t = keys.transpose(1, 2)\n",
    "            autocorrs = (queries @ keys_t) * (self.head_size ** -0.5)  # (batch_size x input_length x input_length)\n",
    "            '''\n",
    "            (batch_size x input_length x embed_size) @ (batch_size x embed_size x input_length) ----> (batch_size x input_length x input_length)\n",
    "            '''\n",
    "            autocorrs = torch.tril(autocorrs)\n",
    "            autocorrs = autocorrs.masked_fill(autocorrs == 0, float('-inf'))\n",
    "            autocorrs = torch.softmax(autocorrs, dim=-1)\n",
    "            values = self.V(x)  # (batch_size x input_length x head_size)\n",
    "            out = autocorrs @ values\n",
    "            return out\n",
    "\n",
    "    def __init__(self, batch_size=4,\n",
    "                 input_length=8,\n",
    "                 embed_size=16,\n",
    "                 sa_head_size=8,\n",
    "                 sa_multihead_count=4,\n",
    "                 pos_embed=False,\n",
    "                 include_mlp=False):\n",
    "        super().__init__()\n",
    "        self.blocks = None\n",
    "        self.ffn = None\n",
    "        self.sa_heads = None\n",
    "        # sa_head_size head_size of self-attention module\n",
    "        self.sa_head_size = sa_head_size\n",
    "        self.sa_multihead_count = sa_multihead_count\n",
    "\n",
    "        self.val_data = None\n",
    "        self.train_data = None\n",
    "        self.val_text = None\n",
    "        self.train_text = None\n",
    "        self.K = None\n",
    "        self.linear_sahead_to_vocab = None\n",
    "        self.vocab = None\n",
    "        self.token_embeddings_table = None\n",
    "        self.vocab_size = None\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.vocab_size: int\n",
    "        self.is_pos_emb = pos_embed\n",
    "        self.include_mlp = include_mlp\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        # input_length = how many consecutive tokens/chars in one input\n",
    "        self.input_length = input_length\n",
    "        # batch_size = how many inputs are going to be processed in-parallel (on GPU)\n",
    "        self.batch_size = batch_size\n",
    "        # embed_size = embedding size\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        self.lm_head = None\n",
    "        self.position_embeddings_table = None\n",
    "\n",
    "    def forward(self, in_ids, target=None):\n",
    "        # print(\"Hello\")\n",
    "        # print(in_ids.shape)\n",
    "        # print(self.token_embeddings_table)\n",
    "        in_ids_emb = self.token_embeddings_table(in_ids[:, -self.input_length:])\n",
    "        # print(\"Hello\")\n",
    "        if self.is_pos_emb:\n",
    "            in_ids_pos_emb = self.position_embeddings_table(\n",
    "                torch.arange(in_ids[:, -self.input_length:].shape[1], device=self.device)\n",
    "            )\n",
    "            in_ids_emb = in_ids_emb + in_ids_pos_emb\n",
    "        block_outputs = self.blocks((in_ids_emb))\n",
    "        logits = self.linear_sahead_to_vocab(block_outputs).to(self.device)  # compute\n",
    "        if target is None:\n",
    "            ce_loss = None\n",
    "        else:\n",
    "            batch_size, input_length, vocab_size = logits.shape\n",
    "            logits_ = logits.view(batch_size * input_length, vocab_size)\n",
    "            targets = target.view(batch_size * input_length)\n",
    "            ce_loss = F.cross_entropy(logits_, targets)\n",
    "        return logits, ce_loss\n",
    "\n",
    "    def fit(self, train_iters=100, eval_iters=10, lr=0.0001):\n",
    "        \"\"\"\n",
    "        train_iters = how many training iterations\n",
    "        eval_iters = how many batches to evaluate to get average performance\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        for iteration in tqdm(range(train_iters)):\n",
    "            if iteration % eval_iters == 0:\n",
    "                avg_loss = self.eval_loss(eval_iters)\n",
    "                print(f\"iter {iteration}: train {avg_loss['train']} val {avg_loss['eval']}\")\n",
    "            inputs, targets = self.get_batch(split='train')\n",
    "            _, ce_loss = self(inputs, targets)\n",
    "            optimizer.zero_grad(set_to_none=True)  # clear gradients of previous step\n",
    "            ce_loss.backward()  # propagate loss back to each unit in the network\n",
    "            optimizer.step()  # update network parameters w.r.t the loss\n",
    "        # torch.save(self, 'sa_pos_')\n",
    "\n",
    "    def generate(self, context_token_ids, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            token_rep, _ = self(context_token_ids)\n",
    "            last_token_rep = token_rep[:, -1, :]\n",
    "            probs = F.softmax(last_token_rep, dim=1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            context_token_ids = torch.cat((context_token_ids, next_token), dim=1)\n",
    "        output_text = self.decoder(context_token_ids[0].tolist())\n",
    "        return output_text\n",
    "\n",
    "    @torch.no_grad()  # tell torch not to prepare for back-propagation (context manager)\n",
    "    def eval_loss(self, eval_iters):\n",
    "        perf = {}\n",
    "        # set dropout and batch normalization layers to evaluation mode before running inference.\n",
    "        self.eval()\n",
    "        for split in ['train', 'eval']:\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "                tokens, targets = self.get_batch(split)  # get random batch of inputs and targete\n",
    "                _, ce_loss = self(tokens, targets)  # forward pass\n",
    "                losses[k] = ce_loss.item()  # the value of loss tensor as a standard Python number\n",
    "            perf[split] = losses.mean()\n",
    "        self.train()  # turn-on training mode-\n",
    "        return perf\n",
    "\n",
    "    def prep(self, corpus):\n",
    "        self.vocab = sorted(list(set(corpus)))\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        c2i = {c: i for i, c in\n",
    "               enumerate(self.vocab)}  # char c to integer i map. assign value i for every word in vocab\n",
    "        i2c = {i: c for c, i in c2i.items()}  # integer i to char c map\n",
    "\n",
    "        self.encoder = lambda doc: [c2i[c] for c in doc]\n",
    "        self.decoder = lambda nums: ''.join([i2c[i] for i in nums])\n",
    "        text=corpus\n",
    "        n = len(text)\n",
    "        self.train_text = text[:int(n * 0.9)]\n",
    "        self.val_text = text[int(n * 0.9):]\n",
    "\n",
    "        self.train_data = torch.tensor(self.encoder(self.train_text), dtype=torch.long)\n",
    "        self.val_data = torch.tensor(self.encoder(self.val_text), dtype=torch.long)\n",
    "\n",
    "        # look-up table for embeddings (vocab_size x embed_size)\n",
    "        # it will be mapping each token id to a vector of embed_size\n",
    "        # a wrapper to store vector representations of each token\n",
    "        self.token_embeddings_table = \\\n",
    "            nn.Embedding(self.vocab_size, self.embed_size).to(self.device)\n",
    "\n",
    "        if self.is_pos_emb:\n",
    "            self.position_embeddings_table = nn.Embedding(self.input_length, self.embed_size).to(self.device)\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size),\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size),\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size),\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size),\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size),\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size),\n",
    "        )\n",
    "        # linear projection of sa_head output to vocabulary\n",
    "        self.linear_sahead_to_vocab = nn.Linear(self.sa_head_size, self.vocab_size).to(self.device)\n",
    "        return c2i\n",
    "    def get_batch(self, split='train'):\n",
    "        data = self.train_data if split == 'train' else self.val_data\n",
    "        # get random chunks of length batch_size from data\n",
    "        ix = torch.randint(len(data) - self.input_length,\n",
    "                           (self.batch_size,))\n",
    "        inputs_batch = torch.stack([data[i:i + self.input_length] for i in ix])\n",
    "        targets_batch = torch.stack([data[i + 1:i + self.input_length + 1] for i in ix])\n",
    "        inputs_batch = inputs_batch.to(self.device)\n",
    "        targets_batch = targets_batch.to(self.device)\n",
    "        # inputs_batch is\n",
    "        return inputs_batch, targets_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params 1112398\n"
     ]
    }
   ],
   "source": [
    "with open('emily_dickonson.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# text = 'a quick brown fox jumps over the lazy dog.\\n ' \\\n",
    "#        'lazy dog and a quick brown fox.\\n' \\\n",
    "#        'the dog is lazy and the fox jumps quickly.\\n' \\\n",
    "#        'a fox jumps over the dog because he is lazy.\\n' \\\n",
    "#        'dog is lazy and fox is brown. she quickly jumps over the lazy dog.'\n",
    "\n",
    "model = TransformerBlockLM(batch_size=64,\n",
    "                           input_length=32,\n",
    "                           embed_size=128,\n",
    "                           sa_multihead_count=8,\n",
    "                           sa_head_size=128,\n",
    "                           pos_embed=True,\n",
    "                           include_mlp=True)\n",
    "model = model.to(model.device)\n",
    "# model=\"cpu\"\n",
    "model.prep(text)\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "print(f'params {sum([np.prod(p.size()) for p in model_parameters])}')\n",
    "# input_batch, output_batch = model.get_batch(split='train')\n",
    "# _, _ = model(input_batch, output_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/4000 [00:23<26:16:58, 23.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: train 5.754003524780273 val 5.807081699371338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1001/4000 [01:53<3:19:43,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1000: train 1.663784146308899 val 1.7214789390563965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2001/4000 [03:22<2:00:21,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2000: train 1.3952398300170898 val 1.7330751419067383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3001/4000 [04:46<51:54,  3.12s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3000: train 1.1602956056594849 val 1.8472594022750854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [05:47<00:00, 11.51it/s]\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_iters=4000, eval_iters=1000, lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "PUTUL.\n",
      "\n",
      "The gave of overges\n",
      "   I tell hem the hills —\n",
      "The istants 'd leave the resent the size\n",
      "As but one, believed for thought.\n",
      "\n",
      "Next not affoded pope; apart of paraise.\n",
      "Sincle placiled as the nest haids,\n",
      "We cribsence rarered them in reached out open, relieves in low,\n",
      "And wend well to knowl, that well.\n",
      "\n",
      "Mage wandering back nought, it real, —\n",
      "To denied it, it makes all,\n",
      "And the bodiless slipped,\n",
      "Sill were suspected landscape\n",
      "     For the birds stars to dails upon man,\n",
      "\n",
      "Speers surpasse,\n",
      "And vented, and dewle a thing\n",
      "Capated etern-ustance plush\n",
      "I could not exist it will come a well!\n",
      "  The woods are plain.\n",
      "\n",
      "XV.\n",
      "\n",
      "THE LOST JOUTY.\n",
      "\n",
      "I'll sound at opposite,\n",
      "This, the midnights all well,\n",
      "   We knowing it is low mat\n",
      "Is rambles on the crose at warm.\n",
      "\n",
      "My second angels, we mornings blossom into report;\n",
      "A ridd the dew:\n",
      "If were in the dare run;\n",
      "When closer twill we are known\n",
      "And then the brakes 't is, the maids?\n",
      "Begeads we doubt, buy night\n",
      "    For heaven everywhere.\n",
      "\n",
      "To see if the wall do,\n",
      "Spinsibly\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(context_token_ids=torch.zeros((1, 1),dtype=torch.long,device=model.device),max_new_tokens=1000)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.A) Identify the points where code is different from the proposed architecture in Google's patent (provided in the module) (5 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Position Embedding**: The code implements Learned Position Embedding, while the Google patent utilizes Sinusoidal Position Embedding, demonstrating superior position understanding.\n",
    "\n",
    "* **Architecture Composition**: The code comprises a variation of the decoder component, diverging from the proposed architecture in the Google patent, which includes both encoder and decoder sections.\n",
    "\n",
    "* **Multi-Head Attention Variations**: While the Google patent introduces three types of Multi-Head Attention (Multi-Head Attention, Encoder-Decoder Multi-Head Attention, and Masked Multi-Head Attention), the code solely employs Masked Multi-Head Attention.\n",
    "\n",
    "* **Decoder Layer Modifications**: In the proposed model, the decoder layer incorporates Masked Multi-Head Attention, Encoder-Decoder Multi-Head Attention, Layer Normalization, Multi-layer Perceptron layer, and skip connections. Conversely, the code implements Masked Multi-Head Attention, Layer Normalization, Multi-layer Perceptron layer, and skip connections, with variations from the original patent architecture.\n",
    "\n",
    "* **Padding**: In the proposed model by google, they use padding whereas oin the given code there is no padding of input tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.B) Re-design the code to make it more intuitive. Give arguments why do you think your code is better. (5 points)\n",
    "#### e.g. Find a bug and fix it, OR restructure the classes, OR read the parameters from a config file OR ask chatgpt OR add visualizations of key, query, value OR any other idea you deem necessary to improve the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-designing the Code for Improved Intuitiveness\n",
    "\n",
    "#### My Arguments:\n",
    "\n",
    "1. **Position Embedding**:  \n",
    "   *Learned Position Embeddings vs. Sinusoidal Position Embeddings*  \n",
    "   - *Learned Position Embeddings*: These embeddings are learned during training, offering flexibility to adapt to specific sequence patterns.\n",
    "   - *Sinusoidal Position Embeddings*: Provide fixed embeddings based on sine and cosine functions, offering simplicity but lack adaptability.\n",
    "\n",
    "2. **Bugs**:  \n",
    "   - Some parts of the code lack compatibility with GPU settings, hindering performance and efficiency.\n",
    "   - Incorrect usage of the number of heads and head dimension leads to incorrect multi-head attention implementation.\n",
    "\n",
    "3. **Code Structure**:  \n",
    "   - The current code structure does not adhere to industry standards, making it challenging to maintain and scale.\n",
    "\n",
    "4. **Data Preparation**:  \n",
    "   - The variable named \"text\" in data preparation should be renamed to \"corpus\" for clarity and consistency.\n",
    "\n",
    "5. **Model Architecture**:  \n",
    "   - The Self Attention module only performs Masked Self Attention, lacking the flexibility to switch between Masked and Non-Masked Self Attention.\n",
    "\n",
    "#### Proposed Improvements:\n",
    "1. **Bug Fixes**: Address compatibility issues with GPU settings and correct calculations for multi-head attention dimensions.\n",
    "2. **Code Refactoring**: Restructure the code following industry best practices for improved readability and maintainability.\n",
    "3. **Parameterization**: Utilize configuration files to manage hyperparameters, enhancing flexibility and ease of experimentation.\n",
    "4. **Comprehensive Attention Mechanisms**: Implement both Masked and Non-Masked Self Attention options to enhance model versatility.\n",
    "5. **Documentation and Comments**: Enhance code documentation and add explanatory comments to improve understanding and collaboration.\n",
    "\n",
    "I have addressed these issues and redesigned the architecture in my version of the model named \"model_venky\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is a rough architecture used in the Google Patent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "        self.transformer = nn.Transformer(d_model, nhead=num_heads, num_encoder_layers=num_layers, num_decoder_layers=num_layers, dim_feedforward=d_ff, dropout=dropout)\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "        output = self.transformer(src_embedded, tgt_embedded)\n",
    "        output = self.fc(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I redesigned the given code to encorporate both Encoder and Decoder architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlockLM(nn.Module):\n",
    "    class TransformerBlock(nn.Module):\n",
    "        def __init__(self, head_count, in_size):\n",
    "            super().__init__()\n",
    "            self.encoder_bl= nn.Sequential(*[TransformerBlockLM.encoder_block(head_count,in_size) for _ in range(6)])\n",
    "            self.decoder_bl= nn.ModuleList([TransformerBlockLM.decoder_block(head_count,in_size) for _ in range(6)])\n",
    "\n",
    "        def forward(self, x,target):\n",
    "            encoder_output=self.encoder_bl(x)\n",
    "            for decoder in self.decoder_bl:\n",
    "                output_temp=decoder(target,encoder_output)\n",
    "                target=output_temp\n",
    "            # print(output_temp.shape)\n",
    "            return output_temp\n",
    "\n",
    "    class MLP(nn.Module):\n",
    "        # FFNN (embed_size, embed_size*4, embed_size)\n",
    "        def __init__(self, embed_size):\n",
    "            super().__init__()\n",
    "            self.mlp = nn.Sequential(nn.Linear(embed_size, embed_size * 4),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(embed_size * 4, embed_size))\n",
    "            self.layerNorm = nn.LayerNorm(embed_size)\n",
    "\n",
    "        def forward(self, x):  # think\n",
    "            return self.layerNorm(self.mlp(x))  # paper - after\n",
    "            # return self.mlp(self.layerNorm(x)) # alternate - before\n",
    "\n",
    "    class MultiHeadAttention(nn.Module):\n",
    "        \"\"\"\n",
    "        multiple parallel SA heads (communication among words)\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, head_count, in_size,mask=False):\n",
    "            super().__init__()\n",
    "            self.heads = nn.ModuleList(\n",
    "                TransformerBlockLM.SelfAttentionHead(in_size, in_size // head_count,mask)\n",
    "                for _ in range(head_count)\n",
    "            )\n",
    "            self.layerNorm = nn.LayerNorm(in_size)\n",
    "            # self.proj = nn.Linear(out_size, out_size)\n",
    "\n",
    "        def forward(self, q,k,v):\n",
    "            # concat over channel/embeddings_size dimension\n",
    "            return self.layerNorm(torch.cat([head(q,k,v) for head in self.heads], dim=-1))  # paper - after\n",
    "            # return torch.cat([head(self.layerNorm(x)) for head in self.heads], dim=-1) # alternate - before\n",
    "            # return self.proj(torch.cat([head(x) for head in self.heads], dim=-1))\n",
    "\n",
    "    class SelfAttentionHead(nn.Module):\n",
    "        def __init__(self, in_size, head_size,mask=False):\n",
    "            \"\"\"\n",
    "            in_size is embed_size\n",
    "            out_size is head_size\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "            self.head_size = head_size\n",
    "            self.K = nn.Linear(in_size, self.head_size, bias=False)\n",
    "            self.Q = nn.Linear(in_size, self.head_size, bias=False)\n",
    "            self.V = nn.Linear(in_size, self.head_size, bias=False)\n",
    "            self.mask=mask\n",
    "\n",
    "        def forward(self, q,k,v):\n",
    "            keys = self.K(k)\n",
    "            queries = self.Q(q)\n",
    "            # affinities :\n",
    "            # all the queries will dot-product with all the keys\n",
    "            # transpose (swap) second dimension (input_length) with third (head_size)\n",
    "            keys_t = keys.transpose(1, 2)\n",
    "            autocorrs = (queries @ keys_t) * (self.head_size ** -0.5)  # (batch_size x input_length x input_length)\n",
    "            '''\n",
    "            (batch_size x input_length x embed_size) @ (batch_size x embed_size x input_length) ----> (batch_size x input_length x input_length)\n",
    "            '''\n",
    "            if self.mask==True:\n",
    "                autocorrs = torch.tril(autocorrs)\n",
    "                autocorrs = autocorrs.masked_fill(autocorrs == 0, float('-inf'))\n",
    "            autocorrs = torch.softmax(autocorrs, dim=-1)\n",
    "            values = self.V(v)  # (batch_size x input_length x head_size)\n",
    "            out = autocorrs @ values\n",
    "            return out\n",
    "    \n",
    "    class encoder_block(nn.Module):\n",
    "        def __init__(self,head_count, in_size):\n",
    "            super().__init__()\n",
    "            embd_size=in_size\n",
    "            self.mha=TransformerBlockLM.MultiHeadAttention(head_count, in_size)\n",
    "            self.mlp=TransformerBlockLM.MLP(embd_size)\n",
    "        def forward(self,input):\n",
    "            x1= self.mha(input,input,input)+input\n",
    "            x2= self.mlp(x1)+x1\n",
    "            return x2\n",
    "            \n",
    "    class decoder_block(nn.Module):\n",
    "        def __init__(self,head_count, in_size):\n",
    "            super().__init__()\n",
    "            self.mask_mha=TransformerBlockLM.MultiHeadAttention(head_count, in_size,mask=True)\n",
    "            self.mha=TransformerBlockLM.MultiHeadAttention(head_count, in_size,mask=False)\n",
    "            self.mlp=TransformerBlockLM.MLP(in_size)\n",
    "        def forward(self,target,enc_output):\n",
    "            x1= self.mha(target, target,target)+target\n",
    "            x2= self.mha(x1, enc_output,enc_output)+x1\n",
    "            x3= self.mlp(x2)+x2\n",
    "            return x3\n",
    "    \n",
    "    def __init__(self, batch_size=4,\n",
    "                 input_length=8,\n",
    "                 embed_size=16,\n",
    "                 sa_multihead_count=4,\n",
    "                 num_encoders=6,\n",
    "                 num_decoders=6,\n",
    "                 pos_embed=False,\n",
    "                 include_mlp=False):\n",
    "    \n",
    "        super().__init__()\n",
    "        self.blocks = None\n",
    "        self.ffn = None\n",
    "        self.sa_heads = None\n",
    "        # sa_head_size head_size of self-attention module\n",
    "        # self.sa_head_size = sa_head_size\n",
    "        self.sa_multihead_count = sa_multihead_count\n",
    "\n",
    "        self.val_data = None\n",
    "        self.train_data = None\n",
    "        self.val_text = None\n",
    "        self.train_text = None\n",
    "        self.K = None\n",
    "        self.linear_sahead_to_vocab = None\n",
    "        self.vocab = None\n",
    "        self.token_embeddings_table = None\n",
    "        self.vocab_size = None\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.vocab_size: int\n",
    "        self.is_pos_emb = pos_embed\n",
    "        self.include_mlp = include_mlp\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        # input_length = how many consecutive tokens/chars in one input\n",
    "        self.input_length = input_length\n",
    "        # batch_size = how many inputs are going to be processed in-parallel (on GPU)\n",
    "        self.batch_size = batch_size\n",
    "        # embed_size = embedding size\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        self.lm_head = None\n",
    "        self.position_embeddings_table = None\n",
    "        self.num_encoders=num_encoders\n",
    "        self.num_decoders=num_decoders\n",
    "        self.Transformer=TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,in_size=self.embed_size)\n",
    "        \n",
    "\n",
    "    def forward(self, in_ids, target=None):\n",
    "        in_ids_emb = self.token_embeddings_table(in_ids[:, -self.input_length:])\n",
    "        if self.is_pos_emb:\n",
    "            in_ids_emb=self.positional_encoding(self.encoder_embedding(in_ids_emb))\n",
    "        if target is None:\n",
    "            decoder_output=self.Transformer(in_ids_emb,target)\n",
    "        else:\n",
    "            target_emb = self.token_embeddings_table(target)\n",
    "            target_emb=self.positional_encoding(self.encoder_embedding(target_emb))\n",
    "            decoder_output=self.Transformer(in_ids_emb,target_emb)\n",
    "        logits = self.linear_vocab(decoder_output)  # compute\n",
    "\n",
    "        if target is None:\n",
    "            ce_loss = None\n",
    "        else:\n",
    "            batch_size, input_length, vocab_size = logits.shape\n",
    "            logits_ = logits.view(batch_size * input_length, vocab_size)            \n",
    "            targets = target.view(batch_size * input_length)\n",
    "            ce_loss = F.cross_entropy(logits_, targets)\n",
    "        return logits, ce_loss\n",
    "\n",
    "    def fit(self, train_iters=100, eval_iters=10, lr=0.0001):\n",
    "        \"\"\"\n",
    "        train_iters = how many training iterations\n",
    "        eval_iters = how many batches to evaluate to get average performance\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        for iteration in range(train_iters):\n",
    "            # print(iteration)\n",
    "            if iteration % eval_iters == 0:\n",
    "                avg_loss = self.eval_loss(eval_iters)\n",
    "                print(f\"iter {iteration}: train loss: {avg_loss['train']} val loss: {avg_loss['eval']}\")\n",
    "            inputs, targets = self.get_batch(split='train')\n",
    "            _, ce_loss = self(inputs, targets)\n",
    "            optimizer.zero_grad(set_to_none=True)  # clear gradients of previous step\n",
    "            ce_loss.backward()  # propagate loss back to each unit in the network\n",
    "            optimizer.step()  # update network parameters w.r.t the loss\n",
    "        # torch.save(self, 'sa_pos_')\n",
    "\n",
    "    def generate(self, context_token_ids, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            token_rep, _ = self(context_token_ids)\n",
    "            last_token_rep = token_rep[:, -1, :]\n",
    "            probs = F.softmax(last_token_rep, dim=1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            context_token_ids = torch.cat((context_token_ids, next_token), dim=1)\n",
    "        output_text = self.decoder(context_token_ids[0].tolist())\n",
    "        return output_text\n",
    "\n",
    "    @torch.no_grad()  # tell torch not to prepare for back-propagation (context manager)\n",
    "    def eval_loss(self, eval_iters):\n",
    "        perf = {}\n",
    "        # set dropout and batch normalization layers to evaluation mode before running inference.\n",
    "        self.eval()\n",
    "        for split in ['train', 'eval']:\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "                tokens, targets = self.get_batch(split)  # get random batch of inputs and targete\n",
    "                _, ce_loss = self(tokens, targets)  # forward pass\n",
    "                losses[k] = ce_loss.item()  # the value of loss tensor as a standard Python number\n",
    "            perf[split] = losses.mean()\n",
    "        self.train()  # turn-on training mode-\n",
    "        return perf\n",
    "\n",
    "    def prep(self, corpus):\n",
    "        self.vocab = sorted(list(set(corpus)))\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        c2i = {c: i for i, c in\n",
    "               enumerate(self.vocab)}  # char c to integer i map. assign value i for every word in vocab\n",
    "        i2c = {i: c for c, i in c2i.items()}  # integer i to char c map\n",
    "\n",
    "        self.encoder = lambda doc: [c2i[c] for c in doc]\n",
    "        self.decoder = lambda nums: ''.join([i2c[i] for i in nums])\n",
    "        n = len(corpus)\n",
    "        # print(corpus)\n",
    "        self.train_text = corpus[:int(n * 0.9)]\n",
    "        self.val_text = corpus[int(n * 0.9):]\n",
    "\n",
    "        self.train_data = torch.tensor(self.encoder(self.train_text), dtype=torch.long)\n",
    "        self.val_data = torch.tensor(self.encoder(self.val_text), dtype=torch.long)\n",
    "\n",
    "        # look-up table for embeddings (vocab_size x embed_size)\n",
    "        # it will be mapping each token id to a vector of embed_size\n",
    "        # a wrapper to store vector representations of each token\n",
    "        self.token_embeddings_table = \\\n",
    "            nn.Embedding(self.vocab_size, self.embed_size)\n",
    "\n",
    "        if self.is_pos_emb:\n",
    "            # self.position_embeddings_table = PositionalEncoding(self.embed_size, self.input_length)\n",
    "            self.position_embeddings_table = PositionalEncoding(self.embed_size, self.input_length)\n",
    "        \n",
    "        # linear projection of sa_head output to vocabulary\n",
    "        self.linear_vocab = nn.Linear(self.embed_size, self.vocab_size)\n",
    "\n",
    "    def get_batch(self, split='train'):\n",
    "        data = self.train_data if split == 'train' else self.val_data\n",
    "        # get random chunks of length batch_size from data\n",
    "        ix = torch.randint(len(data) - self.input_length,(self.batch_size,))\n",
    "        inputs_batch = torch.stack([data[i:i + self.input_length] for i in ix])\n",
    "        targets_batch = torch.stack([data[i + 1:i + self.input_length + 1] for i in ix])\n",
    "        inputs_batch = inputs_batch.to(self.device)\n",
    "        targets_batch = targets_batch.to(self.device)\n",
    "        # inputs_batch is\n",
    "        return inputs_batch, targets_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerBlockLM(\n",
      "  (Transformer): TransformerBlock(\n",
      "    (encoder_bl): Sequential(\n",
      "      (0): encoder_block(\n",
      "        (mha): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0-3): 4 x SelfAttentionHead(\n",
      "              (K): Linear(in_features=16, out_features=4, bias=False)\n",
      "              (Q): Linear(in_features=16, out_features=4, bias=False)\n",
      "              (V): Linear(in_features=16, out_features=4, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (layerNorm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (mlp): Sequential(\n",
      "            (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=64, out_features=16, bias=True)\n",
      "          )\n",
      "          (layerNorm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (1): encoder_block(\n",
      "        (mha): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0-3): 4 x SelfAttentionHead(\n",
      "              (K): Linear(in_features=16, out_features=4, bias=False)\n",
      "              (Q): Linear(in_features=16, out_features=4, bias=False)\n",
      "              (V): Linear(in_features=16, out_features=4, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (layerNorm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (mlp): Sequential(\n",
      "            (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=64, out_features=16, bias=True)\n",
      "          )\n",
      "          (layerNorm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (2): encoder_block(\n",
      "        (mha): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0-3): 4 x SelfAttentionHead(\n",
      "              (K): Linear(in_features=16, out_features=4, bias=False)\n",
      "              (Q): Linear(in_features=16, out_features=4, bias=False)\n",
      "              (V): Linear(in_features=16, out_features=4, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (layerNorm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (mlp): Sequential(\n",
      "            (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=64, out_features=16, bias=True)\n",
      "          )\n",
      "          (layerNorm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (3): encoder_block(\n",
      "        (mha): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0-3): 4 x SelfAttentionHead(\n",
      "              (K): Linear(in_features=16, out_features=4, bias=False)\n",
      "              (Q): Linear(in_features=16, out_features=4, bias=False)\n",
      "              (V): Linear(in_features=16, out_features=4, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (layerNorm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (mlp): Sequential(\n",
      "            (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=64, out_features=16, bias=True)\n",
      "          )\n",
      "          (layerNorm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (4): encoder_block(\n",
      "        (mha): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0-3): 4 x SelfAttentionHead(\n",
      "              (K): Linear(in_features=16, out_features=4, bias=False)\n",
      "              (Q): Linear(in_features=16, out_features=4, bias=False)\n",
      "              (V): Linear(in_features=16, out_features=4, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (layerNorm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (mlp): Sequential(\n",
      "            (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=64, out_features=16, bias=True)\n",
      "          )\n",
      "          (layerNorm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (5): encoder_block(\n",
      "        (mha): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0-3): 4 x SelfAttentionHead(\n",
      "              (K): Linear(in_features=16, out_features=4, bias=False)\n",
      "              (Q): Linear(in_features=16, out_features=4, bias=False)\n",
      "              (V): Linear(in_features=16, out_features=4, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (layerNorm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (mlp): Sequential(\n",
      "            (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=64, out_features=16, bias=True)\n",
      "          )\n",
      "          (layerNorm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (decoder_bl): ModuleList(\n",
      "      (0-5): 6 x decoder_block(\n",
      "        (mask_mha): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0-3): 4 x SelfAttentionHead(\n",
      "              (K): Linear(in_features=16, out_features=4, bias=False)\n",
      "              (Q): Linear(in_features=16, out_features=4, bias=False)\n",
      "              (V): Linear(in_features=16, out_features=4, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (layerNorm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (mha): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0-3): 4 x SelfAttentionHead(\n",
      "              (K): Linear(in_features=16, out_features=4, bias=False)\n",
      "              (Q): Linear(in_features=16, out_features=4, bias=False)\n",
      "              (V): Linear(in_features=16, out_features=4, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (layerNorm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (mlp): Sequential(\n",
      "            (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=64, out_features=16, bias=True)\n",
      "          )\n",
      "          (layerNorm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_enc_dec=TransformerBlockLM(batch_size=4,input_length=8,embed_size=16,sa_multihead_count=4,num_encoders=6,num_decoders=6,pos_embed=True,include_mlp=True)\n",
    "print(model_enc_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: train loss: 6.543510437011719 val loss: 6.64241886138916\n",
      "iter 10: train loss: 2.300812244415283 val loss: 2.2039215564727783\n",
      "iter 20: train loss: 0.3769061863422394 val loss: 0.3353438973426819\n",
      "iter 30: train loss: 0.10858909785747528 val loss: 0.08668842166662216\n",
      "iter 40: train loss: 0.02294185198843479 val loss: 0.02188847027719021\n",
      "iter 50: train loss: 0.010622554458677769 val loss: 0.010144343599677086\n",
      "iter 60: train loss: 0.0023777219466865063 val loss: 0.0025417529977858067\n",
      "iter 70: train loss: 0.0029227244667708874 val loss: 0.003797037061303854\n",
      "iter 80: train loss: 0.0007192973280325532 val loss: 0.0021690037101507187\n",
      "iter 90: train loss: 0.0022298030089586973 val loss: 0.0020070835016667843\n"
     ]
    }
   ],
   "source": [
    "model_enc_dec.fit(train_iters=100, eval_iters=10, lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Apply Transformer code provided in the module to train a language models that generates financial discourse in Warren Buffet's style. Train you model using Warren Buffets's Annual Letters to shareholders "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.A) Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/desai.ven/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/desai.ven/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/desai.ven/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import warnings\n",
    "# import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53837"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Cleaning\n",
    "\n",
    "file_path = 'WarrenBuffet.txt'\n",
    "with open(file_path, 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "english_stopwords = stopwords.words('english')\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text_data):\n",
    "    text_lower = text_data.lower()\n",
    "    text_link_removed = re.sub(r'http\\S+', '', text_lower)\n",
    "    text_punc_removed = re.sub(r'[^\\w\\s]', '', text_link_removed)\n",
    "    text_tokenized = word_tokenize(text_punc_removed)\n",
    "    # text_cleaned = [word for word in text_tokenized if word not in english_stopwords]\n",
    "    text_cleaned = [ps.stem(word) for word in text_tokenized]\n",
    "#     text_cleaned = [lemmatizer.lemmatize(word) for word in text_cleaned]\n",
    "    return text_cleaned\n",
    "\n",
    "cleaned_text = clean_text(text)\n",
    "len(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.B) Changes in the given model - Venky Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length, device=\"cuda\"):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0).to(device))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionHead(nn.Module):\n",
    "        def __init__(self, in_size, head_size,device=\"cuda\"):\n",
    "            \"\"\"\n",
    "            in_size is embed_size\n",
    "            out_size is head_size\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "            self.head_size = head_size\n",
    "            self.K = nn.Linear(in_size, self.head_size, bias=False).to(device)\n",
    "            self.Q = nn.Linear(in_size, self.head_size, bias=False).to(device)\n",
    "            self.V = nn.Linear(in_size, self.head_size, bias=False).to(device)\n",
    "\n",
    "        def forward(self, x,mask=True):\n",
    "            keys = self.K(x)\n",
    "            queries = self.Q(x)\n",
    "            # affinities :\n",
    "            # all the queries will dot-product with all the keys\n",
    "            # transpose (swap) second dimension (input_length) with third (head_size)\n",
    "            keys_t = keys.transpose(1, 2)\n",
    "            autocorrs = (queries @ keys_t) * (self.head_size ** -0.5)  # (batch_size x input_length x input_length)\n",
    "            '''\n",
    "            (batch_size x input_length x embed_size) @ (batch_size x embed_size x input_length) ----> (batch_size x input_length x input_length)\n",
    "            '''\n",
    "            if mask:\n",
    "                autocorrs = torch.tril(autocorrs)\n",
    "                autocorrs = autocorrs.masked_fill(autocorrs == 0, float('-inf'))\n",
    "            autocorrs = torch.softmax(autocorrs, dim=-1)\n",
    "            values = self.V(x) # (batch_size x input_length x head_size)\n",
    "            out = autocorrs @ values\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "        \"\"\"\n",
    "        multiple parallel SA heads (communication among words)\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, head_count, in_size,device=\"cuda\"):\n",
    "            super().__init__()\n",
    "            self.heads = nn.ModuleList(SelfAttentionHead(in_size, in_size // head_count).to(device)\n",
    "                for _ in range(head_count)\n",
    "            )\n",
    "            self.layerNorm = nn.LayerNorm(in_size).to(device)\n",
    "            # self.proj = nn.Linear(out_size, out_size)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # concat over channel/embeddings_size dimension\n",
    "            return self.layerNorm(torch.cat([head(x) for head in self.heads], dim=-1))  # paper - after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "        # FFNN (embed_size, embed_size*4, embed_size)\n",
    "        def __init__(self, embed_size,device=\"cuda\"):\n",
    "            super().__init__()\n",
    "            self.mlp = nn.Sequential(nn.Linear(embed_size, embed_size * 4).to(device),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(embed_size * 4, embed_size).to(device))\n",
    "            self.layerNorm = nn.LayerNorm(embed_size).to(device)\n",
    "\n",
    "        def forward(self, x):  # think\n",
    "            return self.layerNorm(self.mlp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "        def __init__(self, head_count, in_size):\n",
    "            super().__init__()\n",
    "            self.comm = MultiHeadAttention(head_count=head_count,in_size=in_size)#.to(self.device)\n",
    "            self.think = MLP(embed_size=in_size)#.to(self.device)\n",
    "\n",
    "        def forward(self, x):\n",
    "            return (x + self.think(x + self.comm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlockLM(nn.Module):\n",
    "    def __init__(self, batch_size=4,\n",
    "                 input_length=8,\n",
    "                 embed_size=16,\n",
    "                 sa_multihead_count=4,\n",
    "                 pos_embed=True,\n",
    "                 include_mlp=True):\n",
    "        super().__init__()\n",
    "        self.blocks = None\n",
    "        self.ffn = None\n",
    "        self.sa_heads = None\n",
    "        # sa_head_size head_size of self-attention module\n",
    "        self.sa_multihead_count = sa_multihead_count\n",
    "\n",
    "        self.val_data = None\n",
    "        self.train_data = None\n",
    "        self.val_text = None\n",
    "        self.train_text = None\n",
    "        self.K = None\n",
    "        self.linear_sahead_to_vocab = None\n",
    "        self.vocab = None\n",
    "        self.token_embeddings_table = None\n",
    "        self.vocab_size = None\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.vocab_size: int\n",
    "        self.is_pos_emb = pos_embed\n",
    "        self.include_mlp = include_mlp\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        # input_length = how many consecutive tokens/chars in one input\n",
    "        self.input_length = input_length\n",
    "        # batch_size = how many inputs are going to be processed in-parallel (on GPU)\n",
    "        self.batch_size = batch_size\n",
    "        # embed_size = embedding size\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        self.lm_head = None\n",
    "        self.position_embeddings_table = None\n",
    "\n",
    "    def forward(self, in_ids, target=None):\n",
    "        in_ids_emb = self.token_embeddings_table(in_ids[:, -self.input_length:]).to(self.device)\n",
    "        if self.is_pos_emb:\n",
    "            in_ids_emb=self.position_embeddings_table(in_ids_emb).to(self.device)\n",
    "        block_outputs = self.blocks(in_ids_emb)\n",
    "        logits = self.linear_sahead_to_vocab(block_outputs).to(self.device)  # compute\n",
    "        \n",
    "        if target is None:\n",
    "            ce_loss = None\n",
    "        else:\n",
    "            batch_size, input_length, vocab_size = logits.shape\n",
    "            logits_ = logits.view(batch_size * input_length, vocab_size)\n",
    "            targets = target.view(batch_size * input_length)\n",
    "            ce_loss = F.cross_entropy(logits_, targets)\n",
    "        return logits, ce_loss\n",
    "\n",
    "    def fit(self, train_iters=100, eval_iters=10, lr=0.0001):\n",
    "        \"\"\"\n",
    "        train_iters = how many training iterations\n",
    "        eval_iters = how many batches to evaluate to get average performance\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        for iteration in tqdm(range(train_iters)):\n",
    "            if iteration % eval_iters == 0:\n",
    "                avg_loss = self.eval_loss(eval_iters)\n",
    "                print(f\"iter {iteration}: train {avg_loss['train']} val {avg_loss['eval']}\")\n",
    "            inputs, targets = self.get_batch(split='train')\n",
    "            _, ce_loss = self(inputs.to(self.device), targets.to(self.device))\n",
    "            optimizer.zero_grad(set_to_none=True)  # clear gradients of previous step\n",
    "            ce_loss.backward()  # propagate loss back to each unit in the network\n",
    "            optimizer.step()  # update network parameters w.r.t the loss\n",
    "        # torch.save(self, 'sa_pos_')\n",
    "\n",
    "    def generate(self, context_token_ids, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            token_rep, _ = self(context_token_ids)\n",
    "            last_token_rep = token_rep[:, -1, :]\n",
    "            probs = F.softmax(last_token_rep, dim=1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            context_token_ids = torch.cat((context_token_ids, next_token), dim=1)\n",
    "        output_text = self.decoder(context_token_ids[0].tolist())\n",
    "        return output_text\n",
    "\n",
    "    @torch.no_grad()  # tell torch not to prepare for back-propagation (context manager)\n",
    "    def eval_loss(self, eval_iters):\n",
    "        perf = {}\n",
    "        # set dropout and batch normalization layers to evaluation mode before running inference.\n",
    "        self.eval()\n",
    "        for split in ['train', 'eval']:\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "                tokens, targets = self.get_batch(split)  # get random batch of inputs and targete\n",
    "                _, ce_loss = self(tokens, targets)  # forward pass\n",
    "                losses[k] = ce_loss.item()  # the value of loss tensor as a standard Python number\n",
    "            perf[split] = losses.mean()\n",
    "        self.train()  # turn-on training mode-\n",
    "        return perf\n",
    "\n",
    "    def prep(self, corpus):\n",
    "        self.vocab = sorted(list(set(corpus)))\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        c2i = {c: i for i, c in\n",
    "               enumerate(self.vocab)}  # char c to integer i map. assign value i for every word in vocab\n",
    "        i2c = {i: c for c, i in c2i.items()}  # integer i to char c map\n",
    "\n",
    "        self.encoder = lambda doc: [c2i[c] for c in doc]\n",
    "        self.decoder = lambda nums: ' '.join([i2c[i] for i in nums])\n",
    "\n",
    "        text=corpus\n",
    "        n = len(text)#text is the corpus\n",
    "        self.train_text = text[:int(n * 0.9)]\n",
    "        self.val_text = text[int(n * 0.9):]\n",
    "\n",
    "        self.train_data = torch.tensor(self.encoder(self.train_text), dtype=torch.long)\n",
    "        self.val_data = torch.tensor(self.encoder(self.val_text), dtype=torch.long)\n",
    "\n",
    "        # look-up table for embeddings (vocab_size x embed_size)\n",
    "        # it will be mapping each token id to a vector of embed_size\n",
    "        # a wrapper to store vector representations of each token\n",
    "        self.token_embeddings_table = nn.Embedding(self.vocab_size, self.embed_size).to(self.device)\n",
    "\n",
    "        if self.is_pos_emb:\n",
    "            # self.position_embeddings_table = nn.Embedding(self.input_length, self.embed_size)\n",
    "            self.position_embeddings_table = PositionalEncoding(self.embed_size, self.input_length).to(self.device)\n",
    "            \n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size),\n",
    "            TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size),\n",
    "            TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size),\n",
    "            TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size),\n",
    "            TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size),\n",
    "            TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size),\n",
    "        )\n",
    "        # linear projection of sa_head output to vocabulary\n",
    "        self.linear_sahead_to_vocab = nn.Linear(self.embed_size, self.vocab_size).to(self.device)\n",
    "        return c2i\n",
    "\n",
    "    def get_batch(self, split='train'):\n",
    "        data = self.train_data if split == 'train' else self.val_data\n",
    "        # get random chunks of length batch_size from data\n",
    "        ix = torch.randint(len(data) - self.input_length,(self.batch_size,))\n",
    "        inputs_batch = torch.stack([data[i:i + self.input_length] for i in ix])\n",
    "        targets_batch = torch.stack([data[i + 1:i + self.input_length + 1] for i in ix])\n",
    "        inputs_batch = inputs_batch.to(self.device)\n",
    "        targets_batch = targets_batch.to(self.device)\n",
    "        # inputs_batch is\n",
    "        \n",
    "        return inputs_batch, targets_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params 2400755\n"
     ]
    }
   ],
   "source": [
    "model_venky = TransformerBlockLM(batch_size=64,\n",
    "                           input_length=32,\n",
    "                           embed_size=128,\n",
    "                           sa_multihead_count=8,\n",
    "                           pos_embed=True,\n",
    "                           include_mlp=True)\n",
    "model_venky = model_venky.to(model_venky.device)\n",
    "vocab=model_venky.prep(cleaned_text)\n",
    "model_parameters = filter(lambda p: p.requires_grad, model_venky.parameters())\n",
    "print(f'params {sum([np.prod(p.size()) for p in model_parameters])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerBlockLM(\n",
      "  (token_embeddings_table): Embedding(5107, 128)\n",
      "  (position_embeddings_table): PositionalEncoding()\n",
      "  (blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (comm): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-7): 8 x SelfAttentionHead(\n",
      "            (K): Linear(in_features=128, out_features=16, bias=False)\n",
      "            (Q): Linear(in_features=128, out_features=16, bias=False)\n",
      "            (V): Linear(in_features=128, out_features=16, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (layerNorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (think): MLP(\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        )\n",
      "        (layerNorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (comm): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-7): 8 x SelfAttentionHead(\n",
      "            (K): Linear(in_features=128, out_features=16, bias=False)\n",
      "            (Q): Linear(in_features=128, out_features=16, bias=False)\n",
      "            (V): Linear(in_features=128, out_features=16, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (layerNorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (think): MLP(\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        )\n",
      "        (layerNorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (comm): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-7): 8 x SelfAttentionHead(\n",
      "            (K): Linear(in_features=128, out_features=16, bias=False)\n",
      "            (Q): Linear(in_features=128, out_features=16, bias=False)\n",
      "            (V): Linear(in_features=128, out_features=16, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (layerNorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (think): MLP(\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        )\n",
      "        (layerNorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (comm): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-7): 8 x SelfAttentionHead(\n",
      "            (K): Linear(in_features=128, out_features=16, bias=False)\n",
      "            (Q): Linear(in_features=128, out_features=16, bias=False)\n",
      "            (V): Linear(in_features=128, out_features=16, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (layerNorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (think): MLP(\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        )\n",
      "        (layerNorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (comm): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-7): 8 x SelfAttentionHead(\n",
      "            (K): Linear(in_features=128, out_features=16, bias=False)\n",
      "            (Q): Linear(in_features=128, out_features=16, bias=False)\n",
      "            (V): Linear(in_features=128, out_features=16, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (layerNorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (think): MLP(\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        )\n",
      "        (layerNorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (comm): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-7): 8 x SelfAttentionHead(\n",
      "            (K): Linear(in_features=128, out_features=16, bias=False)\n",
      "            (Q): Linear(in_features=128, out_features=16, bias=False)\n",
      "            (V): Linear(in_features=128, out_features=16, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (layerNorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (think): MLP(\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        )\n",
      "        (layerNorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear_sahead_to_vocab): Linear(in_features=128, out_features=5107, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_venky)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/20000 [00:25<141:46:18, 25.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: train 9.808846473693848 val 9.797698020935059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1001/20000 [01:47<24:14:36,  4.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1000: train 4.214922904968262 val 6.238032817840576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2001/20000 [03:08<22:28:02,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2000: train 2.2400100231170654 val 6.643329620361328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3001/20000 [04:30<15:24:12,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3000: train 1.1253684759140015 val 7.279648303985596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4001/20000 [06:02<20:38:39,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4000: train 0.6694943308830261 val 7.815800189971924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5001/20000 [07:30<10:42:04,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 5000: train 0.4890514016151428 val 8.238455772399902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6001/20000 [08:49<9:59:01,  2.57s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 6000: train 0.3979106843471527 val 8.593734741210938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7001/20000 [10:12<13:18:33,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7000: train 0.3452211320400238 val 8.911792755126953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8001/20000 [11:37<14:52:57,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 8000: train 0.3048756718635559 val 9.17525863647461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9001/20000 [12:56<12:09:03,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 9000: train 0.2758282721042633 val 9.408883094787598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10001/20000 [14:22<11:40:02,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 10000: train 0.26136037707328796 val 9.545560836791992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11001/20000 [15:48<7:31:26,  3.01s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 11000: train 0.2503507435321808 val 9.741219520568848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12001/20000 [17:10<6:14:59,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 12000: train 0.23830480873584747 val 9.870199203491211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13001/20000 [18:36<7:02:15,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 13000: train 0.22813695669174194 val 10.062262535095215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14001/20000 [20:02<6:23:20,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 14000: train 0.21847420930862427 val 10.219225883483887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15001/20000 [21:24<4:02:48,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 15000: train 0.21461302042007446 val 10.253060340881348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16001/20000 [22:43<3:03:46,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 16000: train 0.20829837024211884 val 10.416168212890625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17001/20000 [23:57<3:02:01,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 17000: train 0.2042946219444275 val 10.49260425567627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18001/20000 [25:22<1:41:01,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 18000: train 0.20247820019721985 val 10.610663414001465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19001/20000 [26:41<43:04,  2.59s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 19000: train 0.19791699945926666 val 10.659485816955566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [27:48<00:00, 11.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# model_venky.fit(train_iters=20000, eval_iters=1000, lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model_venky.state_dict(), 'transformer_model_venky_no_stopwords_e4.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the saved parameters into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_venky.load_state_dict(torch.load('transformer_model_venky_no_stopwords_e4.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02 236 1974 1978 244 43 201 1975 1979 301 147 154 1976 1980 334 139 195 1977 1981 290 81 209 1978 1982 299 141 158 1979 1983 316 173 143 1980 1984 270 148 122 1981 1985 326 146 180 1982 1986 315 198 117 liabil net the senior citizen off the senior citizen who run start neg in other insur compani under 2002 well be costfre under hi leadership and we dave sokol had about the enorm 1999 304 285 mitek enorm proud of our yearend arizona by hold in aggreg our dividend repurchas higher all the big at midamerican famili 2006 80636 compound growth rate 19652006 275 perform will be about explain whi those for a sharehold quit grow becaus of the quarter from our endoftheyear estim made or iou to the rest of the world like i felt i owe my berkshir share by tad montross at bottom a sound insur oper requir four disciplin 1 an understand of all exposur that might caus a polici to incur loss 2 a conserv evalu of the likelihood of ani 107 41 2401 2105 39037142 walmart store inc 11 1893 2105 358936125 well fargo compani 68 8015 11123 other 3020 4956 total common stock carri at market 33733 61513 thi is our actual purchas price and also our tax basi gaap cost differ in a few case becaus of writeup or writedown that have been requir in addit we own posit in nontrad secur of dow chemic gener electr goldman sach swiss re and wrigley with both prestig and 2005 gener re 144 tab other charg would be greedi when we be approxim right and frequent interrupt for your railroad made or iou to the rest of you that follow deserv your learn monday along with auto insur quot from my 9800 invest there have been the financi behavior by probabl cost that commit oper must also deliv out my idea concurr had fact in case we finish capit keep the origin most recent survey staff you will get your puls race come by bu leav by privat jet 24 an attach to the proxi materi that is enclos with thi report explain how you can obtain the credenti you will need for admiss to the meet and other event as for plane hotel and car reserv we have again sign up american express 8007996634 to give you special help carol pedersen who handl these matter doe a terrif job for us each year and i thank her for it hotel room can be hard to find but work with carol and you will get one at nebraska furnitur mart locat on a 77acr site on 72 nd street between dodg and pacif we will again be have berkshir weekend discount price last year the store did 333 million of busi dure it annual meet sale a volum that as far as i know exce the oneweek total of ani retail store anyplac to obtain the berkshir discount you must make your purchas between tuesday april 26 th and monday may 2 nd inclus and also present your meet credenti the period special price will even appli to the product of sever prestigi manufactur that normal have ironclad rule against discount but which in the spirit of our sharehold weekend have made an except for you we appreci their cooper nfm is open from 10 am to 9 pm monday through saturday and 10 am 23 to 6 pm on sunday on saturday thi excess hous there requir style sentenc thi theori of the move we find a remark entrepreneur and they hunger for mani comp committe help you as it tell you doe when someth that may feel sharehold left us util will be in the face a given when a ration largest of stock price than they would have been at the higher price at some later point our share would be worth perhap 1 vi billion more than if the highpric repurchas scenario had taken place the logic is simpl if you are go to be a net buyer of stock in the futur either directli with your own money or indirectli through your ownership of a compani that is repurchas share you are hurt when stock rise you benefit when stock swoon emot howev too often complic the matter most peopl includ those who will be net buyer in the futur take comfort in see stock price advanc these sharehold resembl a commut who rejoic after the price of ga increas simpli becaus hi tank contain a day suppli charli and i dont expect to win mani of you over to our way of think weve observ enough human behavior to know the futil of that but we do want you to be awar of our person calculu and here a confess is in order in my earli day i too rejoic when the market rose then i read chapter eight of ben graham the intellig investor the chapter deal with how investor should view fluctuat in stock price immedi the scale fell from my eye and low price becam my friend pick up that book wa one of the luckiest moment in my life in the end the busi we receiv premium of way to and what it will be a agre that understand howev by 1990 sale the foundat receiv arrang go the reinsur 176 52 the minor annual letter and also alway tie for dividend level at death are convent sitebuilt home though it not be chang in such activ by jump on sunday have been a ibm rel histori and 2028 we didnt they never sold 769898 and it will be held on saturday april 30 2010 we acquir tape 18 a few chang in the target it manner if charli and i were to have been terribl on her theme updat the sp 500 in year when of with it year when they are do when they are set\n"
     ]
    }
   ],
   "source": [
    "outputs_venky = model_venky.generate(context_token_ids=torch.zeros((1, 1),dtype=torch.long,device=model_venky.device),max_new_tokens=1000)\n",
    "print(outputs_venky)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.C) Prof. Model response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "''' Look at all previous tokens to generate next\n",
    "    @Author: Uzair Ahmad\n",
    "    2022\n",
    "    +TransformerBlock \n",
    "'''\n",
    "\n",
    "\n",
    "class TransformerBlockLM(nn.Module):\n",
    "    class TransformerBlock(nn.Module):\n",
    "        def __init__(self, head_count, in_size, out_size,device=\"cuda\"):\n",
    "            super().__init__()\n",
    "            self.comm = TransformerBlockLM.MultiHeadAttention(head_count=head_count,\n",
    "                                                              in_size=in_size,\n",
    "                                                              out_size=out_size).to(device)\n",
    "            self.think = TransformerBlockLM.MLP(embed_size=out_size).to(device)\n",
    "\n",
    "        def forward(self, x):\n",
    "            return x + self.think(x + self.comm(x))\n",
    "\n",
    "    class MLP(nn.Module):\n",
    "        # FFNN (embed_size, embed_size*4, embed_size)\n",
    "        def __init__(self, embed_size,device=\"cuda\"):\n",
    "            super().__init__()\n",
    "            self.mlp = nn.Sequential(nn.Linear(embed_size, embed_size * 4).to(device),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(embed_size * 4, embed_size).to(device))\n",
    "            self.layerNorm = nn.LayerNorm(embed_size).to(device)\n",
    "\n",
    "        def forward(self, x):  # think\n",
    "            return self.layerNorm(self.mlp(x))  # paper - after\n",
    "            # return self.mlp(self.layerNorm(x)) # alternate - before\n",
    "\n",
    "    class MultiHeadAttention(nn.Module):\n",
    "        \"\"\"\n",
    "        multiple parallel SA heads (communication among words)\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, head_count, in_size, out_size,device=\"cuda\"):\n",
    "            super().__init__()\n",
    "            self.heads = nn.ModuleList(\n",
    "                TransformerBlockLM.SelfAttentionHead(in_size, out_size // head_count).to(device)\n",
    "                for _ in range(head_count)\n",
    "            )\n",
    "            self.layerNorm = nn.LayerNorm(out_size).to(device)\n",
    "            # self.proj = nn.Linear(out_size, out_size)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # concat over channel/embeddings_size dimension\n",
    "            return self.layerNorm(torch.cat([head(x) for head in self.heads], dim=-1))  # paper - after\n",
    "            # return torch.cat([head(self.layerNorm(x)) for head in self.heads], dim=-1) # alternate - before\n",
    "            # return self.proj(torch.cat([head(x) for head in self.heads], dim=-1))\n",
    "\n",
    "    class SelfAttentionHead(nn.Module):\n",
    "        def __init__(self, in_size, out_size,device=\"cuda\"):\n",
    "            \"\"\"\n",
    "            in_size is embed_size\n",
    "            out_size is head_size\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "            self.head_size = out_size\n",
    "            self.K = nn.Linear(in_size, self.head_size, bias=False).to(device)\n",
    "            self.Q = nn.Linear(in_size, self.head_size, bias=False).to(device)\n",
    "            self.V = nn.Linear(in_size, self.head_size, bias=False).to(device)\n",
    "\n",
    "        def forward(self, x):\n",
    "            keys = self.K(x)\n",
    "            queries = self.Q(x)\n",
    "            # affinities :\n",
    "            # all the queries will dot-product with all the keys\n",
    "            # transpose (swap) second dimension (input_length) with third (head_size)\n",
    "            keys_t = keys.transpose(1, 2)\n",
    "            autocorrs = (queries @ keys_t) * (self.head_size ** -0.5)  # (batch_size x input_length x input_length)\n",
    "            '''\n",
    "            (batch_size x input_length x embed_size) @ (batch_size x embed_size x input_length) ----> (batch_size x input_length x input_length)\n",
    "            '''\n",
    "            autocorrs = torch.tril(autocorrs)\n",
    "            autocorrs = autocorrs.masked_fill(autocorrs == 0, float('-inf'))\n",
    "            autocorrs = torch.softmax(autocorrs, dim=-1)\n",
    "            values = self.V(x)  # (batch_size x input_length x head_size)\n",
    "            out = autocorrs @ values\n",
    "            return out\n",
    "\n",
    "    def __init__(self, batch_size=4,\n",
    "                 input_length=8,\n",
    "                 embed_size=16,\n",
    "                 sa_head_size=8,\n",
    "                 sa_multihead_count=4,\n",
    "                 pos_embed=False,\n",
    "                 include_mlp=False):\n",
    "        super().__init__()\n",
    "        self.blocks = None\n",
    "        self.ffn = None\n",
    "        self.sa_heads = None\n",
    "        # sa_head_size head_size of self-attention module\n",
    "        self.sa_head_size = sa_head_size\n",
    "        self.sa_multihead_count = sa_multihead_count\n",
    "\n",
    "        self.val_data = None\n",
    "        self.train_data = None\n",
    "        self.val_text = None\n",
    "        self.train_text = None\n",
    "        self.K = None\n",
    "        self.linear_sahead_to_vocab = None\n",
    "        self.vocab = None\n",
    "        self.token_embeddings_table = None\n",
    "        self.vocab_size = None\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.vocab_size: int\n",
    "        self.is_pos_emb = pos_embed\n",
    "        self.include_mlp = include_mlp\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        # input_length = how many consecutive tokens/chars in one input\n",
    "        self.input_length = input_length\n",
    "        # batch_size = how many inputs are going to be processed in-parallel (on GPU)\n",
    "        self.batch_size = batch_size\n",
    "        # embed_size = embedding size\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        self.lm_head = None\n",
    "        self.position_embeddings_table = None\n",
    "\n",
    "    def forward(self, in_ids, target=None):\n",
    "        # print(\"Hello\")\n",
    "        # print(in_ids.shape)\n",
    "        # print(self.token_embeddings_table)\n",
    "        in_ids_emb = self.token_embeddings_table(in_ids[:, -self.input_length:])\n",
    "        # print(\"Hello\")\n",
    "        if self.is_pos_emb:\n",
    "            in_ids_pos_emb = self.position_embeddings_table(\n",
    "                torch.arange(in_ids[:, -self.input_length:].shape[1], device=self.device)\n",
    "            )\n",
    "            in_ids_emb = in_ids_emb + in_ids_pos_emb\n",
    "        block_outputs = self.blocks((in_ids_emb))\n",
    "        logits = self.linear_sahead_to_vocab(block_outputs).to(self.device)  # compute\n",
    "        if target is None:\n",
    "            ce_loss = None\n",
    "        else:\n",
    "            batch_size, input_length, vocab_size = logits.shape\n",
    "            logits_ = logits.view(batch_size * input_length, vocab_size)\n",
    "            targets = target.view(batch_size * input_length)\n",
    "            ce_loss = F.cross_entropy(logits_, targets)\n",
    "        return logits, ce_loss\n",
    "\n",
    "    def fit(self, train_iters=100, eval_iters=10, lr=0.0001):\n",
    "        \"\"\"\n",
    "        train_iters = how many training iterations\n",
    "        eval_iters = how many batches to evaluate to get average performance\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        for iteration in tqdm(range(train_iters)):\n",
    "            if iteration % eval_iters == 0:\n",
    "                avg_loss = self.eval_loss(eval_iters)\n",
    "                print(f\"iter {iteration}: train {avg_loss['train']} val {avg_loss['eval']}\")\n",
    "            inputs, targets = self.get_batch(split='train')\n",
    "            _, ce_loss = self(inputs, targets)\n",
    "            optimizer.zero_grad(set_to_none=True)  # clear gradients of previous step\n",
    "            ce_loss.backward()  # propagate loss back to each unit in the network\n",
    "            optimizer.step()  # update network parameters w.r.t the loss\n",
    "        # torch.save(self, 'sa_pos_')\n",
    "\n",
    "    def generate(self, context_token_ids, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            token_rep, cse_loss = self(context_token_ids)\n",
    "            last_token_rep = token_rep[:, -1, :]\n",
    "            probs = F.softmax(last_token_rep, dim=1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            context_token_ids = torch.cat((context_token_ids, next_token), dim=1)\n",
    "        output_text = self.decoder(context_token_ids[0].tolist())\n",
    "        return output_text\n",
    "\n",
    "    @torch.no_grad()  # tell torch not to prepare for back-propagation (context manager)\n",
    "    def eval_loss(self, eval_iters):\n",
    "        perf = {}\n",
    "        # set dropout and batch normalization layers to evaluation mode before running inference.\n",
    "        self.eval()\n",
    "        for split in ['train', 'eval']:\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "                tokens, targets = self.get_batch(split)  # get random batch of inputs and targete\n",
    "                _, ce_loss = self(tokens, targets)  # forward pass\n",
    "                losses[k] = ce_loss.item()  # the value of loss tensor as a standard Python number\n",
    "            perf[split] = losses.mean()\n",
    "        self.train()  # turn-on training mode-\n",
    "        return perf\n",
    "\n",
    "    def prep(self, corpus):\n",
    "        self.vocab = sorted(list(set(corpus)))\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        c2i = {c: i for i, c in\n",
    "               enumerate(self.vocab)}  # char c to integer i map. assign value i for every word in vocab\n",
    "        i2c = {i: c for c, i in c2i.items()}  # integer i to char c map\n",
    "\n",
    "        self.encoder = lambda doc: [c2i[c] for c in doc]\n",
    "        self.decoder = lambda nums: ' '.join([i2c[i] for i in nums])\n",
    "        text=corpus\n",
    "        n = len(text)\n",
    "        self.train_text = text[:int(n * 0.9)]\n",
    "        self.val_text = text[int(n * 0.9):]\n",
    "\n",
    "        self.train_data = torch.tensor(self.encoder(self.train_text), dtype=torch.long)\n",
    "        self.val_data = torch.tensor(self.encoder(self.val_text), dtype=torch.long)\n",
    "\n",
    "        # look-up table for embeddings (vocab_size x embed_size)\n",
    "        # it will be mapping each token id to a vector of embed_size\n",
    "        # a wrapper to store vector representations of each token\n",
    "        self.token_embeddings_table = \\\n",
    "            nn.Embedding(self.vocab_size, self.embed_size).to(self.device)\n",
    "\n",
    "        if self.is_pos_emb:\n",
    "            self.position_embeddings_table = nn.Embedding(self.input_length, self.embed_size).to(self.device)\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size),\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size),\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size),\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size),\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size),\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size),\n",
    "        )\n",
    "        # linear projection of sa_head output to vocabulary\n",
    "        self.linear_sahead_to_vocab = nn.Linear(self.sa_head_size, self.vocab_size).to(self.device)\n",
    "        return c2i\n",
    "    def get_batch(self, split='train'):\n",
    "        data = self.train_data if split == 'train' else self.val_data\n",
    "        # get random chunks of length batch_size from data\n",
    "        ix = torch.randint(len(data) - self.input_length,\n",
    "                           (self.batch_size,))\n",
    "        inputs_batch = torch.stack([data[i:i + self.input_length] for i in ix])\n",
    "        targets_batch = torch.stack([data[i + 1:i + self.input_length + 1] for i in ix])\n",
    "        inputs_batch = inputs_batch.to(self.device)\n",
    "        targets_batch = targets_batch.to(self.device)\n",
    "        # inputs_batch is\n",
    "        return inputs_batch, targets_batch\n",
    "\n",
    "model = TransformerBlockLM(batch_size=64,\n",
    "                           input_length=32,\n",
    "                           embed_size=128,\n",
    "                           sa_multihead_count=8,\n",
    "                           sa_head_size=128,\n",
    "                           pos_embed=True,\n",
    "                           include_mlp=True)\n",
    "model = model.to(model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params 2404851\n"
     ]
    }
   ],
   "source": [
    "vocab=model.prep(cleaned_text)\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "print(f'params {sum([np.prod(p.size()) for p in model_parameters])}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/20000 [00:25<142:55:49, 25.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: train 10.834053993225098 val 10.818982124328613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1001/20000 [01:50<17:03:35,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1000: train 5.281001091003418 val 6.567872047424316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2003/20000 [03:20<11:48:43,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2000: train 2.9567742347717285 val 6.759474754333496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3003/20000 [04:47<14:00:59,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3000: train 1.7320491075515747 val 7.25107479095459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4003/20000 [06:14<11:36:19,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4000: train 1.0589375495910645 val 7.79243278503418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5001/20000 [07:46<16:43:28,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 5000: train 0.7249415516853333 val 8.351359367370605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6001/20000 [09:18<17:21:20,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 6000: train 0.5594689249992371 val 8.754654884338379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7001/20000 [10:43<10:01:12,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7000: train 0.46669816970825195 val 9.127132415771484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8001/20000 [12:12<10:18:10,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 8000: train 0.40510857105255127 val 9.44779109954834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9001/20000 [13:35<10:28:55,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 9000: train 0.3588620126247406 val 9.716057777404785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10001/20000 [15:03<8:28:20,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 10000: train 0.32399845123291016 val 9.969474792480469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11003/20000 [16:37<6:41:51,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 11000: train 0.2953031063079834 val 10.221504211425781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12001/20000 [18:03<6:13:45,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 12000: train 0.2738821506500244 val 10.466636657714844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13001/20000 [19:28<5:24:38,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 13000: train 0.25600090622901917 val 10.681992530822754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14001/20000 [20:52<4:14:17,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 14000: train 0.24152036011219025 val 10.898113250732422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15001/20000 [22:17<3:28:13,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 15000: train 0.22816354036331177 val 11.106903076171875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16001/20000 [23:48<3:34:14,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 16000: train 0.2183024287223816 val 11.269912719726562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17001/20000 [25:08<2:35:51,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 17000: train 0.20949403941631317 val 11.472888946533203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18003/20000 [26:30<1:18:31,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 18000: train 0.20234547555446625 val 11.673047065734863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19001/20000 [27:50<51:39,  3.10s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 19000: train 0.19651968777179718 val 11.7802734375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [28:53<00:00, 11.54it/s]\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_iters=20000, eval_iters=1000, lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'transformer_model_prof_no_stopwords_e4.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('transformer_model_prof_no_stopwords_e4.pth'))\n",
    "model = model.to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02 236 1974 1978 244 43 201 1975 1979 301 147 154 1976 1980 334 139 195 1977 1981 290 81 209 1978 1982 299 141 158 1979 1983 316 173 143 1980 1984 270 148 122 1981 1985 326 146 180 1982 1986 315 198 117 1983 1987 274 164 110 1984 1988 250 152 98 1985 1989 311 203 108 1986 1990 229 131 98 1987 1991 254 153 101 1988 1992 256 158 98 1989 1993 244 145 99 1990 1994 186 87 99 1991 1995 256 165 91 1992 1996 242 152 90 1993 1997 269 202 67 1994 1998 337 240 97 1995 1999 304 285 19 1996 2000 229 183 46 1997 2001 148 107 41 1998 2002 104 06 110 1999 2003 60 well the frailti thought process and i have need your here 4 pm on saturday afternoon for sharehold who have come from outsid of north america everi year our meet draw mani peopl from around the globe and charli and i want to be sure we person greet those who have come so far last year we enjoy meet more than 400 of you from mani dozen of countri ani sharehold who come from other than the us or canada will be given a special credenti and instruct for attend thi function charli and i are extraordinarili lucki we were born in america had terrif parent who saw that we got good educ have enjoy wonder famili and great health and came equip with a busi gene that allow us to prosper in a manner huge disproportion to that experienc by mani peopl who contribut as much or more to our societi wellb moreov we have long had job that we love in which we are help everi day in countless way by talent and cheer associ no wonder we tap danc to work but noth is more fun for us than get togeth with our shareholderpartn at berkshir annual meet so join us on may 5 th at the qwest for our annual woodstock for capitalist well see you there februari 28 2007 warren e buffett chairman of the board 24 berkshir corpor perform vs the sp 500 annual percentag chang in pershar in sp 500 book valu of with dividend rel berkshir includ result year 1 2 l2 1965 238 100 138 1966 203 117 320 1967 110 309 199 1968 190 110 80 1969 162 84 246 1970 120 39 81 1971 164 146 18 1972 217 189 28 1973 47 148 195 1974 55 264 319 1975 219 372 153 1976 593 236 357 1977 319 74 393 1978 240 64 176 1979 357 182 175 1980 193 323 130 1981 314 50 364 1982 400 214 186 1983 323 224 99 1984 136 61 75 1985 482 316 166 1986 261 186 75 1987 195 51 144 1988 201 166 35 1989 444 317 127 1990 74 31 105 1991 396 305 91 1992 203 76 127 1993 143 101 42 1994 139 13 126 1995 431 376 55 1996 318 230 88 1997 341 334 7 1998 483 286 197 1999 5 210 205 2000 65 91 156 2001 62 119 57 2002 100 221 321 2003 210 287 77 2004 105 109 4 2005 64 49 15 2006 184 158 26 2007 110 55 55 2008 96 370 274 2009 198 265 67 2010 130 151 21 compound annual gain 19652010 202 94 108 overal gain 19642010 490409 6262 note data are for calendar year with these except 1965 and 1966 year end 930 1967 15 month end 1231 start in 1979 account rule requir insur compani to valu the equiti secur they hold at market rather than at the lower of cost or market which wa previous the requir in thi tabl berkshir result through 1978 have been restat to conform to the chang rule in all other respect the result are calcul use the number origin report the sp 500 number are pretax wherea the berkshir number are aftertax if a corpor such as berkshir were simpli to have own the sp 500 and accru the appropri tax it result are we will soon no attempt to increas it invest return by account john our new the compani we have enjoy cash for thi year wa hemorrhag deal on furthermor when you can be sure of our had terrif of scriptur as well will come to think hell be as we know if charli until and i catch 2003 here our consult in 1956 to date went as a number by govern hi product from dozen result look into a ago ha clearli been of huge regul in execut but in the lubrizol need money did lloyd the float affect the prospect busi it also be forc is our flexibl the enorm of invest huge stori were were were were to make a dollar to oper that would doubl our busi perform about view fluctuat includ the few dollar also of deliv us instead we can tri to be as ive said in these memo for more than 25 year we can afford to lose money even a lot of money but we cant afford to lose reput even a shred of reput we must continu to measur everi act against not onli what is legal but also what we would be happi to have written about on the front page of a nation newspap in an articl written by an unfriendli but intellig report sometim your associ will say everybodi els is do it thi rational is almost alway a bad one if it is the main justif for a busi action it is total unaccept when evalu a moral decis whenev somebodi offer that phrase as a rational in effect they are say that they cant come up with a good reason if anyon give thi explan tell them to tri use it with a report or a judg and\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(context_token_ids=torch.zeros((1, 1),dtype=torch.long,device=model.device),max_new_tokens=1000)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.D) Perplexity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemented the model perplexity defined in this article - https://medium.com/@priyankads/perplexity-of-language-models-41160427ed72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(text, vocab,device):\n",
    "    cleaned_data=clean_text(text)\n",
    "    # print(cleaned_data)\n",
    "    indx_id=[]\n",
    "    for text in cleaned_data:\n",
    "        try:\n",
    "            indx_id.append(vocab[text])\n",
    "        except:\n",
    "            print(\"Not present in vocab:\",text)\n",
    "    tensor = torch.tensor(indx_id, dtype=torch.long, device=device)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: BERKSHIRE HATHAWAY INC. \n",
      "\n",
      "\n",
      "\n",
      "To the Shareholders of Berkshire Hathaway Inc.: \n",
      "\n",
      "Our gain in net worth during 2006 was $16.9 billion, which increased the per-share book value of \n",
      "both our Class A and Class B stock by 18.4%. Over the last 42 years (that is, since present management \n",
      "took over) book value has grown from $19 to $70,281, a rate of 21.4% compounded annually.* \n",
      "\n",
      "We believe that $16.9 billion is a record for a one-year gain in net worth - more than has ever \n",
      "been booked by any American business, leaving aside boosts that have occurred because of mergers (e.g., \n",
      "AOL's purchase of Time Warner). Of course, Exxon Mobil and other companies earn far more than \n",
      "Berkshire, but their earnings largely go to dividends and/or repurchases, rather than to building net worth. \n",
      "\n",
      "All that said, a confession about our 2006 gain is in order. Our most important business, \n",
      "insurance, benefited from a large dose of luck: Mother Nature, bless her heart, went on vacation. After \n",
      "hammering us with hurricanes in 2004 and 2005 - storms that caused us to lose a bundle on super-cat \n",
      "insurance - she just vanished. Last year, the red ink from this activity turned black - very black. \n",
      "\n",
      "In addition, the great majority of our 73 businesses did outstandingly well in 2006. Let me focus \n",
      "for a moment on one of our largest operations, GEICO. What management accomplished there was simply \n",
      "extraordinary. \n",
      "\n",
      "As I've told you before, Tony Nicely, GEICO' s CEO, went to work at the company 45 years ago, \n",
      "two months after turning 18. He became CEO in 1992, and from then on the company's growth exploded. \n",
      "In addition, Tony has delivered staggering productivity gains in recent years. Between yearend 2003 and \n",
      "yearend 2006, the number of GEICO policies increased from 5.7 million to 8.1 million, a jump of 42%. \n",
      "Yet during that same period, the company's employees (measured on a fulltime-equivalent basis) fell 3.5%. \n",
      "So productivity grew 47%. And GEICO didn't start fat. \n",
      "\n",
      "That remarkable gain has allowed GEICO to maintain its all-important position as a low-cost \n",
      "producer, even though it has dramatically increased advertising expenditures. Last year GEICO spent $631 \n",
      "million on ads, up from $238 million in 2003 (and up from $31 million in 1995, when Berkshire took \n",
      "control). Today, GEICO spends far more on ads than any of its competitors, even those much larger. We \n",
      "will continue to raise the bar. \n",
      "\n",
      "Last year I told you that if you had a new son or grandson to be sure to name him Tony. But Don \n",
      "Keough, a Berkshire director, recently had a better idea. After reviewing GEICO's performance in 2006, \n",
      "he wrote me, \"Forget births. Tell the shareholders to immediately change the names of their present \n",
      "children to Tony or Antoinette.\" Don signed his letter \"Tony.\" \n",
      "\n",
      "Charlie Munger - my partner and Berkshire's vice chairman - and I run what has turned out to be \n",
      "a big business, one with 217,000 employees and annual revenues approaching $100 billion. We certainly \n",
      "didn't plan it that way. Charlie began as a lawyer, and I thought of myself as a security analyst. Sitting in \n",
      "those seats, we both grew skeptical about the ability of big entities of any type to function well. Size seems \n",
      "to make many organizations slow-thinking, resistant to change and smug. In Churchill's words: \"We shape \n",
      "our buildings, and afterwards our buildings shape us.\" Here's a telling fact: Of the ten non-oil companies \n",
      "having the largest market capitalization in 1965 - titans such as General Motors, Sears, DuPont and \n",
      "Eastman Kodak - only one made the 2006 list. \n",
      "\n",
      "\n",
      "\n",
      "*A11 per-share figures used in this report apply to Berkshire's A shares. Figures for the B shares \n",
      "are 1/30* of those shown for the A. \n",
      "\n",
      "\n",
      "3 \n",
      "\n",
      "\n",
      "In fairness, we've seen plenty of successes as well, some truly outstanding. There are many giant- \n",
      "company managers whom I greatly admire; Ken Chenault of American Express, Jeff Immelt of G.E. and \n",
      "Dick Kovacevich of Wells Fargo come quickly to mind. But I don't think I could do the management job \n",
      "they do. And I know I wouldn't enjoy many of the duties that come with their positions - meetings, \n",
      "speeches, foreign travel, the charity circuit and governmental relations. For me, Ronald Reagan had it \n",
      "right: \"It's probably true that hard work never killed anyone - but why take the chance?\" \n",
      "\n",
      "So I've taken the easy route, just sitting back and working through great managers who run their \n",
      "own shows. My only tasks are to cheer them on, sculpt and harden our corporate culture, and make major \n",
      "capital-allocation decisions. Our managers have returned this trust by working hard and effectively. \n",
      "\n",
      "For their performance over the last 42 years - and particularly for 2006 - Charlie and I thank \n",
      "\n",
      "them. \n",
      "\n",
      "Yardsticks \n",
      "\n",
      "Charlie and I measure Berkshire's progress and evaluate its intrinsic value in a number of ways. \n",
      "No single criterion is effective in doing these jobs, and even an avalanche of statistics will not capture some \n",
      "factors that are important. For example, it's essential that we have managers much younger than I available \n",
      "to succeed me. Berkshire has never been in better shape in this regard - but I can't prove it to you with \n",
      "numbers. \n",
      "\n",
      "There are two statistics, however, that are of real importance. The first is the amount of \n",
      "investments (including cash and cash-equivalents) that we own on a per-share basis. Arriving at this figure, \n",
      "we exclude investments held in our finance operation because these are largely offset by borrowings. \n",
      "Here's the record since present management acquired control of Berkshire: \n",
      "\n",
      "\n",
      "Year Per-Share Investments * \n",
      "\n",
      "1965 $ 4 \n",
      "\n",
      "1975 159 \n",
      "\n",
      "1985 2,407 \n",
      "\n",
      "1995 21,817 \n",
      "\n",
      "2006 $80,636 \n",
      "\n",
      "Compound Growth Rate 1965-2006 27.5% \n",
      "\n",
      "Compound Growth Rate 1995-2006 12.6% \n",
      "\n",
      "\n",
      "*Net of minority interests \n",
      "\n",
      "In our early years we put most of our retained earnings and insurance float into investments in \n",
      "marketable securities. Because of this emphasis, and because the securities we purchased generally did \n",
      "well, our growth rate in investments was for a long time quite high. \n",
      "\n",
      "Over the years, however, we have focused more and more on the acquisition of operating \n",
      "businesses. Using our funds for these purchases has both slowed our growth in investments and accelerated \n",
      "our gains in pre-tax earnings from non-insurance businesses, the second yardstick we use. Here's how \n",
      "those earnings have looked: \n",
      "\n",
      "\n",
      "4 \n",
      "\n",
      "\n",
      "Year \n",
      "\n",
      "\n",
      "Pre-Tax Earnings Per Share * \n",
      "\n",
      "\n",
      "1965 $ 4 \n",
      "\n",
      "1975 4 \n",
      "\n",
      "1985 52 \n",
      "\n",
      "1995 175 \n",
      "\n",
      "2006 $3,625 \n",
      "\n",
      "Compound Growth Rate 1965-2006 17.9% \n",
      "\n",
      "Compound Growth Rate 1995-2006 31.7% \n",
      "\n",
      "\n",
      "*Excluding purchase-accounting adjustments and net of minority interests \n",
      "\n",
      "Last year we had a good increase in non-insurance earnings - 38%. Large gains from here on in, \n",
      "though, will come only if we are able to make major, and sensible, acquisitions. That will not be easy. We \n",
      "do, however, have one advantage: More and more, Berkshire has become \"the buyer of choice\" for \n",
      "business owners and managers. Initially, we were viewed that way only in the U.S. (and more often than \n",
      "not by private companies). We've long wanted, nonetheless, to extend Berkshire's appeal beyond U.S. \n",
      "borders. And last year, our globe-trotting finally got underway. \n",
      "\n",
      "Acquisitions \n",
      "\n",
      "We began 2006 by completing the three acquisitions pending at yearend 2005, spending about $6 \n",
      "billion for PacifiCorp, Business Wire and Applied Underwriters. All are performing very well. \n",
      "\n",
      "The highlight of the year, however, was our July 5 th acquisition of most of ISCAR, an Israeli \n",
      "company, and our new association with its chairman, Eitan Wertheimer, and CEO, Jacob Harpaz. The \n",
      "story here began on October 25, 2005, when I received a 114-page letter from Eitan, of whom I then knew \n",
      "nothing. The letter began, \"I am writing to introduce you to ISCAR,\" and proceeded to describe a cutting- \n",
      "tool business carried on in 61 countries. Then Eitan wrote, \"We have for some time considered the issues \n",
      "of generational transfer and ownership that are typical for large family enterprises, and have given much \n",
      "thought to ISCAR' s future. Our conclusion is that Berkshire Hathaway would be the ideal home for \n",
      "ISCAR. We believe that ISCAR would continue to thrive as a part of your portfolio of businesses.\" \n",
      "\n",
      "Overall, Eitan' s letter made the quality of the company and the character of its management leap \n",
      "off the page. It also made me want to learn more, and in November, Eitan, Jacob and ISCAR' s CFO, \n",
      "Danny Goldman, came to Omaha. A few hours with them convinced me that if we were to make a deal, we \n",
      "would be teaming up with extraordinarily talented managers who could be trusted to run the business after \n",
      "a sale with all of the energy and dedication that they had exhibited previously. However, having never \n",
      "bought a business based outside of the U.S. (though I had bought a number of foreign stocks), I needed to \n",
      "get educated on some tax and jurisdictional matters. With that task completed, Berkshire purchased 80% of \n",
      "ISCAR for $4 billion. The remaining 20% stays in the hands of the Wertheimer family, making it our \n",
      "valued partner. \n",
      "\n",
      "ISCAR' s products are small, consumable cutting tools that are used in conjunction with large and \n",
      "expensive machine tools. It's a business without magic except for that imparted by the people who run it. \n",
      "But Eitan, Jacob and their associates are true managerial magicians who constantly develop tools that make \n",
      "their customers' machines more productive. The result: ISCAR makes money because it enables its \n",
      "customers to make more money. There is no better recipe for continued success. \n",
      "\n",
      "\n",
      "5 \n",
      "\n",
      "\n",
      "In September, Charlie and I, along with five Berkshire associates, visited ISCAR in Israel. We - \n",
      "and I mean every one of us - have never been more impressed with any operation. At ISCAR, as \n",
      "throughout Israel, brains and energy are ubiquitous. Berkshire shareholders are lucky to have joined with \n",
      "Eitan, Jacob, Danny and their talented associates. \n",
      "\n",
      "A few months later, Berkshire again became \"the buyer of choice\" in a deal brought to us by my \n",
      "friend, John Roach, of Fort Worth. John, many of you will remember, was Chairman of Justin Industries, \n",
      "which we bought in 2000. At that time John was helping John Justin, who was terminally ill, find a \n",
      "permanent home for his company. John Justin died soon after we bought Justin Industries, but it has since \n",
      "been run exactly as we promised him it would be. \n",
      "\n",
      "Visiting me in November, John Roach brought along Paul Andrews, Jr., owner of about 80% of \n",
      "TTI, a Fort Worth distributor of electronic components. Over a 35-year period, Paul built TTI from \n",
      "$112,000 of sales to $1.3 billion. He is a remarkable entrepreneur and operator. \n",
      "\n",
      "Paul, 64, loves running his business. But not long ago he happened to witness how disruptive the \n",
      "death of a founder can be both to a private company's employees and the owner's family. What starts out \n",
      "as disruptive, furthermore, often evolves into destructive. About a year ago, therefore, Paul began to think \n",
      "about selling TTI. His goal was to put his business in the hands of an owner he had carefully chosen, rather \n",
      "than allowing a trust officer or lawyer to conduct an auction after his death. \n",
      "\n",
      "Paul rejected the idea of a \"strategic\" buyer, knowing that in the pursuit of \"synergies,\" an owner \n",
      "of that type would be apt to dismantle what he had so carefully built, a move that would uproot hundreds of \n",
      "his associates (and perhaps wound TTI's business in the process). He also ruled out a private equity firm, \n",
      "which would very likely load the company with debt and then flip it as soon as possible. \n",
      "\n",
      "That left Berkshire. Paul and I met on the morning of November 15 th and made a deal before \n",
      "lunch. Later he wrote me: \"After our meeting, I am confident that Berkshire is the right owner for TTI . . . \n",
      "I am proud of our past and excited about our future.\" And so are Charlie and I. \n",
      "\n",
      "We also made some \"tuck-in\" acquisitions during 2006 at Fruit of the Loom (\"Fruit\"), MiTek, \n",
      "CTB, Shaw and Clayton. Fruit made the largest purchases. First, it bought Russell Corp., a leading \n",
      "producer of athletic apparel and uniforms for about $1.2 billion (including assumed debt) and in December \n",
      "it agreed to buy the intimate apparel business of VF Corp. Together, these acquisitions add about $2.2 \n",
      "billion to Fruit's sales and bring with them about 23,000 employees. \n",
      "\n",
      "Charlie and I love it when we can acquire businesses that can be placed under managers, such as \n",
      "John Holland at Fruit, who have already shown their stuff at Berkshire. MiTek, for example, has made 14 \n",
      "acquisitions since we purchased it in 2001, and Gene Toombs has delivered results from these deals far in \n",
      "excess of what he had predicted. In effect, we leverage the managerial talent already with us by these tuck- \n",
      "in deals. We will make many more. \n",
      "\n",
      "We continue, however, to need \"elephants\" in order for us to use Berkshire's flood of incoming \n",
      "cash. Charlie and I must therefore ignore the pursuit of mice and focus our acquisition efforts on much \n",
      "bigger game. \n",
      "\n",
      "Our exemplar is the older man who crashed his grocery cart into that of a much younger fellow \n",
      "while both were shopping. The elderly man explained apologetically that he had lost track of his wife and \n",
      "was preoccupied searching for her. His new acquaintance said that by coincidence his wife had also \n",
      "wandered off and suggested that it might be more efficient if they jointly looked for the two women. \n",
      "Agreeing, the older man asked his new companion what his wife looked like. \"She's a gorgeous blonde,\" \n",
      "the fellow answered, \"with a body that would cause a bishop to go through a stained glass window, and \n",
      "she's wearing tight white shorts. How about yours?\" The senior citizen wasted no words: \"Forget her, \n",
      "we'll look for yours.\" \n",
      "\n",
      "\n",
      "6 \n",
      "\n",
      "\n",
      "What we are looking for is described on page 25. If you have an acquisition candidate that fits, \n",
      "call me - day or night. And then watch me shatter a stained glass window. \n",
      "\n",
      "Now, let's examine the four major operating sectors of Berkshire. Lumping their financial figures \n",
      "together impedes analysis. So we'll look at them as four separate businesses, starting with the all- \n",
      "important insurance group. \n",
      "\n",
      "Insurance \n",
      "\n",
      "Next month marks the 40 th anniversary of our entrance into the insurance business. It was on \n",
      "March 9, 1967, that Berkshire purchased National Indemnity and its companion company, National Fire & \n",
      "Marine, from Jack Ringwalt for $8.6 million. \n",
      "\n",
      "Jack was a long-time friend of mine and an excellent, but somewhat eccentric, businessman. For \n",
      "about ten minutes every year he would get the urge to sell his company. But those moods - perhaps \n",
      "brought on by a tiff with regulators or an unfavorable jury verdict - quickly vanished. \n",
      "\n",
      "In the mid-1960s, I asked investment banker Charlie Heider, a mutual friend of mine and Jack's, \n",
      "to alert me the next time Jack was \"in heat.\" When Charlie's call came, I sped to meet Jack. We made a \n",
      "deal in a few minutes, with me waiving an audit, \"due diligence\" or anything else that would give Jack an \n",
      "opportunity to reconsider. We just shook hands, and that was that. \n",
      "\n",
      "When we were due to close the purchase at Charlie's office, Jack was late. Finally arriving, he \n",
      "explained that he had been driving around looking for a parking meter with some unexpired time. That was \n",
      "a magic moment for me. I knew then that Jack was going to be my kind of manager. \n",
      "\n",
      "When Berkshire purchased Jack's two insurers, they had \"float\" of $17 million. We've regularly \n",
      "offered a long explanation of float in earlier reports, which you can read on our website. Simply put, float \n",
      "is money we hold that is not ours but which we get to invest. \n",
      "\n",
      "At the end of 2006, our float had grown to $50.9 billion, and we have since written a huge \n",
      "retroactive reinsurance contract with Equitas - which I will describe in the next section - that boosts float \n",
      "by another $7 billion. Much of the gain we've made has come through our acquisition of other insurers, \n",
      "but we've also had outstanding internal growth, particularly at Ajit Jain's amazing reinsurance operation. \n",
      "Naturally, I had no notion in 1967 that our float would develop as it has. There's much to be said for just \n",
      "putting one foot in front of the other every day. \n",
      "\n",
      "The float from retroactive reinsurance contracts, of which we have many, automatically drifts \n",
      "down over time. Therefore, it will be difficult for us to increase float in the future unless we make new \n",
      "acquisitions in the insurance field. Whatever its size, however, the all-important cost of Berkshire's float \n",
      "over time is likely to be significantly below that of the industry, perhaps even falling to less than zero. \n",
      "Note the words \"over time.\" There will be bad years periodically. You can be sure of that. \n",
      "\n",
      "In 2006, though, everything went right in insurance - really right. Our managers - Tony Nicely \n",
      "(GEICO), Ajit Jain (B-H Reinsurance), Joe Brandon and Tad Montross (General Re), Don Wurster \n",
      "(National Indemnity Primary), Tom Nerney (U.S. Liability), Tim Kenesey (Medical Protective), Rod \n",
      "Eldred (Homestate Companies and Cypress), Sid Ferenc and Steve Menzies (Applied Underwriters), John \n",
      "Kizer (Central States) and Don Towle (Kansas Bankers Surety) - simply shot the lights out. When I recite \n",
      "their names, I feel as if I'm at Cooperstown, reading from the Hall of Fame roster. Of course, the overall \n",
      "insurance industry also had a terrific year in 2006. But our managers delivered results generally superior to \n",
      "those of their competitors. \n",
      "\n",
      "\n",
      "7 \n",
      "\n",
      "\n",
      "Below is the tally on our underwriting and float for each major sector of insurance. Enjoy the \n",
      "view, because you won't soon see another like it. \n",
      "\n",
      "\n",
      "(in $ millions) \n",
      "\n",
      "\n",
      "Underwriting Profit (Loss) \n",
      "\n",
      "\n",
      "Yearend Float \n",
      "\n",
      "\n",
      "Insurance Operations 2006 2005 \n",
      "\n",
      "General Re $ 526 $( 334) \n",
      "\n",
      "B-H Reinsurance 1,658 (1,069) \n",
      "\n",
      "GEICO 1,314 1,221 \n",
      "\n",
      "Other Primary 340 ** 235 * \n",
      "\n",
      "Total $3.838 $ 53 \n",
      "\n",
      "* Includes MedPro from June 30, 2005. \n",
      "** Includes Applied Underwriters from May 19, 2006. \n",
      "\n",
      "\n",
      "$22,827 $22,920 \n",
      "\n",
      "16,860 16,233 \n",
      "\n",
      "7,171 6,692 \n",
      "\n",
      "4,029 3,442 \n",
      "\n",
      "$50.887 $49.287 \n",
      "\n",
      "\n",
      "2006 2005 \n",
      "\n",
      "\n",
      "In 2007, our results from the bread-and-butter lines of insurance will deteriorate, though I think \n",
      "they will remain satisfactory. The big unknown is super-cat insurance. Were the terrible hurricane seasons \n",
      "of 2004-05 aberrations? Or were they our planet's first warning that the climate of the 21 st Century will \n",
      "differ materially from what we've seen in the past? If the answer to the second question is yes, 2006 will \n",
      "soon be perceived as a misleading period of calm preceding a series of devastating storms. These could \n",
      "rock the insurance industry. It's naive to think of Katrina as anything close to a worst-case event. \n",
      "\n",
      "Neither Ajit Jain, who manages our super-cat operation, nor I know what lies ahead. We do know \n",
      "that it would be a huge mistake to bet that evolving atmospheric changes are benign in their implications \n",
      "for insurers. \n",
      "\n",
      "Don't think, however, that we have lost our taste for risk. We remain prepared to lose $6 billion \n",
      "in a single event, ifwe have been paid appropriately for assuming that risk. We are not willing, though, to \n",
      "take on even very small exposures at prices that don't reflect our evaluation of loss probabilities. \n",
      "Appropriate prices don't guarantee profits in any given year, but inappropriate prices most certainly \n",
      "guarantee eventual losses. Rates have recently fallen because a flood of capital has entered the super-cat \n",
      "field. We have therefore sharply reduced our wind exposures. Our behavior here parallels that which we \n",
      "employ in financial markets: Be fearful when others are greedy, and be greedy when others are fearful. \n",
      "\n",
      "Lloyd's, Equitas and Retroactive Reinsurance \n",
      "\n",
      "Last year - we are getting now to Equitas - Berkshire agreed to enter into a huge retroactive \n",
      "reinsurance contract, a policy that protects an insurer against losses that have already happened, but whose \n",
      "cost is not yet known. I'll give you details of the agreement shortly. But let's first take a journey through \n",
      "insurance history, following the route that led to our deal. \n",
      "\n",
      "Our tale begins around 1688, when Edward Lloyd opened a small coffee house in London. \n",
      "Though no Starbucks, his shop was destined to achieve worldwide fame because of the commercial \n",
      "activities of its clientele - shipowners, merchants and venturesome British capitalists. As these parties \n",
      "sipped Edward's brew, they began to write contracts transferring the risk of a disaster at sea from the \n",
      "owners of ships and their cargo to the capitalists, who wagered that a given voyage would be completed \n",
      "without incident. These capitalists eventually became known as \"underwriters at Lloyd's.\" \n",
      "\n",
      "Though many people believe Lloyd's to be an insurance company, that is not the case. It is \n",
      "instead a place where many member-insurers transact business, just as they did centuries ago. \n",
      "\n",
      "Over time, the underwriters solicited passive investors to join in syndicates. Additionally, the \n",
      "business broadened beyond marine risks into every imaginable form of insurance, including exotic \n",
      "coverages that spread the fame of Lloyd's far and wide. The underwriters left the coffee house, found \n",
      "grander quarters and formalized some rules of association. And those persons who passively backed the \n",
      "underwriters became known as \"names.\" \n",
      "\n",
      "\n",
      "8 \n",
      "\n",
      "\n",
      "Eventually, the names came to include many thousands of people from around the world, who \n",
      "joined expecting to pick up some extra change without effort or serious risk. True, prospective names were \n",
      "always solemnly told that they would have unlimited and everlasting liability for the consequences of their \n",
      "syndicate's underwriting - \"down to the last cufflink,\" as the quaint description went. But that warning \n",
      "came to be viewed as perfunctory. Three hundred years of retained cufflinks acted as a powerful sedative \n",
      "to the names poised to sign up. \n",
      "\n",
      "Then came asbestos. When its prospective costs were added to the tidal wave of environmental \n",
      "and product claims that surfaced in the 1980s, Lloyd's began to implode. Policies written decades earlier - \n",
      "and largely forgotten about - were developing huge losses. No one could intelligently estimate their total, \n",
      "but it was certain to be many tens of billions of dollars. The specter of unending and unlimited losses \n",
      "terrified existing names and scared away prospects. Many names opted for bankruptcy; some even chose \n",
      "suicide. \n",
      "\n",
      "From these shambles, there came a desperate effort to resuscitate Lloyd's. In 1996, the powers \n",
      "that be at the institution allotted £11.1 billion to a new company, Equitas, and made it responsible for \n",
      "paying all claims on policies written before 1993. In effect, this plan pooled the misery of the many \n",
      "syndicates in trouble. Of course, the money allotted could prove to be insufficient - and if that happened, \n",
      "the names remained liable for the shortfall. \n",
      "\n",
      "But the new plan, by concentrating all of the liabilities in one place, had the advantage of \n",
      "eliminating much of the costly intramural squabbling that went on among syndicates. Moreover, the \n",
      "pooling allowed claims evaluation, negotiation and litigation to be handled more intelligently than had been \n",
      "the case previously. Equitas embraced Ben Franklin's thinking: \"We must all hang together, or assuredly \n",
      "we shall hang separately.\" \n",
      "\n",
      "From the start, many people predicted Equitas would eventually fail. But as Ajit and I reviewed \n",
      "the facts in the spring of 2006 - 13 years after the last exposed policy had been written and after the \n",
      "payment of £11.3 billion in claims - we concluded that the patient was likely to survive. And so we \n",
      "decided to offer a huge reinsurance policy to Equitas. \n",
      "\n",
      "Because plenty of imponderables continue to exist, Berkshire could not provide Equitas, and its \n",
      "27,972 names, unlimited protection. But we said - and I'm simplifying - that if Equitas would give us \n",
      "$7.12 billion in cash and securities (this is the float I spoke about), we would pay all of its future claims and \n",
      "expenses up to $13.9 billion. That amount was $5.7 billion above what Equitas had recently guessed its \n",
      "ultimate liabilities to be. Thus the names received a huge - and almost certainly sufficient - amount of \n",
      "future protection against unpleasant surprises. Indeed the protection is so large that Equitas plans a cash \n",
      "payment to its thousands of names, an event few of them had ever dreamed possible. \n",
      "\n",
      "And how will Berkshire fare? That depends on how much \"known\" claims will end up costing us, \n",
      "how many yet-to-be-presented claims will surface and what they will cost, how soon claim payments will \n",
      "be made and how much we earn on the cash we receive before it must be paid out. Ajit and I think the odds \n",
      "are in our favor. And should we be wrong, Berkshire can handle it. \n",
      "\n",
      "Scott Moser, the CEO of Equitas, summarized the transaction neatly: \"Names wanted to sleep \n",
      "easy at night, and we think we've just bought them the world's best mattress.\" \n",
      "\n",
      "Warning: It's time to eat your broccoli - I am now going to talk about accounting matters. I owe \n",
      "this to those Berkshire shareholders who love reading about debits and credits. I hope both of you find this \n",
      "discussion helpful. All others can skip this section; there will be no quiz. \n",
      "\n",
      "Berkshire has done many retroactive transactions - in both number and amount a multiple of such \n",
      "policies entered into by any other insurer. We are the reinsurer of choice for these coverages because the \n",
      "obligations that are transferred to us - for example, lifetime indemnity and medical payments to be made to \n",
      "injured workers - may not be fully satisfied for 50 years or more. No other company can offer the certainty \n",
      "\n",
      "\n",
      "9 \n",
      "\n",
      "\n",
      "that Berkshire can, in terms of guaranteeing the full and fair settlement of these obligations. This fact is \n",
      "important to the original insurer, policyholders and regulators. \n",
      "\n",
      "The accounting procedure for retroactive transactions is neither well known nor intuitive. The \n",
      "best way for shareholders to understand it, therefore, is for us to simply lay out the debits and credits. \n",
      "Charlie and I would like to see this done more often. We sometimes encounter accounting footnotes about \n",
      "important transactions that leave us baffled, and we go away suspicious that the reporting company wished \n",
      "it that way. (For example, try comprehending transactions \"described\" in the old 10-Ks of Enron, even \n",
      "after you know how the movie ended.) \n",
      "\n",
      "So let us summarize our accounting for the Equitas transaction. The major debits will be to Cash \n",
      "and Investments, Reinsurance Recoverable, and Deferred Charges for Reinsurance Assumed (\"DCRA\"). \n",
      "The major credit will be to Reserve for Losses and Loss Adjustment Expense. No profit or loss will be \n",
      "recorded at the inception of the transaction, but underwriting losses will thereafter be incurred annually as \n",
      "the DCRA asset is amortized downward. The amount of the annual amortization charge will be primarily \n",
      "determined by how our end-of-the-year estimates as to the timing and amount of future loss payments \n",
      "compare to the estimates made at the beginning of the year. Eventually, when the last claim has been paid, \n",
      "the DCRA account will be reduced to zero. That day is 50 years or more away. \n",
      "\n",
      "What's important to remember is that retroactive insurance contracts always produce underwriting \n",
      "losses for us. Whether these losses are worth experiencing depends on whether the cash we have received \n",
      "produces investment income that exceeds the losses. Recently our DCRA charges have annually delivered \n",
      "$300 million or so of underwriting losses, which have been more than offset by the income we have \n",
      "realized through use of the cash we received as a premium. Absent new retroactive contracts, the amount \n",
      "of the annual charge would normally decline over time. After the Equitas transaction, however, the annual \n",
      "DCRA cost will initially increase to about $450 million a year. This means that our other insurance \n",
      "operations must generate at least that much underwriting gain for our overall float to be cost-free. That \n",
      "amount is quite a hurdle but one that I believe we will clear in many, if not most, years. \n",
      "\n",
      "\n",
      "Aren't you glad that I promised you there would be no quiz? \n",
      "\n",
      "\n",
      "Manufacturing, Service and Retailing Operations \n",
      "\n",
      "Our activities in this part of Berkshire cover the waterfront. Let's look, though, at a summary \n",
      "balance sheet and earnings statement for the entire group. \n",
      "\n",
      "\n",
      "Balance Sheet 12/31/06 (in millions) \n",
      "\n",
      "\n",
      "Assets \n",
      "\n",
      "Cash and equivalents \n",
      "\n",
      "Accounts and notes receivable . \n",
      "\n",
      "Inventory \n",
      "\n",
      "Other current assets \n",
      "\n",
      "Total current assets \n",
      "\n",
      "\n",
      "& 1,543 \n",
      "3,793 \n",
      "5,257 \n",
      "363 \n",
      "10,956 \n",
      "\n",
      "\n",
      "Liabilities and Equity \n",
      "\n",
      "Notes payable $ 1,468 \n",
      "\n",
      "Other current liabilities 6,635 \n",
      "\n",
      "Total current liabilities 8,103 \n",
      "\n",
      "\n",
      "Goodwill and other intangibles 13,314 Deferred taxes 540 \n",
      "\n",
      "Fixed assets 8,934 Term debt and other liabilities... 3,014 \n",
      "\n",
      "Other assets 1,168 Equity 22,715 \n",
      "\n",
      "$34.372 $34.372 \n",
      "\n",
      "\n",
      "10 \n",
      "\n",
      "\n",
      "Earnings Statement (in millions) \n",
      "\n",
      "\n",
      "\n",
      "2006 \n",
      "\n",
      "2005 \n",
      "\n",
      "2004 \n",
      "\n",
      "Revenues \n",
      "\n",
      "$52,660 \n",
      "\n",
      "$46,896 \n",
      "\n",
      "$44,142 \n",
      "\n",
      "Operating expenses (including depreciation of $823 in 2006, \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "$699 in 2005 and $676 in 2004) \n",
      "\n",
      "49,002 \n",
      "\n",
      "44,190 \n",
      "\n",
      "41,604 \n",
      "\n",
      "Interest expense \n",
      "\n",
      "132 \n",
      "\n",
      "83 \n",
      "\n",
      "57 \n",
      "\n",
      "Pre-tax earnings \n",
      "\n",
      "3,526* \n",
      "\n",
      "2,623* \n",
      "\n",
      "2,481* \n",
      "\n",
      "Income taxes and minority interests \n",
      "\n",
      "1,395 \n",
      "\n",
      "977 \n",
      "\n",
      "941 \n",
      "\n",
      "Net income \n",
      "\n",
      "$ 2,131 \n",
      "\n",
      "$ 1.646 \n",
      "\n",
      "$ 1.540 \n",
      "\n",
      "\n",
      "*Does not include purchase-accounting adjustments. \n",
      "\n",
      "This motley group, which sells products ranging from lollipops to motor homes, earned a pleasing \n",
      "25% on average tangible net worth last year. It's noteworthy also that these operations used only minor \n",
      "financial leverage in achieving that return. Clearly we own some terrific businesses. We purchased many \n",
      "of them, however, at large premiums to net worth - a point reflected in the goodwill item shown on the \n",
      "balance sheet - and that fact reduces the earnings on our average carrying value to 10.8%. \n",
      "\n",
      "Here are a few newsworthy items about companies in this sector: \n",
      "\n",
      "• Bob Shaw, a remarkable entrepreneur who from a standing start built Shaw Industries into the \n",
      "country's largest carpet producer, elected last year, at age 75, to retire. To succeed him, Bob \n",
      "recommended Vance Bell, a 31-year veteran at Shaw, and Bob, as usual, made the right call. \n",
      "Weakness in housing has caused the carpet business to slow. Shaw, however, remains a \n",
      "powerhouse and a major contributor to Berkshire's earnings. \n",
      "\n",
      "• MiTek, a manufacturer of connectors for roof trusses at the time we purchased it in 2001, is \n",
      "developing into a mini-conglomerate. At the rate it is growing, in fact, \"mini\" may soon be \n",
      "inappropriate. In purchasing MiTek for $420 million, we lent the company $200 million at 9% \n",
      "and bought $198 million of stock, priced at $10,000 per share. Additionally, 55 employees bought \n",
      "2,200 shares for $22 million. Each employee paid exactly the same price that we did, in most \n",
      "cases borrowing money to do so. \n",
      "\n",
      "And are they ever glad they did! Five years later, MiTek' s sales have tripled and the stock is \n",
      "valued at $71,699 per share. Despite its making 14 acquisitions, at a cost of $291 million, MiTek \n",
      "has paid off its debt to Berkshire and holds $35 million of cash. We celebrated the fifth \n",
      "anniversary of our purchase with a party in July. I told the group that it would be embarrassing if \n",
      "MiTek's stock price soared beyond that of Berkshire \"A\" shares. Don't be surprised, however, if \n",
      "that happens (though Charlie and I will try to make our shares a moving target). \n",
      "\n",
      "• Not all of our businesses are destined to increase profits. When an industry's underlying \n",
      "economics are crumbling, talented management may slow the rate of decline. Eventually, though, \n",
      "eroding fundamentals will overwhelm managerial brilliance. (As a wise friend told me long ago, \n",
      "\"If you want to get a reputation as a good businessman, be sure to get into a good business.\") And \n",
      "fundamentals are definitely eroding in the newspaper industry, a trend that has caused the profits \n",
      "of our Buffalo News to decline. The skid will almost certainly continue. \n",
      "\n",
      "When Charlie and I were young, the newspaper business was as easy a way to make huge returns \n",
      "as existed in America. As one not-too-bright publisher famously said, \"I owe my fortune to two \n",
      "great American institutions: monopoly and nepotism.\" No paper in a one -paper city, however bad \n",
      "the product or however inept the management, could avoid gushing profits. \n",
      "\n",
      "The industry's staggering returns could be simply explained. For most of the 20 th Century, \n",
      "newspapers were the primary source of information for the American public. Whether the subject \n",
      "was sports, finance, or politics, newspapers reigned supreme. Just as important, their ads were the \n",
      "easiest way to find job opportunities or to learn the price of groceries at your town's supermarkets. \n",
      "\n",
      "\n",
      "11 \n",
      "\n",
      "\n",
      "The great majority of families therefore felt the need for a paper every day, but understandably \n",
      "most didn't wish to pay for two. Advertisers preferred the paper with the most circulation, and \n",
      "readers tended to want the paper with the most ads and news pages. This circularity led to a law \n",
      "of the newspaper jungle: Survival of the Fattest. \n",
      "\n",
      "Thus, when two or more papers existe\n"
     ]
    }
   ],
   "source": [
    "test_text = text[:int(len(text) * 0.1)]\n",
    "# text_2=test_text[:200]\n",
    "print(\"Input:\",test_text)\n",
    "tokens=generate(test_text,vocab,\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Venky Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Venky Model - without stopwords - lr=6e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,loss= model_venky(tokens[0:25].reshape(1,-1),tokens[1:26].reshape(1,-1))\n",
    "print(\"Perplexity\",np.exp(loss.cpu().tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Venky model - lr=6e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity 1.6726893632587998\n"
     ]
    }
   ],
   "source": [
    "_,loss= model_venky(tokens[0:25].reshape(1,-1),tokens[1:26].reshape(1,-1))\n",
    "print(\"Perplexity\",np.exp(loss.cpu().tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Venky Model - lr=e4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity 1.3393933768307407\n"
     ]
    }
   ],
   "source": [
    "_,loss= model_venky(tokens[0:25].reshape(1,-1),tokens[1:26].reshape(1,-1))\n",
    "print(\"Perplexity\",np.exp(loss.cpu().tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Venky Model - lr=e5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity 4.489131593828046\n"
     ]
    }
   ],
   "source": [
    "_,loss= model_venky(tokens[0:25].reshape(1,-1),tokens[1:26].reshape(1,-1))\n",
    "print(\"Perplexity\",np.exp(loss.cpu().tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prof. Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prof. Model (lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity 1.3162753617234262\n"
     ]
    }
   ],
   "source": [
    "_,loss= model(tokens[0:25].reshape(1,-1),tokens[1:26].reshape(1,-1))\n",
    "print(\"Perplexity\",np.exp(loss.cpu().tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prof. Model (lr=6e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity 1.3775242324140997\n"
     ]
    }
   ],
   "source": [
    "_,loss= model(tokens[0:25].reshape(1,-1),tokens[1:26].reshape(1,-1))\n",
    "print(\"Perplexity\",np.exp(loss.cpu().tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.D) Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(text, vocab,device):\n",
    "    cleaned_data=clean_text(text)\n",
    "    # print(cleaned_data)\n",
    "    indx_id=[]\n",
    "    input=[]\n",
    "    for text in cleaned_data:\n",
    "        try:\n",
    "            indx_id.append(vocab[text])\n",
    "            input.append(text)\n",
    "        except:\n",
    "            print(\"Not present in vocab:\",text)\n",
    "    tensor = torch.tensor(indx_id, dtype=torch.long, device=device)\n",
    "    return tensor, input\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_popular = [\n",
    "    \"Rule No. 1 is never lose money. Rule No. 2 is never forget Rule No. 1.\",\n",
    "    \"It's far better to buy a wonderful company at a fair price than a fair company at a wonderful price\",\n",
    "    \"Opportunities come infrequently. When it rains gold, put out the bucket, not the thimble\",\n",
    "    \"We simply attempt to be fearful when others are greedy and to be greedy only when others are fearful\",\n",
    "    \"The most important quality for an investor is temperament, not intellect. You need a temperament that neither derives great pleasure from being with the crowd or against the crowd\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: The most important quality for an investor is temperament, not intellect. You need a temperament that neither derives great pleasure from being with the crowd or against the crowd\n",
      "************************************************************************************************************************************************************************************************************************\n",
      "Input tokens: ['the', 'most', 'import', 'qualiti', 'for', 'an', 'investor', 'is', 'tempera', 'not']\n",
      "************************************************************************************************************************************************************************************************************************\n",
      "Prof. Model:  the most import qualiti for an investor is tempera not ga had million at the qwest s stand will be surpris to wwwgeicocom lord 8 stock 198 compound annual an charli and i we earn repres no when we bought justin industri but we are usual ignor instead consult and to 96 home the futur need 500 as a whole beyond it did member of the one that pull ahead perform rel you use sever area with their lend standard their futur leverag acquisit in septemb 2008 though onli one as well piccolo combin mani other insur but nevertheless though i would like to st with two interest and sold hi letter is unlik ha both our in year and great your neighbor wonder and after midamerican purchas inde a whole in most should take is not true of the countri lead author in the 2008 of the door to work with minor interest in dividend in the us lou do better some year we financ clayton 187 206 major oper sector each differ from the other in anticip in oper that our overwhelm mani year he most nearbi norman beck a remark magician from dalla will bewild onlook addit we will have bob hamman and sharon osberg two of the world top bridg expert avail to play bridg with our sharehold on sunday afternoon to add to the sunday fun at borsheim ariel hsing will play tabl tenni pingpong to the uniniti from 1 pm to 4 pm against anyon brave enough to take her on ariel though onli 11 is rank number one among girl under 16 in the us and number 1 among both boy and girl under 12 the week i turn 75 i play ariel then 9 and bare tall enough to see across the tabl think i would take it easi on her so as not to crush her young spirit instead she crush me ive sinc devis a plan that will give me a chanc against her at 1 pm on sunday i will initi play with a 2point game against ariel if i somehow win the first point i will then feign injuri and claim victori after thi strenuou encount wear ariel down our sharehold can then tri their luck against her gorat will again be open exclus for berkshir sharehold on sunday may 6 th and will be serv from 4 pm until 10 pm pleas rememb that to come to gorat on that day you must have a reserv to make one call 4025513733 on april 1 st but not befor in the 20062007 school year 35 univers class includ one from ibmec in brazil will come to omaha for session with no like make money for that ha been one will simpli to get one at charli offic in the success of most buyer attend by industri favor from 382000 that type would be rewritten of my job of million an averag of 300 crew whichev way you our low all of you are hurt when stock rise you benefit when stock swoon emot howev too often complic the matter most peopl includ those who will be net buyer in the futur take comfort in see stock price advanc these sharehold resembl a commut who rejoic after the price of ga increas simpli becaus hi tank contain a day suppli charli and i dont expect to win mani of you over to our way of think weve observ enough human behavior to know the futil of that but we do want you to be awar of our person calculu and here a confess is in order in my earli day i too rejoic when the market rose then i read chapter eight of ben graham the intellig investor the chapter deal with how investor should view fluctuat in stock price immedi the scale fell from my eye and low price becam my friend pick up that book wa one of the luckiest moment in my life in the end the success of our ibm invest will be determin primarili by it futur earn but an import secondari factor will be how mani share the compani purchas with the substanti sum it is like to devot to thi activ and if repurchas ever reduc the ibm share the number decis that commit cultur into busi forward and backward is a ration decisionmak and a joy to work with becaus of acquisit clayton now employ 14787 peopl compar to 6661 at the time of our purchas we have two leas oper huge invest held but the return we deserv on the fund we invest in earlier day charli and i shun capitalintens busi such as public util inde the best busi by far for owner continu to be those that have high return on capit and that requir littl increment invest to grow we are fortun to own a number of such busi and we would love to buy more anticip howev that berkshir will gener everincreas amount of cash we are today quit will to enter busi that regularli requir larg capit expenditur we expect onli that these busi have reason expect of earn decent return on the increment sum they invest if our expect are met and we believ that in turn what they will fall below the moment of the industri perhap be more no money the opportun and an index sinc we purchas the stock market averag way to tell you with auto insur quot in most case geico will be abl to give you a sharehold discount usual 8 thi special offer is permit by 44 of the 51 jurisdict in which we oper one supplement point the discount is not addit if you qualifi for anoth such as that given certain group bring the detail of your exist insur and check out whether we can save you money for at least 50 of you i believ we can and while your at it sign up for the new geico credit card it the one i\n",
      "************************************************************************************************************************************************************************************************************************\n",
      "Venky's Model:  the most import qualiti for an investor is tempera not hard of my longtim friend of the time or bank and then learn where credit is the case we will take on all of the acquisit of my errant purchas berkshir newer sharehold quit a desper effort geico in a few year ago combin damag goldman sach and equiti pretax earn 2972 2459 2461 valu kim as perman home and omaha by be deliv for payment to be sure berkshir ha achiev sinc retain an estim in 83 i would be use black veri gradual becaus perform be adequ summar the differ to be made the right individu buyer in the right person into the sp 500 should put me an be contempl in board more those shown not billion of cours uncertain but nevertheless these stock usual sell at berkshir howev there is permit compar to be forc and help you save ratio alongsid a iscar for the salari while charli and i view them howev let had acquir march 21 2009 our manag will be levi against by toni geico the insur benefit he negoti amount of 21210 can handl these or someon from the high of see candi thi collectnow paylat model leav us hold larg sum money we call float that will eventu go to other meanwhil we get to invest thi float for berkshir benefit though individu polici and claim come and go the amount of float we hold remain remark stabl in relat to premium volum consequ as our busi grow so doe our float and how we have grown as the follow tabl show year float in mil 1970 39 1980 237 1990 1632 2000 27871 2010 65832 2011 70571 it unlik that our float will grow much if at all from it current level that mainli becaus hous start fat to sam walton aris as one point the second it meant for late 1960 have yet then do so rest bad news page america these investe last year ago walter who come against ariel then reawak with their mind drive after the stock market well present pershar in million policyhold help were keep were keep it will be embarrass from the follow tabl clayton earn are small when it huge neg in 2005 and annuiti wonder gentleman seem odd are in septemb oper by the world jacob and i did howev both work as alway particularli at the time and i believ that those that rail and quickli vanish on the origin purchas of the smaller compani hi wallet is get no restrict them around look for a park meter with some unexpir time that wa a magic moment for me i knew then that jack wa go to be my kind of manag when berkshir purchas jack two insur they had float of 17 million weve regularli offer a long explan of float in earlier report which you can mortgag that it will understand their profit margin bring huge increment equiti but of your book valu 11 110 a knew thi amount for thi amount have you decid from coke 2009 your safeti repres a manufactur servic and also to berkshir sharehold 125 4 there credenti second other current asset cash and invest are like howev are like to and invest reinsur recover and defer charg for reinsur assum dcra the major credit will be to reserv for loss and loss adjust long on the way we expect default remain reason had record of employ that geico s largest to obtain the million from 17 million in loss which came from the earlier day of the sp 500 and we have earn money my admir for loss of the gain loss yet to requir style box such as longshort macro intern equiti at the 20 the 20 the tabl someth both board and we knew we have a nonfriday disclosur we had for our in sentenc there ha been invalu in the import deal take fund the abil to us will have onli perpetu foundat befor it earlier report inde the first part of the sentenc while make no mention whatsoev of it end i regard thi as terribl journal misinform reader or viewer may well have thought that charli and i were forecast bad thing for the stock market though we had not onli profit in that sentenc but also elsewher also elsewher made it clear we werent predict the market at all ani investor who were misl by the sensationalist paid a big price the dow close the day of the letter at 7063 and finish the year at 10428 given a few experi weve had like that you can understand whi i prefer that our commun with you remain as direct and unabridg as possibl let move to the specif of berkshir oper we have four major oper sector each differ from the other in balanc sheet and incom account characterist therefor lump them togeth as is standard in financi statement imped analysi so well present them as four separ busi which is how charli and i view them 5 insur our propertycasualti pc insur busi ha been the engin behind berkshir growth railroad own berkshir chief invest great manag by it particularli at today see candi sinc present i return on a huge reinsur of insur busi we understood loss thi debt to consid by have been poor also appli to earn say our 68 8015 11123 him thi book valu moreov we manag scale we keep our equiti put at least 60 util rel find smart except abl and tax on sculpt 15 2006 184 2000 50229 19902000 245 2010 592604 20002010 long ago acquisit of burlington compound annual rent on sever recess live that exactli what manag them had i have larg sale that have botch compound growth credit it ha incur of valuedestroy busi there her were down dollar 453 japanes yen 1 time horizon activ the american express 88 of the american economi circulatori system oblig to constantli maintain and improv our 23000\n"
     ]
    }
   ],
   "source": [
    "print(\"Original: The most important quality for an investor is temperament, not intellect. You need a temperament that neither derives great pleasure from being with the crowd or against the crowd\")\n",
    "print(\"************************************************************************************************************************************************************************************************************************\")\n",
    "text_1 =\"The most important quality for an investor is temperament, not\"\n",
    "tokens,input=generate(text_1,vocab,\"cuda\")\n",
    "print(\"Input tokens:\", input)\n",
    "print(\"************************************************************************************************************************************************************************************************************************\")\n",
    "tokens=tokens.reshape(1,-1) #BxL\n",
    "outputs = model.generate(context_token_ids=tokens,max_new_tokens=1000)\n",
    "# outputs = model.generate(context_token_ids,max_new_tokens=1000)\n",
    "print(\"Prof. Model: \",outputs)\n",
    "\n",
    "print(\"************************************************************************************************************************************************************************************************************************\")\n",
    "\n",
    "# print(tokens.reshape(-1,1).shape)\n",
    "\n",
    "outputs = model_venky.generate(context_token_ids=tokens,max_new_tokens=1000)\n",
    "# outputs = model.generate(context_token_ids,max_new_tokens=1000)\n",
    "print(\"Venky's Model: \",outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: BERKSHIRE HATHAWAY INC. \n",
      "\n",
      "\n",
      "\n",
      "To the Shareholders of Berkshire Hathaway Inc.: \n",
      "\n",
      "Our gain in net worth during 2006 was $16.9 billion, which increased the per-share book value of \n",
      "both our Class A and Class B stock by 18.4%. Over the last 42 years (that is, since present management \n",
      "took over) book value has grown from $19 to $70,281, a rate of 21.4% compounded annually.* \n",
      "\n",
      "We believe that $16.9 billion is a record for a one-year gain in net worth - more than has ever \n",
      "been booked by any American business, leaving aside boosts that have occurred because of mergers (e.g., \n",
      "AOL's purchase of Time Warner). Of course, Exxon Mobil and other companies earn far more than \n",
      "Berkshire, but their earnings largely go to dividends and/or repurchases, rather than to building net worth. \n",
      "\n",
      "All that said, a confession about our 2006 gain is in order. Our most important business, \n",
      "insurance, benefited from a large dose of luck: Mother Nature, bless her heart, went on vacation. After \n",
      "hammering us with hurricanes in 2004 and 2005 - storms that caused us to lose a bundle on super-cat \n",
      "insurance - she just vanished. Last year, the red ink from this activity turned black - very black. \n",
      "\n",
      "In addition, the great majority of our 73 businesses did outstandingly well in 2006. Let me focus \n",
      "for a moment on one of our largest operations, GEICO. What management accomplished there was simply \n",
      "extraordinary. \n",
      "\n",
      "As I've told you before, Tony Nicely, GEICO' s CEO, went to work at the company 45 years ago, \n",
      "two months after turning 18. He became CEO in 1992, and from then on the company's growth exploded. \n",
      "In addition, Tony has delivered staggering productivity gains in recent years. Between yearend 2003 and \n",
      "yearend 2006, the number of GEICO policies increased from 5.7 million to 8.1 million, a jump of 42%. \n",
      "Yet during that same period, the company's employees (measured on a fulltime-equivalent basis) fell 3.5%. \n",
      "So productivity grew 47%. And GEICO didn't start fat. \n",
      "\n",
      "That remarkable gain has allowed GEICO to maintain its all-important position as a low-cost \n",
      "producer, even though it has dramatically increased advertising expenditures. Last year GEICO spent $631 \n",
      "million on ads, up from $238 million in 2003 (and up from $31 million in 1995, when Berkshire took \n",
      "control). Today, GEICO spends far more on ads than any of its competitors, even those much larger. We \n",
      "will continue to raise the bar. \n",
      "\n",
      "Last year I told you that if you had a new son or grandson to be sure to name him Tony. But Don \n",
      "Keough, a Berkshire director, recently had a better idea. After reviewing GEICO's performance in 2006, \n",
      "he wrote me, \"Forget births. Tell the shareholders to immediately change the names of their present \n",
      "children to Tony or Antoinette.\" Don signed his letter \"Tony.\" \n",
      "\n",
      "Charlie Munger - my partner and Berkshire's vice chairman - and I run what has turned out to be \n",
      "a big business, one with 217,000 employees and annual revenues approaching $100 billion. We certainly \n",
      "didn't plan it that way. Charlie began as a lawyer, and I thought of myself as a security analyst. Sitting in \n",
      "those seats, we both grew skeptical about the ability of big entities of any type to function well. Size seems \n",
      "to make many organizations slow-thinking, resistant to change and smug. In Churchill's words: \"We shape \n",
      "our buildings, and afterwards our buildings shape us.\" Here's a telling fact: Of the ten non-oil companies \n",
      "having the largest market capitalization in 1965 - titans such as General Motors, Sears, DuPont and \n",
      "Eastman Kodak - only one made the 2006 list. \n",
      "\n",
      "\n",
      "\n",
      "*A11 per-share figures used in this report apply to Berkshire's A shares. Figures for the B shares \n",
      "are 1/30* of those shown for the A. \n",
      "\n",
      "\n",
      "3 \n",
      "\n",
      "\n",
      "In fairness, we've seen plenty of successes as well, some truly outstanding. There are many giant- \n",
      "company managers whom I greatly admire; Ken Chenault of American Express, Jeff Immelt of G.E. and \n",
      "Dick Kovacevich of Wells Fargo come quickly to mind. But I don't think I could do the management job \n",
      "they do. And I know I wouldn't enjoy many of the duties that come with their positions - meetings, \n",
      "speeches, foreign travel, the charity circuit and governmental relations. For me, Ronald Reagan had it \n",
      "right: \"It's probably true that hard work never killed anyone - but why take the chance?\" \n",
      "\n",
      "So I've taken the easy route, just sitting back and working through great managers who run their \n",
      "own shows. My only tasks are to cheer them on, sculpt and harden our corporate culture, and make major \n",
      "capital-allocation decisions. Our managers have returned this trust by working hard and effectively. \n",
      "\n",
      "For their performance over the last 42 years - and particularly for 2006 - Charlie and I thank \n",
      "\n",
      "them. \n",
      "\n",
      "Yardsticks \n",
      "\n",
      "Charlie and I measure Berkshire's progress and evaluate its intrinsic value in a number of ways. \n",
      "No single criterion is effective in doing these jobs, and even an avalanche of statistics will not capture some \n",
      "factors that are important. For example, it's essential that we have managers much younger than I available \n",
      "to succeed me. Berkshire has never been in better shape in this regard - but I can't prove it to you with \n",
      "numbers. \n",
      "\n",
      "There are two statistics, however, that are of real importance. The first is the amount of \n",
      "investments (including cash and cash-equivalents) that we own on a per-share basis. Arriving at this figure, \n",
      "we exclude investments held in our finance operation because these are largely offset by borrowings. \n",
      "Here's the record since present management acquired control of Berkshire: \n",
      "\n",
      "\n",
      "Year Per-Share Investments * \n",
      "\n",
      "1965 $ 4 \n",
      "\n",
      "1975 159 \n",
      "\n",
      "1985 2,407 \n",
      "\n",
      "1995 21,817 \n",
      "\n",
      "2006 $80,636 \n",
      "\n",
      "Compound Growth Rate 1965-2006 27.5% \n",
      "\n",
      "Compound Growth Rate 1995-2006 12.6% \n",
      "\n",
      "\n",
      "*Net of minority interests \n",
      "\n",
      "In our early years we put most of our retained earnings and insurance float into investments in \n",
      "marketable securities. Because of this emphasis, and because the securities we purchased generally did \n",
      "well, our growth rate in investments was for a long time quite high. \n",
      "\n",
      "Over the years, however, we have focused more and more on the acquisition of operating \n",
      "businesses. Using our funds for these purchases has both slowed our growth in investments and accelerated \n",
      "our gains in pre-tax earnings from non-insurance businesses, the second yardstick we use. Here's how \n",
      "those earnings have looked: \n",
      "\n",
      "\n",
      "4 \n",
      "\n",
      "\n",
      "Year \n",
      "\n",
      "\n",
      "Pre-Tax Earnings Per Share * \n",
      "\n",
      "\n",
      "1965 $ 4 \n",
      "\n",
      "1975 4 \n",
      "\n",
      "1985 52 \n",
      "\n",
      "1995 175 \n",
      "\n",
      "2006 $3,625 \n",
      "\n",
      "Compound Growth Rate 1965-2006 17.9% \n",
      "\n",
      "Compound Growth Rate 1995-2006 31.7% \n",
      "\n",
      "\n",
      "*Excluding purchase-accounting adjustments and net of minority interests \n",
      "\n",
      "Last year we had a good increase in non-insurance earnings - 38%. Large gains from here on in, \n",
      "though, will come only if we are able to make major, and sensible, acquisitions. That will not be easy. We \n",
      "do, however, have one advantage: More and more, Berkshire has become \"the buyer of choice\" for \n",
      "business owners and managers. Initially, we were viewed that way only in the U.S. (and more often than \n",
      "not by private companies). We've long wanted, nonetheless, to extend Berkshire's appeal beyond U.S. \n",
      "borders. And last year, our globe-trotting finally got underway. \n",
      "\n",
      "Acquisitions \n",
      "\n",
      "We began 2006 by completing the three acquisitions pending at yearend 2005, spending about $6 \n",
      "billion for PacifiCorp, Business Wire and Applied Underwriters. All are performing very well. \n",
      "\n",
      "The highlight of the year, however, was our July 5 th acquisition of most of ISCAR, an Israeli \n",
      "company, and our new association with its chairman, Eitan Wertheimer, and CEO, Jacob Harpaz. The \n",
      "story here began on October 25, 2005, when I received a 114-page letter from Eitan, of whom I then knew \n",
      "nothing. The letter began, \"I am writing to introduce you to ISCAR,\" and proceeded to describe a cutting- \n",
      "tool business carried on in 61 countries. Then Eitan wrote, \"We have for some time considered the issues \n",
      "of generational transfer and ownership that are typical for large family enterprises, and have given much \n",
      "thought to ISCAR' s future. Our conclusion is that Berkshire Hathaway would be the ideal home for \n",
      "ISCAR. We believe that ISCAR would continue to thrive as a part of your portfolio of businesses.\" \n",
      "\n",
      "Overall, Eitan' s letter made the quality of the company and the character of its management leap \n",
      "off the page. It also made me want to learn more, and in November, Eitan, Jacob and ISCAR' s CFO, \n",
      "Danny Goldman, came to Omaha. A few hours with them convinced me that if we were to make a deal, we \n",
      "would be teaming up with extraordinarily talented managers who could be trusted to run the business after \n",
      "a sale with all of the energy and dedication that they had exhibited previously. However, having never \n",
      "bought a business based outside of the U.S. (though I had bought a number of foreign stocks), I needed to \n",
      "get educated on some tax and jurisdictional matters. With that task completed, Berkshire purchased 80% of \n",
      "ISCAR for $4 billion. The remaining 20% stays in the hands of the Wertheimer family, making it our \n",
      "valued partner. \n",
      "\n",
      "ISCAR' s products are small, consumable cutting tools that are used in conjunction with large and \n",
      "expensive machine tools. It's a business without magic except for that imparted by the people who run it. \n",
      "But Eitan, Jacob and their associates are true managerial magicians who constantly develop tools that make \n",
      "their customers' machines more productive. The result: ISCAR makes money because it enables its \n",
      "customers to make more money. There is no better recipe for continued success. \n",
      "\n",
      "\n",
      "5 \n",
      "\n",
      "\n",
      "In September, Charlie and I, along with five Berkshire associates, visited ISCAR in Israel. We - \n",
      "and I mean every one of us - have never been more impressed with any operation. At ISCAR, as \n",
      "throughout Israel, brains and energy are ubiquitous. Berkshire shareholders are lucky to have joined with \n",
      "Eitan, Jacob, Danny and their talented associates. \n",
      "\n",
      "A few months later, Berkshire again became \"the buyer of choice\" in a deal brought to us by my \n",
      "friend, John Roach, of Fort Worth. John, many of you will remember, was Chairman of Justin Industries, \n",
      "which we bought in 2000. At that time John was helping John Justin, who was terminally ill, find a \n",
      "permanent home for his company. John Justin died soon after we bought Justin Industries, but it has since \n",
      "been run exactly as we promised him it would be. \n",
      "\n",
      "Visiting me in November, John Roach brought along Paul Andrews, Jr., owner of about 80% of \n",
      "TTI, a Fort Worth distributor of electronic components. Over a 35-year period, Paul built TTI from \n",
      "$112,000 of sales to $1.3 billion. He is a remarkable entrepreneur and operator. \n",
      "\n",
      "Paul, 64, loves running his business. But not long ago he happened to witness how disruptive the \n",
      "death of a founder can be both to a private company's employees and the owner's family. What starts out \n",
      "as disruptive, furthermore, often evolves into destructive. About a year ago, therefore, Paul began to think \n",
      "about selling TTI. His goal was to put his business in the hands of an owner he had carefully chosen, rather \n",
      "than allowing a trust officer or lawyer to conduct an auction after his death. \n",
      "\n",
      "Paul rejected the idea of a \"strategic\" buyer, knowing that in the pursuit of \"synergies,\" an owner \n",
      "of that type would be apt to dismantle what he had so carefully built, a move that would uproot hundreds of \n",
      "his associates (and perhaps wound TTI's business in the process). He also ruled out a private equity firm, \n",
      "which would very likely load the company with debt and then flip it as soon as possible. \n",
      "\n",
      "That left Berkshire. Paul and I met on the morning of November 15 th and made a deal before \n",
      "lunch. Later he wrote me: \"After our meeting, I am confident that Berkshire is the right owner for TTI . . . \n",
      "I am proud of our past and excited about our future.\" And so are Charlie and I. \n",
      "\n",
      "We also made some \"tuck-in\" acquisitions during 2006 at Fruit of the Loom (\"Fruit\"), MiTek, \n",
      "CTB, Shaw and Clayton. Fruit made the largest purchases. First, it bought Russell Corp., a leading \n",
      "producer of athletic apparel and uniforms for about $1.2 billion (including assumed debt) and in December \n",
      "it agreed to buy the intimate apparel business of VF Corp. Together, these acquisitions add about $2.2 \n",
      "billion to Fruit's sales and bring with them about 23,000 employees. \n",
      "\n",
      "Charlie and I love it when we can acquire businesses that can be placed under managers, such as \n",
      "John Holland at Fruit, who have already shown their stuff at Berkshire. MiTek, for example, has made 14 \n",
      "acquisitions since we purchased it in 2001, and Gene Toombs has delivered results from these deals far in \n",
      "excess of what he had predicted. In effect, we leverage the managerial talent already with us by these tuck- \n",
      "in deals. We will make many more. \n",
      "\n",
      "We continue, however, to need \"elephants\" in order for us to use Berkshire's flood of incoming \n",
      "cash. Charlie and I must therefore ignore the pursuit of mice and focus our acquisition efforts on much \n",
      "bigger game. \n",
      "\n",
      "Our exemplar is the older man who crashed his grocery cart into that of a much younger fellow \n",
      "while both were shopping. The elderly man explained apologetically that he had lost track of his wife and \n",
      "was preoccupied searching for her. His new acquaintance said that by coincidence his wife had also \n",
      "wandered off and suggested that it might be more efficient if they jointly looked for the two women. \n",
      "Agreeing, the older man asked his new companion what his wife looked like. \"She's a gorgeous blonde,\" \n",
      "the fellow answered, \"with a body that would cause a bishop to go through a stained glass window, and \n",
      "she's wearing tight white shorts. How about yours?\" The senior citizen wasted no words: \"Forget her, \n",
      "we'll look for yours.\" \n",
      "\n",
      "\n",
      "6 \n",
      "\n",
      "\n",
      "What we are looking for is described on page 25. If you have an acquisition candidate that fits, \n",
      "call me - day or night. And then watch me shatter a stained glass window. \n",
      "\n",
      "Now, let's examine the four major operating sectors of Berkshire. Lumping their financial figures \n",
      "together impedes analysis. So we'll look at them as four separate businesses, starting with the all- \n",
      "important insurance group. \n",
      "\n",
      "Insurance \n",
      "\n",
      "Next month marks the 40 th anniversary of our entrance into the insurance business. It was on \n",
      "March 9, 1967, that Berkshire purchased National Indemnity and its companion company, National Fire & \n",
      "Marine, from Jack Ringwalt for $8.6 million. \n",
      "\n",
      "Jack was a long-time friend of mine and an excellent, but somewhat eccentric, businessman. For \n",
      "about ten minutes every year he would get the urge to sell his company. But those moods - perhaps \n",
      "brought on by a tiff with regulators or an unfavorable jury verdict - quickly vanished. \n",
      "\n",
      "In the mid-1960s, I asked investment banker Charlie Heider, a mutual friend of mine and Jack's, \n",
      "to alert me the next time Jack was \"in heat.\" When Charlie's call came, I sped to meet Jack. We made a \n",
      "deal in a few minutes, with me waiving an audit, \"due diligence\" or anything else that would give Jack an \n",
      "opportunity to reconsider. We just shook hands, and that was that. \n",
      "\n",
      "When we were due to close the purchase at Charlie's office, Jack was late. Finally arriving, he \n",
      "explained that he had been driving around looking for a parking meter with some unexpired time. That was \n",
      "a magic moment for me. I knew then that Jack was going to be my kind of manager. \n",
      "\n",
      "When Berkshire purchased Jack's two insurers, they had \"float\" of $17 million. We've regularly \n",
      "offered a long explanation of float in earlier reports, which you can read on our website. Simply put, float \n",
      "is money we hold that is not ours but which we get to invest. \n",
      "\n",
      "At the end of 2006, our float had grown to $50.9 billion, and we have since written a huge \n",
      "retroactive reinsurance contract with Equitas - which I will describe in the next section - that boosts float \n",
      "by another $7 billion. Much of the gain we've made has come through our acquisition of other insurers, \n",
      "but we've also had outstanding internal growth, particularly at Ajit Jain's amazing reinsurance operation. \n",
      "Naturally, I had no notion in 1967 that our float would develop as it has. There's much to be said for just \n",
      "putting one foot in front of the other every day. \n",
      "\n",
      "The float from retroactive reinsurance contracts, of which we have many, automatically drifts \n",
      "down over time. Therefore, it will be difficult for us to increase float in the future unless we make new \n",
      "acquisitions in the insurance field. Whatever its size, however, the all-important cost of Berkshire's float \n",
      "over time is likely to be significantly below that of the industry, perhaps even falling to less than zero. \n",
      "Note the words \"over time.\" There will be bad years periodically. You can be sure of that. \n",
      "\n",
      "In 2006, though, everything went right in insurance - really right. Our managers - Tony Nicely \n",
      "(GEICO), Ajit Jain (B-H Reinsurance), Joe Brandon and Tad Montross (General Re), Don Wurster \n",
      "(National Indemnity Primary), Tom Nerney (U.S. Liability), Tim Kenesey (Medical Protective), Rod \n",
      "Eldred (Homestate Companies and Cypress), Sid Ferenc and Steve Menzies (Applied Underwriters), John \n",
      "Kizer (Central States) and Don Towle (Kansas Bankers Surety) - simply shot the lights out. When I recite \n",
      "their names, I feel as if I'm at Cooperstown, reading from the Hall of Fame roster. Of course, the overall \n",
      "insurance industry also had a terrific year in 2006. But our managers delivered results generally superior to \n",
      "those of their competitors. \n",
      "\n",
      "\n",
      "7 \n",
      "\n",
      "\n",
      "Below is the tally on our underwriting and float for each major sector of insurance. Enjoy the \n",
      "view, because you won't soon see another like it. \n",
      "\n",
      "\n",
      "(in $ millions) \n",
      "\n",
      "\n",
      "Underwriting Profit (Loss) \n",
      "\n",
      "\n",
      "Yearend Float \n",
      "\n",
      "\n",
      "Insurance Operations 2006 2005 \n",
      "\n",
      "General Re $ 526 $( 334) \n",
      "\n",
      "B-H Reinsurance 1,658 (1,069) \n",
      "\n",
      "GEICO 1,314 1,221 \n",
      "\n",
      "Other Primary 340 ** 235 * \n",
      "\n",
      "Total $3.838 $ 53 \n",
      "\n",
      "* Includes MedPro from June 30, 2005. \n",
      "** Includes Applied Underwriters from May 19, 2006. \n",
      "\n",
      "\n",
      "$22,827 $22,920 \n",
      "\n",
      "16,860 16,233 \n",
      "\n",
      "7,171 6,692 \n",
      "\n",
      "4,029 3,442 \n",
      "\n",
      "$50.887 $49.287 \n",
      "\n",
      "\n",
      "2006 2005 \n",
      "\n",
      "\n",
      "In 2007, our results from the bread-and-butter lines of insurance will deteriorate, though I think \n",
      "they will remain satisfactory. The big unknown is super-cat insurance. Were the terrible hurricane seasons \n",
      "of 2004-05 aberrations? Or were they our planet's first warning that the climate of the 21 st Century will \n",
      "differ materially from what we've seen in the past? If the answer to the second question is yes, 2006 will \n",
      "soon be perceived as a misleading period of calm preceding a series of devastating storms. These could \n",
      "rock the insurance industry. It's naive to think of Katrina as anything close to a worst-case event. \n",
      "\n",
      "Neither Ajit Jain, who manages our super-cat operation, nor I know what lies ahead. We do know \n",
      "that it would be a huge mistake to bet that evolving atmospheric changes are benign in their implications \n",
      "for insurers. \n",
      "\n",
      "Don't think, however, that we have lost our taste for risk. We remain prepared to lose $6 billion \n",
      "in a single event, ifwe have been paid appropriately for assuming that risk. We are not willing, though, to \n",
      "take on even very small exposures at prices that don't reflect our evaluation of loss probabilities. \n",
      "Appropriate prices don't guarantee profits in any given year, but inappropriate prices most certainly \n",
      "guarantee eventual losses. Rates have recently fallen because a flood of capital has entered the super-cat \n",
      "field. We have therefore sharply reduced our wind exposures. Our behavior here parallels that which we \n",
      "employ in financial markets: Be fearful when others are greedy, and be greedy when others are fearful. \n",
      "\n",
      "Lloyd's, Equitas and Retroactive Reinsurance \n",
      "\n",
      "Last year - we are getting now to Equitas - Berkshire agreed to enter into a huge retroactive \n",
      "reinsurance contract, a policy that protects an insurer against losses that have already happened, but whose \n",
      "cost is not yet known. I'll give you details of the agreement shortly. But let's first take a journey through \n",
      "insurance history, following the route that led to our deal. \n",
      "\n",
      "Our tale begins around 1688, when Edward Lloyd opened a small coffee house in London. \n",
      "Though no Starbucks, his shop was destined to achieve worldwide fame because of the commercial \n",
      "activities of its clientele - shipowners, merchants and venturesome British capitalists. As these parties \n",
      "sipped Edward's brew, they began to write contracts transferring the risk of a disaster at sea from the \n",
      "owners of ships and their cargo to the capitalists, who wagered that a given voyage would be completed \n",
      "without incident. These capitalists eventually became known as \"underwriters at Lloyd's.\" \n",
      "\n",
      "Though many people believe Lloyd's to be an insurance company, that is not the case. It is \n",
      "instead a place where many member-insurers transact business, just as they did centuries ago. \n",
      "\n",
      "Over time, the underwriters solicited passive investors to join in syndicates. Additionally, the \n",
      "business broadened beyond marine risks into every imaginable form of insurance, including exotic \n",
      "coverages that spread the fame of Lloyd's far and wide. The underwriters left the coffee house, found \n",
      "grander quarters and formalized some rules of association. And those persons who passively backed the \n",
      "underwriters became known as \"names.\" \n",
      "\n",
      "\n",
      "8 \n",
      "\n",
      "\n",
      "Eventually, the names came to include many thousands of people from around the world, who \n",
      "joined expecting to pick up some extra change without effort or serious risk. True, prospective names were \n",
      "always solemnly told that they would have unlimited and everlasting liability for the consequences of their \n",
      "syndicate's underwriting - \"down to the last cufflink,\" as the quaint description went. But that warning \n",
      "came to be viewed as perfunctory. Three hundred years of retained cufflinks acted as a powerful sedative \n",
      "to the names poised to sign up. \n",
      "\n",
      "Then came asbestos. When its prospective costs were added to the tidal wave of environmental \n",
      "and product claims that surfaced in the 1980s, Lloyd's began to implode. Policies written decades earlier - \n",
      "and largely forgotten about - were developing huge losses. No one could intelligently estimate their total, \n",
      "but it was certain to be many tens of billions of dollars. The specter of unending and unlimited losses \n",
      "terrified existing names and scared away prospects. Many names opted for bankruptcy; some even chose \n",
      "suicide. \n",
      "\n",
      "From these shambles, there came a desperate effort to resuscitate Lloyd's. In 1996, the powers \n",
      "that be at the institution allotted £11.1 billion to a new company, Equitas, and made it responsible for \n",
      "paying all claims on policies written before 1993. In effect, this plan pooled the misery of the many \n",
      "syndicates in trouble. Of course, the money allotted could prove to be insufficient - and if that happened, \n",
      "the names remained liable for the shortfall. \n",
      "\n",
      "But the new plan, by concentrating all of the liabilities in one place, had the advantage of \n",
      "eliminating much of the costly intramural squabbling that went on among syndicates. Moreover, the \n",
      "pooling allowed claims evaluation, negotiation and litigation to be handled more intelligently than had been \n",
      "the case previously. Equitas embraced Ben Franklin's thinking: \"We must all hang together, or assuredly \n",
      "we shall hang separately.\" \n",
      "\n",
      "From the start, many people predicted Equitas would eventually fail. But as Ajit and I reviewed \n",
      "the facts in the spring of 2006 - 13 years after the last exposed policy had been written and after the \n",
      "payment of £11.3 billion in claims - we concluded that the patient was likely to survive. And so we \n",
      "decided to offer a huge reinsurance policy to Equitas. \n",
      "\n",
      "Because plenty of imponderables continue to exist, Berkshire could not provide Equitas, and its \n",
      "27,972 names, unlimited protection. But we said - and I'm simplifying - that if Equitas would give us \n",
      "$7.12 billion in cash and securities (this is the float I spoke about), we would pay all of its future claims and \n",
      "expenses up to $13.9 billion. That amount was $5.7 billion above what Equitas had recently guessed its \n",
      "ultimate liabilities to be. Thus the names received a huge - and almost certainly sufficient - amount of \n",
      "future protection against unpleasant surprises. Indeed the protection is so large that Equitas plans a cash \n",
      "payment to its thousands of names, an event few of them had ever dreamed possible. \n",
      "\n",
      "And how will Berkshire fare? That depends on how much \"known\" claims will end up costing us, \n",
      "how many yet-to-be-presented claims will surface and what they will cost, how soon claim payments will \n",
      "be made and how much we earn on the cash we receive before it must be paid out. Ajit and I think the odds \n",
      "are in our favor. And should we be wrong, Berkshire can handle it. \n",
      "\n",
      "Scott Moser, the CEO of Equitas, summarized the transaction neatly: \"Names wanted to sleep \n",
      "easy at night, and we think we've just bought them the world's best mattress.\" \n",
      "\n",
      "Warning: It's time to eat your broccoli - I am now going to talk about accounting matters. I owe \n",
      "this to those Berkshire shareholders who love reading about debits and credits. I hope both of you find this \n",
      "discussion helpful. All others can skip this section; there will be no quiz. \n",
      "\n",
      "Berkshire has done many retroactive transactions - in both number and amount a multiple of such \n",
      "policies entered into by any other insurer. We are the reinsurer of choice for these coverages because the \n",
      "obligations that are transferred to us - for example, lifetime indemnity and medical payments to be made to \n",
      "injured workers - may not be fully satisfied for 50 years or more. No other company can offer the certainty \n",
      "\n",
      "\n",
      "9 \n",
      "\n",
      "\n",
      "that Berkshire can, in terms of guaranteeing the full and fair settlement of these obligations. This fact is \n",
      "important to the original insurer, policyholders and regulators. \n",
      "\n",
      "The accounting procedure for retroactive transactions is neither well known nor intuitive. The \n",
      "best way for shareholders to understand it, therefore, is for us to simply lay out the debits and credits. \n",
      "Charlie and I would like to see this done more often. We sometimes encounter accounting footnotes about \n",
      "important transactions that leave us baffled, and we go away suspicious that the reporting company wished \n",
      "it that way. (For example, try comprehending transactions \"described\" in the old 10-Ks of Enron, even \n",
      "after you know how the movie ended.) \n",
      "\n",
      "So let us summarize our accounting for the Equitas transaction. The major debits will be to Cash \n",
      "and Investments, Reinsurance Recoverable, and Deferred Charges for Reinsurance Assumed (\"DCRA\"). \n",
      "The major credit will be to Reserve for Losses and Loss Adjustment Expense. No profit or loss will be \n",
      "recorded at the inception of the transaction, but underwriting losses will thereafter be incurred annually as \n",
      "the DCRA asset is amortized downward. The amount of the annual amortization charge will be primarily \n",
      "determined by how our end-of-the-year estimates as to the timing and amount of future loss payments \n",
      "compare to the estimates made at the beginning of the year. Eventually, when the last claim has been paid, \n",
      "the DCRA account will be reduced to zero. That day is 50 years or more away. \n",
      "\n",
      "What's important to remember is that retroactive insurance contracts always produce underwriting \n",
      "losses for us. Whether these losses are worth experiencing depends on whether the cash we have received \n",
      "produces investment income that exceeds the losses. Recently our DCRA charges have annually delivered \n",
      "$300 million or so of underwriting losses, which have been more than offset by the income we have \n",
      "realized through use of the cash we received as a premium. Absent new retroactive contracts, the amount \n",
      "of the annual charge would normally decline over time. After the Equitas transaction, however, the annual \n",
      "DCRA cost will initially increase to about $450 million a year. This means that our other insurance \n",
      "operations must generate at least that much underwriting gain for our overall float to be cost-free. That \n",
      "amount is quite a hurdle but one that I believe we will clear in many, if not most, years. \n",
      "\n",
      "\n",
      "Aren't you glad that I promised you there would be no quiz? \n",
      "\n",
      "\n",
      "Manufacturing, Service and Retailing Operations \n",
      "\n",
      "Our activities in this part of Berkshire cover the waterfront. Let's look, though, at a summary \n",
      "balance sheet and earnings statement for the entire group. \n",
      "\n",
      "\n",
      "Balance Sheet 12/31/06 (in millions) \n",
      "\n",
      "\n",
      "Assets \n",
      "\n",
      "Cash and equivalents \n",
      "\n",
      "Accounts and notes receivable . \n",
      "\n",
      "Inventory \n",
      "\n",
      "Other current assets \n",
      "\n",
      "Total current assets \n",
      "\n",
      "\n",
      "& 1,543 \n",
      "3,793 \n",
      "5,257 \n",
      "363 \n",
      "10,956 \n",
      "\n",
      "\n",
      "Liabilities and Equity \n",
      "\n",
      "Notes payable $ 1,468 \n",
      "\n",
      "Other current liabilities 6,635 \n",
      "\n",
      "Total current liabilities 8,103 \n",
      "\n",
      "\n",
      "Goodwill and other intangibles 13,314 Deferred taxes 540 \n",
      "\n",
      "Fixed assets 8,934 Term debt and other liabilities... 3,014 \n",
      "\n",
      "Other assets 1,168 Equity 22,715 \n",
      "\n",
      "$34.372 $34.372 \n",
      "\n",
      "\n",
      "10 \n",
      "\n",
      "\n",
      "Earnings Statement (in millions) \n",
      "\n",
      "\n",
      "\n",
      "2006 \n",
      "\n",
      "2005 \n",
      "\n",
      "2004 \n",
      "\n",
      "Revenues \n",
      "\n",
      "$52,660 \n",
      "\n",
      "$46,896 \n",
      "\n",
      "$44,142 \n",
      "\n",
      "Operating expenses (including depreciation of $823 in 2006, \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "$699 in 2005 and $676 in 2004) \n",
      "\n",
      "49,002 \n",
      "\n",
      "44,190 \n",
      "\n",
      "41,604 \n",
      "\n",
      "Interest expense \n",
      "\n",
      "132 \n",
      "\n",
      "83 \n",
      "\n",
      "57 \n",
      "\n",
      "Pre-tax earnings \n",
      "\n",
      "3,526* \n",
      "\n",
      "2,623* \n",
      "\n",
      "2,481* \n",
      "\n",
      "Income taxes and minority interests \n",
      "\n",
      "1,395 \n",
      "\n",
      "977 \n",
      "\n",
      "941 \n",
      "\n",
      "Net income \n",
      "\n",
      "$ 2,131 \n",
      "\n",
      "$ 1.646 \n",
      "\n",
      "$ 1.540 \n",
      "\n",
      "\n",
      "*Does not include purchase-accounting adjustments. \n",
      "\n",
      "This motley group, which sells products ranging from lollipops to motor homes, earned a pleasing \n",
      "25% on average tangible net worth last year. It's noteworthy also that these operations used only minor \n",
      "financial leverage in achieving that return. Clearly we own some terrific businesses. We purchased many \n",
      "of them, however, at large premiums to net worth - a point reflected in the goodwill item shown on the \n",
      "balance sheet - and that fact reduces the earnings on our average carrying value to 10.8%. \n",
      "\n",
      "Here are a few newsworthy items about companies in this sector: \n",
      "\n",
      "• Bob Shaw, a remarkable entrepreneur who from a standing start built Shaw Industries into the \n",
      "country's largest carpet producer, elected last year, at age 75, to retire. To succeed him, Bob \n",
      "recommended Vance Bell, a 31-year veteran at Shaw, and Bob, as usual, made the right call. \n",
      "Weakness in housing has caused the carpet business to slow. Shaw, however, remains a \n",
      "powerhouse and a major contributor to Berkshire's earnings. \n",
      "\n",
      "• MiTek, a manufacturer of connectors for roof trusses at the time we purchased it in 2001, is \n",
      "developing into a mini-conglomerate. At the rate it is growing, in fact, \"mini\" may soon be \n",
      "inappropriate. In purchasing MiTek for $420 million, we lent the company $200 million at 9% \n",
      "and bought $198 million of stock, priced at $10,000 per share. Additionally, 55 employees bought \n",
      "2,200 shares for $22 million. Each employee paid exactly the same price that we did, in most \n",
      "cases borrowing money to do so. \n",
      "\n",
      "And are they ever glad they did! Five years later, MiTek' s sales have tripled and the stock is \n",
      "valued at $71,699 per share. Despite its making 14 acquisitions, at a cost of $291 million, MiTek \n",
      "has paid off its debt to Berkshire and holds $35 million of cash. We celebrated the fifth \n",
      "anniversary of our purchase with a party in July. I told the group that it would be embarrassing if \n",
      "MiTek's stock price soared beyond that of Berkshire \"A\" shares. Don't be surprised, however, if \n",
      "that happens (though Charlie and I will try to make our shares a moving target). \n",
      "\n",
      "• Not all of our businesses are destined to increase profits. When an industry's underlying \n",
      "economics are crumbling, talented management may slow the rate of decline. Eventually, though, \n",
      "eroding fundamentals will overwhelm managerial brilliance. (As a wise friend told me long ago, \n",
      "\"If you want to get a reputation as a good businessman, be sure to get into a good business.\") And \n",
      "fundamentals are definitely eroding in the newspaper industry, a trend that has caused the profits \n",
      "of our Buffalo News to decline. The skid will almost certainly continue. \n",
      "\n",
      "When Charlie and I were young, the newspaper business was as easy a way to make huge returns \n",
      "as existed in America. As one not-too-bright publisher famously said, \"I owe my fortune to two \n",
      "great American institutions: monopoly and nepotism.\" No paper in a one -paper city, however bad \n",
      "the product or however inept the management, could avoid gushing profits. \n",
      "\n",
      "The industry's staggering returns could be simply explained. For most of the 20 th Century, \n",
      "newspapers were the primary source of information for the American public. Whether the subject \n",
      "was sports, finance, or politics, newspapers reigned supreme. Just as important, their ads were the \n",
      "easiest way to find job opportunities or to learn the price of groceries at your town's supermarkets. \n",
      "\n",
      "\n",
      "11 \n",
      "\n",
      "\n",
      "The great majority of families therefore felt the need for a paper every day, but understandably \n",
      "most didn't wish to pay for two. Advertisers preferred the paper with the most circulation, and \n",
      "readers tended to want the paper with the most ads and news pages. This circularity led to a law \n",
      "of the newspaper jungle: Survival of the Fattest. \n",
      "\n",
      "Thus, when two or more papers existe\n"
     ]
    }
   ],
   "source": [
    "test_text = text[:int(len(text) * 0.1)]\n",
    "print(\"Input:\",test_text)\n",
    "tokens, input =generate(test_text,vocab,\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tokens: ['berkshir', 'hathaway', 'inc', 'to', 'the', 'sharehold', 'of', 'berkshir', 'hathaway', 'inc', 'our', 'gain', 'in', 'net', 'worth', 'dure', '2006', 'wa', '169', 'billion', 'which', 'increas', 'the', 'pershar', 'book', 'valu', 'of', 'both', 'our', 'class', 'a', 'and', 'class', 'b', 'stock', 'by', '184', 'over', 'the', 'last', '42', 'year', 'that', 'is', 'sinc', 'present', 'manag', 'took', 'over', 'book', 'valu', 'ha', 'grown', 'from', '19', 'to', '70281', 'a', 'rate', 'of', '214', 'compound', 'annual', 'we', 'believ', 'that', '169', 'billion', 'is', 'a', 'record', 'for', 'a', 'oneyear', 'gain', 'in', 'net', 'worth', 'more', 'than', 'ha', 'ever', 'been', 'book', 'by', 'ani', 'american', 'busi', 'leav', 'asid', 'boost', 'that', 'have', 'occur', 'becaus', 'of', 'merger', 'eg', 'aol', 'purchas', 'of', 'time', 'warner', 'of', 'cours', 'exxon', 'mobil', 'and', 'other', 'compani', 'earn', 'far', 'more', 'than', 'berkshir', 'but', 'their', 'earn', 'larg', 'go', 'to', 'dividend', 'andor', 'repurchas', 'rather', 'than', 'to', 'build', 'net', 'worth', 'all', 'that', 'said', 'a', 'confess', 'about', 'our', '2006', 'gain', 'is', 'in', 'order', 'our', 'most', 'import', 'busi', 'insur', 'benefit', 'from', 'a', 'larg', 'dose', 'of', 'luck', 'mother', 'natur', 'bless', 'her', 'heart', 'went', 'on', 'vacat', 'after', 'hammer', 'us', 'with', 'hurrican', 'in', '2004', 'and', '2005', 'storm', 'that', 'caus', 'us', 'to', 'lose', 'a', 'bundl', 'on', 'supercat', 'insur', 'she', 'just', 'vanish', 'last', 'year', 'the', 'red', 'ink', 'from', 'thi', 'activ', 'turn', 'black', 'veri', 'black', 'in', 'addit', 'the', 'great', 'major', 'of', 'our', '73', 'busi', 'did', 'outstandingli', 'well', 'in', '2006', 'let', 'me', 'focu', 'for', 'a', 'moment', 'on', 'one', 'of', 'our', 'largest', 'oper', 'geico', 'what', 'manag', 'accomplish', 'there', 'wa', 'simpli', 'extraordinari', 'as', 'ive', 'told', 'you', 'befor', 'toni', 'nice', 'geico', 's', 'ceo', 'went', 'to', 'work', 'at', 'the', 'compani', '45', 'year', 'ago', 'two', 'month', 'after', 'turn', '18', 'he', 'becam', 'ceo', 'in', '1992', 'and', 'from', 'then', 'on', 'the', 'compani', 'growth', 'explod', 'in', 'addit', 'toni', 'ha', 'deliv', 'stagger', 'product', 'gain', 'in', 'recent', 'year', 'between', 'yearend', '2003', 'and', 'yearend', '2006', 'the', 'number', 'of', 'geico', 'polici', 'increas', 'from', '57', 'million', 'to', '81', 'million', 'a', 'jump', 'of', '42', 'yet', 'dure', 'that', 'same', 'period', 'the', 'compani', 'employe', 'measur', 'on', 'a', 'fulltimeequival', 'basi', 'fell', '35', 'so', 'product', 'grew', '47', 'and', 'geico', 'didnt', 'start', 'fat', 'that', 'remark', 'gain', 'ha', 'allow', 'geico', 'to', 'maintain', 'it', 'allimport', 'posit', 'as', 'a', 'lowcost', 'produc', 'even', 'though', 'it', 'ha', 'dramat', 'increas', 'advertis', 'expenditur', 'last', 'year', 'geico', 'spent', '631', 'million', 'on', 'ad', 'up', 'from', '238', 'million', 'in', '2003', 'and', 'up', 'from', '31', 'million', 'in', '1995', 'when', 'berkshir', 'took', 'control', 'today', 'geico', 'spend', 'far', 'more', 'on', 'ad', 'than', 'ani', 'of', 'it', 'competitor', 'even', 'those', 'much', 'larger', 'we', 'will', 'continu', 'to', 'rais', 'the', 'bar', 'last', 'year', 'i', 'told', 'you', 'that', 'if', 'you', 'had', 'a', 'new', 'son', 'or', 'grandson', 'to', 'be', 'sure', 'to', 'name', 'him', 'toni', 'but', 'don', 'keough', 'a', 'berkshir', 'director', 'recent', 'had', 'a', 'better', 'idea', 'after', 'review', 'geico', 'perform', 'in', '2006', 'he', 'wrote', 'me', 'forget', 'birth', 'tell', 'the', 'sharehold', 'to', 'immedi', 'chang', 'the', 'name', 'of', 'their', 'present', 'children', 'to', 'toni', 'or', 'antoinett', 'don', 'sign', 'hi', 'letter', 'toni', 'charli', 'munger', 'my', 'partner', 'and', 'berkshir', 'vice', 'chairman', 'and', 'i', 'run', 'what', 'ha', 'turn', 'out', 'to', 'be', 'a', 'big', 'busi', 'one', 'with', '217000', 'employe', 'and', 'annual', 'revenu', 'approach', '100', 'billion', 'we', 'certainli', 'didnt', 'plan', 'it', 'that', 'way', 'charli', 'began', 'as', 'a', 'lawyer', 'and', 'i', 'thought', 'of', 'myself', 'as', 'a', 'secur', 'analyst', 'sit', 'in', 'those', 'seat', 'we', 'both', 'grew', 'skeptic', 'about', 'the', 'abil', 'of', 'big', 'entiti', 'of', 'ani', 'type', 'to', 'function', 'well', 'size', 'seem', 'to', 'make', 'mani', 'organ', 'slowthink', 'resist', 'to', 'chang', 'and', 'smug', 'in', 'churchil', 'word', 'we', 'shape', 'our', 'build', 'and', 'afterward', 'our', 'build', 'shape', 'us', 'here', 'a', 'tell', 'fact', 'of', 'the', 'ten', 'nonoil', 'compani', 'have', 'the', 'largest', 'market', 'capit', 'in', '1965', 'titan', 'such', 'as', 'gener', 'motor', 'sear', 'dupont', 'and', 'eastman', 'kodak', 'onli', 'one', 'made', 'the', '2006', 'list', 'a11', 'pershar', 'figur', 'use', 'in', 'thi', 'report', 'appli', 'to', 'berkshir', 'a', 'share', 'figur', 'for', 'the', 'b', 'share', 'are', '130', 'of', 'those', 'shown', 'for', 'the', 'a', '3', 'in', 'fair', 'weve', 'seen', 'plenti', 'of', 'success', 'as', 'well', 'some', 'truli', 'outstand', 'there', 'are', 'mani', 'giant', 'compani', 'manag', 'whom', 'i', 'greatli', 'admir', 'ken', 'chenault', 'of', 'american', 'express', 'jeff', 'immelt', 'of', 'ge', 'and', 'dick', 'kovacevich', 'of', 'well', 'fargo', 'come', 'quickli', 'to', 'mind', 'but', 'i', 'dont', 'think', 'i', 'could', 'do', 'the', 'manag', 'job', 'they', 'do', 'and', 'i', 'know', 'i', 'wouldnt', 'enjoy', 'mani', 'of', 'the', 'duti', 'that', 'come', 'with', 'their', 'posit', 'meet', 'speech', 'foreign', 'travel', 'the', 'chariti', 'circuit', 'and', 'government', 'relat', 'for', 'me', 'ronald', 'reagan', 'had', 'it', 'right', 'it', 'probabl', 'true', 'that', 'hard', 'work', 'never', 'kill', 'anyon', 'but', 'whi', 'take', 'the', 'chanc', 'so', 'ive', 'taken', 'the', 'easi', 'rout', 'just', 'sit', 'back', 'and', 'work', 'through', 'great', 'manag', 'who', 'run', 'their', 'own', 'show', 'my', 'onli', 'task', 'are', 'to', 'cheer', 'them', 'on', 'sculpt', 'and', 'harden', 'our', 'corpor', 'cultur', 'and', 'make', 'major', 'capitalalloc', 'decis', 'our', 'manag', 'have', 'return', 'thi', 'trust', 'by', 'work', 'hard', 'and', 'effect', 'for', 'their', 'perform', 'over', 'the', 'last', '42', 'year', 'and', 'particularli', 'for', '2006', 'charli', 'and', 'i', 'thank', 'them', 'yardstick', 'charli', 'and', 'i', 'measur', 'berkshir', 'progress', 'and', 'evalu', 'it', 'intrins', 'valu', 'in', 'a', 'number', 'of', 'way', 'no', 'singl', 'criterion', 'is', 'effect', 'in', 'do', 'these', 'job', 'and', 'even', 'an', 'avalanch', 'of', 'statist', 'will', 'not', 'captur', 'some', 'factor', 'that', 'are', 'import', 'for', 'exampl', 'it', 'essenti', 'that', 'we', 'have', 'manag', 'much', 'younger', 'than', 'i', 'avail', 'to', 'succeed', 'me', 'berkshir', 'ha', 'never', 'been', 'in', 'better', 'shape', 'in', 'thi', 'regard', 'but', 'i', 'cant', 'prove', 'it', 'to', 'you', 'with', 'number', 'there', 'are', 'two', 'statist', 'howev', 'that', 'are', 'of', 'real', 'import', 'the', 'first', 'is', 'the', 'amount', 'of', 'invest', 'includ', 'cash', 'and', 'cashequival', 'that', 'we', 'own', 'on', 'a', 'pershar', 'basi', 'arriv', 'at', 'thi', 'figur', 'we', 'exclud', 'invest', 'held', 'in', 'our', 'financ', 'oper', 'becaus', 'these', 'are', 'larg', 'offset', 'by', 'borrow', 'here', 'the', 'record', 'sinc', 'present', 'manag', 'acquir', 'control', 'of', 'berkshir', 'year', 'pershar', 'invest', '1965', '4', '1975', '159', '1985', '2407', '1995', '21817', '2006', '80636', 'compound', 'growth', 'rate', '19652006', '275', 'compound', 'growth', 'rate', '19952006', '126', 'net', 'of', 'minor', 'interest', 'in', 'our', 'earli', 'year', 'we', 'put', 'most', 'of', 'our', 'retain', 'earn', 'and', 'insur', 'float', 'into', 'invest', 'in', 'market', 'secur', 'becaus', 'of', 'thi', 'emphasi', 'and', 'becaus', 'the', 'secur', 'we', 'purchas', 'gener', 'did', 'well', 'our', 'growth', 'rate', 'in', 'invest', 'wa', 'for', 'a', 'long', 'time', 'quit', 'high', 'over', 'the', 'year', 'howev', 'we', 'have', 'focus', 'more', 'and', 'more', 'on', 'the', 'acquisit', 'of', 'oper', 'busi', 'use', 'our', 'fund', 'for', 'these', 'purchas', 'ha', 'both', 'slow', 'our', 'growth', 'in', 'invest', 'and', 'acceler', 'our', 'gain', 'in', 'pretax', 'earn', 'from', 'noninsur', 'busi', 'the', 'second', 'yardstick', 'we', 'use', 'here', 'how', 'those', 'earn', 'have', 'look', '4', 'year', 'pretax', 'earn', 'per', 'share', '1965', '4', '1975', '4', '1985', '52', '1995', '175', '2006', '3625', 'compound', 'growth', 'rate', '19652006', '179', 'compound', 'growth', 'rate', '19952006', '317', 'exclud', 'purchaseaccount', 'adjust', 'and', 'net', 'of', 'minor', 'interest', 'last', 'year', 'we', 'had', 'a', 'good', 'increas', 'in', 'noninsur', 'earn', '38', 'larg', 'gain', 'from', 'here', 'on', 'in', 'though', 'will', 'come', 'onli', 'if', 'we', 'are', 'abl', 'to', 'make', 'major', 'and', 'sensibl', 'acquisit', 'that', 'will', 'not', 'be', 'easi', 'we', 'do', 'howev', 'have', 'one', 'advantag', 'more', 'and', 'more', 'berkshir', 'ha', 'becom', 'the', 'buyer', 'of', 'choic', 'for', 'busi', 'owner', 'and', 'manag', 'initi', 'we', 'were', 'view', 'that', 'way', 'onli', 'in', 'the', 'us', 'and', 'more', 'often', 'than', 'not', 'by', 'privat', 'compani', 'weve', 'long', 'want', 'nonetheless', 'to', 'extend', 'berkshir', 'appeal', 'beyond', 'us', 'border', 'and', 'last', 'year', 'our', 'globetrot', 'final', 'got', 'underway', 'acquisit', 'we', 'began', '2006', 'by', 'complet', 'the', 'three', 'acquisit', 'pend', 'at', 'yearend', '2005', 'spend', 'about', '6', 'billion', 'for', 'pacificorp', 'busi', 'wire', 'and', 'appli', 'underwrit', 'all', 'are', 'perform', 'veri', 'well', 'the', 'highlight', 'of', 'the', 'year', 'howev', 'wa', 'our', 'juli', '5', 'th', 'acquisit', 'of', 'most', 'of', 'iscar', 'an', 'isra', 'compani', 'and', 'our', 'new', 'associ', 'with', 'it', 'chairman', 'eitan', 'wertheim', 'and', 'ceo', 'jacob', 'harpaz', 'the', 'stori', 'here', 'began', 'on', 'octob', '25', '2005', 'when', 'i', 'receiv', 'a', '114page', 'letter', 'from', 'eitan', 'of', 'whom', 'i', 'then', 'knew', 'noth', 'the', 'letter', 'began', 'i', 'am', 'write', 'to', 'introduc', 'you', 'to', 'iscar', 'and', 'proceed', 'to', 'describ', 'a', 'cut', 'tool', 'busi', 'carri', 'on', 'in', '61', 'countri', 'then', 'eitan', 'wrote', 'we', 'have', 'for', 'some', 'time', 'consid', 'the', 'issu', 'of', 'gener', 'transfer', 'and', 'ownership', 'that', 'are', 'typic', 'for', 'larg', 'famili', 'enterpris', 'and', 'have', 'given', 'much', 'thought', 'to', 'iscar', 's', 'futur', 'our', 'conclus', 'is', 'that', 'berkshir', 'hathaway', 'would', 'be', 'the', 'ideal', 'home', 'for', 'iscar', 'we', 'believ', 'that', 'iscar', 'would', 'continu', 'to', 'thrive', 'as', 'a', 'part', 'of', 'your', 'portfolio', 'of', 'busi', 'overal', 'eitan', 's', 'letter', 'made', 'the', 'qualiti', 'of', 'the', 'compani', 'and', 'the', 'charact', 'of', 'it', 'manag', 'leap', 'off', 'the', 'page', 'it', 'also', 'made', 'me', 'want', 'to', 'learn', 'more', 'and', 'in', 'novemb', 'eitan', 'jacob', 'and', 'iscar', 's', 'cfo', 'danni', 'goldman', 'came', 'to', 'omaha', 'a', 'few', 'hour', 'with', 'them', 'convinc', 'me', 'that', 'if', 'we', 'were', 'to', 'make', 'a', 'deal', 'we', 'would', 'be', 'team', 'up', 'with', 'extraordinarili', 'talent', 'manag', 'who', 'could', 'be', 'trust', 'to', 'run', 'the', 'busi', 'after', 'a', 'sale', 'with', 'all', 'of', 'the', 'energi', 'and', 'dedic', 'that', 'they', 'had', 'exhibit', 'previous', 'howev', 'have', 'never', 'bought', 'a', 'busi', 'base', 'outsid', 'of', 'the', 'us', 'though', 'i', 'had', 'bought', 'a', 'number', 'of', 'foreign', 'stock', 'i', 'need', 'to', 'get', 'educ', 'on', 'some', 'tax', 'and', 'jurisdict', 'matter', 'with', 'that', 'task', 'complet', 'berkshir', 'purchas', '80', 'of', 'iscar', 'for', '4', 'billion', 'the', 'remain', '20', 'stay', 'in', 'the', 'hand', 'of', 'the', 'wertheim', 'famili', 'make', 'it', 'our', 'valu', 'partner', 'iscar', 's', 'product', 'are', 'small', 'consum', 'cut', 'tool', 'that', 'are', 'use', 'in', 'conjunct', 'with', 'larg', 'and', 'expens', 'machin', 'tool', 'it', 'a', 'busi', 'without', 'magic', 'except', 'for', 'that', 'impart', 'by', 'the', 'peopl', 'who', 'run', 'it', 'but', 'eitan', 'jacob', 'and', 'their', 'associ', 'are', 'true', 'manageri', 'magician', 'who', 'constantli', 'develop', 'tool', 'that', 'make', 'their', 'custom', 'machin', 'more', 'product', 'the', 'result', 'iscar', 'make', 'money', 'becaus', 'it', 'enabl', 'it', 'custom', 'to', 'make', 'more', 'money', 'there', 'is', 'no', 'better', 'recip', 'for', 'continu', 'success', '5', 'in', 'septemb', 'charli', 'and', 'i', 'along', 'with', 'five', 'berkshir', 'associ', 'visit', 'iscar', 'in', 'israel', 'we', 'and', 'i', 'mean', 'everi', 'one', 'of', 'us', 'have', 'never', 'been', 'more', 'impress', 'with', 'ani', 'oper', 'at', 'iscar', 'as', 'throughout', 'israel', 'brain', 'and', 'energi', 'are', 'ubiquit', 'berkshir', 'sharehold', 'are', 'lucki', 'to', 'have', 'join', 'with', 'eitan', 'jacob', 'danni', 'and', 'their', 'talent', 'associ', 'a', 'few', 'month', 'later', 'berkshir', 'again', 'becam', 'the', 'buyer', 'of', 'choic', 'in', 'a', 'deal', 'brought', 'to', 'us', 'by', 'my', 'friend', 'john', 'roach', 'of', 'fort', 'worth', 'john', 'mani', 'of', 'you', 'will', 'rememb', 'wa', 'chairman', 'of', 'justin', 'industri', 'which', 'we', 'bought', 'in', '2000', 'at', 'that', 'time', 'john', 'wa', 'help', 'john', 'justin', 'who', 'wa', 'termin', 'ill', 'find', 'a', 'perman', 'home', 'for', 'hi', 'compani', 'john', 'justin', 'die', 'soon', 'after', 'we', 'bought', 'justin', 'industri', 'but', 'it', 'ha', 'sinc', 'been', 'run', 'exactli', 'as', 'we', 'promis', 'him', 'it', 'would', 'be', 'visit', 'me', 'in', 'novemb', 'john', 'roach', 'brought', 'along', 'paul', 'andrew', 'jr', 'owner', 'of', 'about', '80', 'of', 'tti', 'a', 'fort', 'worth', 'distributor', 'of', 'electron', 'compon', 'over', 'a', '35year', 'period', 'paul', 'built', 'tti', 'from', '112000', 'of', 'sale', 'to', '13', 'billion', 'he', 'is', 'a', 'remark', 'entrepreneur', 'and', 'oper', 'paul', '64', 'love', 'run', 'hi', 'busi', 'but', 'not', 'long', 'ago', 'he', 'happen', 'to', 'wit', 'how', 'disrupt', 'the', 'death', 'of', 'a', 'founder', 'can', 'be', 'both', 'to', 'a', 'privat', 'compani', 'employe', 'and', 'the', 'owner', 'famili', 'what', 'start', 'out', 'as', 'disrupt', 'furthermor', 'often', 'evolv', 'into', 'destruct', 'about', 'a', 'year', 'ago', 'therefor', 'paul', 'began', 'to', 'think', 'about', 'sell', 'tti', 'hi', 'goal', 'wa', 'to', 'put', 'hi', 'busi', 'in', 'the', 'hand', 'of', 'an', 'owner', 'he', 'had', 'care', 'chosen', 'rather', 'than', 'allow', 'a', 'trust', 'offic', 'or', 'lawyer', 'to', 'conduct', 'an', 'auction', 'after', 'hi', 'death', 'paul', 'reject', 'the', 'idea', 'of', 'a', 'strateg', 'buyer', 'know', 'that', 'in', 'the', 'pursuit', 'of', 'synergi', 'an', 'owner', 'of', 'that', 'type', 'would', 'be', 'apt', 'to', 'dismantl', 'what', 'he', 'had', 'so', 'care', 'built', 'a', 'move', 'that', 'would', 'uproot', 'hundr', 'of', 'hi', 'associ', 'and', 'perhap', 'wound', 'tti', 'busi', 'in', 'the', 'process', 'he', 'also', 'rule', 'out', 'a', 'privat', 'equiti', 'firm', 'which', 'would', 'veri', 'like', 'load', 'the', 'compani', 'with', 'debt', 'and', 'then', 'flip', 'it', 'as', 'soon', 'as', 'possibl', 'that', 'left', 'berkshir', 'paul', 'and', 'i', 'met', 'on', 'the', 'morn', 'of', 'novemb', '15', 'th', 'and', 'made', 'a', 'deal', 'befor', 'lunch', 'later', 'he', 'wrote', 'me', 'after', 'our', 'meet', 'i', 'am', 'confid', 'that', 'berkshir', 'is', 'the', 'right', 'owner', 'for', 'tti', 'i', 'am', 'proud', 'of', 'our', 'past', 'and', 'excit', 'about', 'our', 'futur', 'and', 'so', 'are', 'charli', 'and', 'i', 'we', 'also', 'made', 'some', 'tuckin', 'acquisit', 'dure', '2006', 'at', 'fruit', 'of', 'the', 'loom', 'fruit', 'mitek', 'ctb', 'shaw', 'and', 'clayton', 'fruit', 'made', 'the', 'largest', 'purchas', 'first', 'it', 'bought', 'russel', 'corp', 'a', 'lead', 'produc', 'of', 'athlet', 'apparel', 'and', 'uniform', 'for', 'about', '12', 'billion', 'includ', 'assum', 'debt', 'and', 'in', 'decemb', 'it', 'agre', 'to', 'buy', 'the', 'intim', 'apparel', 'busi', 'of', 'vf', 'corp', 'togeth', 'these', 'acquisit', 'add', 'about', '22', 'billion', 'to', 'fruit', 'sale', 'and', 'bring', 'with', 'them', 'about', '23000', 'employe', 'charli', 'and', 'i', 'love', 'it', 'when', 'we', 'can', 'acquir', 'busi', 'that', 'can', 'be', 'place', 'under', 'manag', 'such', 'as', 'john', 'holland', 'at', 'fruit', 'who', 'have', 'alreadi', 'shown', 'their', 'stuff', 'at', 'berkshir', 'mitek', 'for', 'exampl', 'ha', 'made', '14', 'acquisit', 'sinc', 'we', 'purchas', 'it', 'in', '2001', 'and', 'gene', 'toomb', 'ha', 'deliv', 'result', 'from', 'these', 'deal', 'far', 'in', 'excess', 'of', 'what', 'he', 'had', 'predict', 'in', 'effect', 'we', 'leverag', 'the', 'manageri', 'talent', 'alreadi', 'with', 'us', 'by', 'these', 'tuck', 'in', 'deal', 'we', 'will', 'make', 'mani', 'more', 'we', 'continu', 'howev', 'to', 'need', 'eleph', 'in', 'order', 'for', 'us', 'to', 'use', 'berkshir', 'flood', 'of', 'incom', 'cash', 'charli', 'and', 'i', 'must', 'therefor', 'ignor', 'the', 'pursuit', 'of', 'mice', 'and', 'focu', 'our', 'acquisit', 'effort', 'on', 'much', 'bigger', 'game', 'our', 'exemplar', 'is', 'the', 'older', 'man', 'who', 'crash', 'hi', 'groceri', 'cart', 'into', 'that', 'of', 'a', 'much', 'younger', 'fellow', 'while', 'both', 'were', 'shop', 'the', 'elderli', 'man', 'explain', 'apologet', 'that', 'he', 'had', 'lost', 'track', 'of', 'hi', 'wife', 'and', 'wa', 'preoccupi', 'search', 'for', 'her', 'hi', 'new', 'acquaint', 'said', 'that', 'by', 'coincid', 'hi', 'wife', 'had', 'also', 'wander', 'off', 'and', 'suggest', 'that', 'it', 'might', 'be', 'more', 'effici', 'if', 'they', 'jointli', 'look', 'for', 'the', 'two', 'women', 'agre', 'the', 'older', 'man', 'ask', 'hi', 'new', 'companion', 'what', 'hi', 'wife', 'look', 'like', 'she', 'a', 'gorgeou', 'blond', 'the', 'fellow', 'answer', 'with', 'a', 'bodi', 'that', 'would', 'caus', 'a', 'bishop', 'to', 'go', 'through', 'a', 'stain', 'glass', 'window', 'and', 'she', 'wear', 'tight', 'white', 'short', 'how', 'about', 'your', 'the', 'senior', 'citizen', 'wast', 'no', 'word', 'forget', 'her', 'well', 'look', 'for', 'your', '6', 'what', 'we', 'are', 'look', 'for', 'is', 'describ', 'on', 'page', '25', 'if', 'you', 'have', 'an', 'acquisit', 'candid', 'that', 'fit', 'call', 'me', 'day', 'or', 'night', 'and', 'then', 'watch', 'me', 'shatter', 'a', 'stain', 'glass', 'window', 'now', 'let', 'examin', 'the', 'four', 'major', 'oper', 'sector', 'of', 'berkshir', 'lump', 'their', 'financi', 'figur', 'togeth', 'imped', 'analysi', 'so', 'well', 'look', 'at', 'them', 'as', 'four', 'separ', 'busi', 'start', 'with', 'the', 'all', 'import', 'insur', 'group', 'insur', 'next', 'month', 'mark', 'the', '40', 'th', 'anniversari', 'of', 'our', 'entranc', 'into', 'the', 'insur', 'busi', 'it', 'wa', 'on', 'march', '9', '1967', 'that', 'berkshir', 'purchas', 'nation', 'indemn', 'and', 'it', 'companion', 'compani', 'nation', 'fire', 'marin', 'from', 'jack', 'ringwalt', 'for', '86', 'million', 'jack', 'wa', 'a', 'longtim', 'friend', 'of', 'mine', 'and', 'an', 'excel', 'but', 'somewhat', 'eccentr', 'businessman', 'for', 'about', 'ten', 'minut', 'everi', 'year', 'he', 'would', 'get', 'the', 'urg', 'to', 'sell', 'hi', 'compani', 'but', 'those', 'mood', 'perhap', 'brought', 'on', 'by', 'a', 'tiff', 'with', 'regul', 'or', 'an', 'unfavor', 'juri', 'verdict', 'quickli', 'vanish', 'in', 'the', 'mid1960', 'i', 'ask', 'invest', 'banker', 'charli', 'heider', 'a', 'mutual', 'friend', 'of', 'mine', 'and', 'jack', 'to', 'alert', 'me', 'the', 'next', 'time', 'jack', 'wa', 'in', 'heat', 'when', 'charli', 'call', 'came', 'i', 'sped', 'to', 'meet', 'jack', 'we', 'made', 'a', 'deal', 'in', 'a', 'few', 'minut', 'with', 'me', 'waiv', 'an', 'audit', 'due', 'dilig', 'or', 'anyth', 'els', 'that', 'would', 'give', 'jack', 'an', 'opportun', 'to', 'reconsid', 'we', 'just', 'shook', 'hand', 'and', 'that', 'wa', 'that', 'when', 'we', 'were', 'due', 'to', 'close', 'the', 'purchas', 'at', 'charli', 'offic', 'jack', 'wa', 'late', 'final', 'arriv', 'he', 'explain', 'that', 'he', 'had', 'been', 'drive', 'around', 'look', 'for', 'a', 'park', 'meter', 'with', 'some', 'unexpir', 'time', 'that', 'wa', 'a', 'magic', 'moment', 'for', 'me', 'i', 'knew', 'then', 'that', 'jack', 'wa', 'go', 'to', 'be', 'my', 'kind', 'of', 'manag', 'when', 'berkshir', 'purchas', 'jack', 'two', 'insur', 'they', 'had', 'float', 'of', '17', 'million', 'weve', 'regularli', 'offer', 'a', 'long', 'explan', 'of', 'float', 'in', 'earlier', 'report', 'which', 'you', 'can', 'read', 'on', 'our', 'websit', 'simpli', 'put', 'float', 'is', 'money', 'we', 'hold', 'that', 'is', 'not', 'our', 'but', 'which', 'we', 'get', 'to', 'invest', 'at', 'the', 'end', 'of', '2006', 'our', 'float', 'had', 'grown', 'to', '509', 'billion', 'and', 'we', 'have', 'sinc', 'written', 'a', 'huge', 'retroact', 'reinsur', 'contract', 'with', 'equita', 'which', 'i', 'will', 'describ', 'in', 'the', 'next', 'section', 'that', 'boost', 'float', 'by', 'anoth', '7', 'billion', 'much', 'of', 'the', 'gain', 'weve', 'made', 'ha', 'come', 'through', 'our', 'acquisit', 'of', 'other', 'insur', 'but', 'weve', 'also', 'had', 'outstand', 'intern', 'growth', 'particularli', 'at', 'ajit', 'jain', 'amaz', 'reinsur', 'oper', 'natur', 'i', 'had', 'no', 'notion', 'in', '1967', 'that', 'our', 'float', 'would', 'develop', 'as', 'it', 'ha', 'there', 'much', 'to', 'be', 'said', 'for', 'just', 'put', 'one', 'foot', 'in', 'front', 'of', 'the', 'other', 'everi', 'day', 'the', 'float', 'from', 'retroact', 'reinsur', 'contract', 'of', 'which', 'we', 'have', 'mani', 'automat', 'drift', 'down', 'over', 'time', 'therefor', 'it', 'will', 'be', 'difficult', 'for', 'us', 'to', 'increas', 'float', 'in', 'the', 'futur', 'unless', 'we', 'make', 'new', 'acquisit', 'in', 'the', 'insur', 'field', 'whatev', 'it', 'size', 'howev', 'the', 'allimport', 'cost', 'of', 'berkshir', 'float', 'over', 'time', 'is', 'like', 'to', 'be', 'significantli', 'below', 'that', 'of', 'the', 'industri', 'perhap', 'even', 'fall', 'to', 'less', 'than', 'zero', 'note', 'the', 'word', 'over', 'time', 'there', 'will', 'be', 'bad', 'year', 'period', 'you', 'can', 'be', 'sure', 'of', 'that', 'in', '2006', 'though', 'everyth', 'went', 'right', 'in', 'insur', 'realli', 'right', 'our', 'manag', 'toni', 'nice', 'geico', 'ajit', 'jain', 'bh', 'reinsur', 'joe', 'brandon', 'and', 'tad', 'montross', 'gener', 're', 'don', 'wurster', 'nation', 'indemn', 'primari', 'tom', 'nerney', 'us', 'liabil', 'tim', 'kenesey', 'medic', 'protect', 'rod', 'eldr', 'homest', 'compani', 'and', 'cypress', 'sid', 'ferenc', 'and', 'steve', 'menzi', 'appli', 'underwrit', 'john', 'kizer', 'central', 'state', 'and', 'don', 'towl', 'kansa', 'banker', 'sureti', 'simpli', 'shot', 'the', 'light', 'out', 'when', 'i', 'recit', 'their', 'name', 'i', 'feel', 'as', 'if', 'im', 'at', 'cooperstown', 'read', 'from', 'the', 'hall', 'of', 'fame', 'roster', 'of', 'cours', 'the', 'overal', 'insur', 'industri', 'also', 'had', 'a', 'terrif', 'year', 'in', '2006', 'but', 'our', 'manag', 'deliv', 'result', 'gener', 'superior', 'to', 'those', 'of', 'their', 'competitor', '7', 'below', 'is', 'the', 'talli', 'on', 'our', 'underwrit', 'and', 'float', 'for', 'each', 'major', 'sector', 'of', 'insur', 'enjoy', 'the', 'view', 'becaus', 'you', 'wont', 'soon', 'see', 'anoth', 'like', 'it', 'in', 'million', 'underwrit', 'profit', 'loss', 'yearend', 'float', 'insur', 'oper', '2006', '2005', 'gener', 're', '526', '334', 'bh', 'reinsur', '1658', '1069', 'geico', '1314', '1221', 'other', 'primari', '340', '235', 'total', '3838', '53', 'includ', 'medpro', 'from', 'june', '30', '2005', 'includ', 'appli', 'underwrit', 'from', 'may', '19', '2006', '22827', '22920', '16860', '16233', '7171', '6692', '4029', '3442', '50887', '49287', '2006', '2005', 'in', '2007', 'our', 'result', 'from', 'the', 'breadandbutt', 'line', 'of', 'insur', 'will', 'deterior', 'though', 'i', 'think', 'they', 'will', 'remain', 'satisfactori', 'the', 'big', 'unknown', 'is', 'supercat', 'insur', 'were', 'the', 'terribl', 'hurrican', 'season', 'of', '200405', 'aberr', 'or', 'were', 'they', 'our', 'planet', 'first', 'warn', 'that', 'the', 'climat', 'of', 'the', '21', 'st', 'centuri', 'will', 'differ', 'materi', 'from', 'what', 'weve', 'seen', 'in', 'the', 'past', 'if', 'the', 'answer', 'to', 'the', 'second', 'question', 'is', 'ye', '2006', 'will', 'soon', 'be', 'perceiv', 'as', 'a', 'mislead', 'period', 'of', 'calm', 'preced', 'a', 'seri', 'of', 'devast', 'storm', 'these', 'could', 'rock', 'the', 'insur', 'industri', 'it', 'naiv', 'to', 'think', 'of', 'katrina', 'as', 'anyth', 'close', 'to', 'a', 'worstcas', 'event', 'neither', 'ajit', 'jain', 'who', 'manag', 'our', 'supercat', 'oper', 'nor', 'i', 'know', 'what', 'lie', 'ahead', 'we', 'do', 'know', 'that', 'it', 'would', 'be', 'a', 'huge', 'mistak', 'to', 'bet', 'that', 'evolv', 'atmospher', 'chang', 'are', 'benign', 'in', 'their', 'implic', 'for', 'insur', 'dont', 'think', 'howev', 'that', 'we', 'have', 'lost', 'our', 'tast', 'for', 'risk', 'we', 'remain', 'prepar', 'to', 'lose', '6', 'billion', 'in', 'a', 'singl', 'event', 'ifw', 'have', 'been', 'paid', 'appropri', 'for', 'assum', 'that', 'risk', 'we', 'are', 'not', 'will', 'though', 'to', 'take', 'on', 'even', 'veri', 'small', 'exposur', 'at', 'price', 'that', 'dont', 'reflect', 'our', 'evalu', 'of', 'loss', 'probabl', 'appropri', 'price', 'dont', 'guarante', 'profit', 'in', 'ani', 'given', 'year', 'but', 'inappropri', 'price', 'most', 'certainli', 'guarante', 'eventu', 'loss', 'rate', 'have', 'recent', 'fallen', 'becaus', 'a', 'flood', 'of', 'capit', 'ha', 'enter', 'the', 'supercat', 'field', 'we', 'have', 'therefor', 'sharpli', 'reduc', 'our', 'wind', 'exposur', 'our', 'behavior', 'here', 'parallel', 'that', 'which', 'we', 'employ', 'in', 'financi', 'market', 'be', 'fear', 'when', 'other', 'are', 'greedi', 'and', 'be', 'greedi', 'when', 'other', 'are', 'fear', 'lloyd', 'equita', 'and', 'retroact', 'reinsur', 'last', 'year', 'we', 'are', 'get', 'now', 'to', 'equita', 'berkshir', 'agre', 'to', 'enter', 'into', 'a', 'huge', 'retroact', 'reinsur', 'contract', 'a', 'polici', 'that', 'protect', 'an', 'insur', 'against', 'loss', 'that', 'have', 'alreadi', 'happen', 'but', 'whose', 'cost', 'is', 'not', 'yet', 'known', 'ill', 'give', 'you', 'detail', 'of', 'the', 'agreement', 'shortli', 'but', 'let', 'first', 'take', 'a', 'journey', 'through', 'insur', 'histori', 'follow', 'the', 'rout', 'that', 'led', 'to', 'our', 'deal', 'our', 'tale', 'begin', 'around', '1688', 'when', 'edward', 'lloyd', 'open', 'a', 'small', 'coffe', 'hous', 'in', 'london', 'though', 'no', 'starbuck', 'hi', 'shop', 'wa', 'destin', 'to', 'achiev', 'worldwid', 'fame', 'becaus', 'of', 'the', 'commerci', 'activ', 'of', 'it', 'clientel', 'shipown', 'merchant', 'and', 'venturesom', 'british', 'capitalist', 'as', 'these', 'parti', 'sip', 'edward', 'brew', 'they', 'began', 'to', 'write', 'contract', 'transfer', 'the', 'risk', 'of', 'a', 'disast', 'at', 'sea', 'from', 'the', 'owner', 'of', 'ship', 'and', 'their', 'cargo', 'to', 'the', 'capitalist', 'who', 'wager', 'that', 'a', 'given', 'voyag', 'would', 'be', 'complet', 'without', 'incid', 'these', 'capitalist', 'eventu', 'becam', 'known', 'as', 'underwrit', 'at', 'lloyd', 'though', 'mani', 'peopl', 'believ', 'lloyd', 'to', 'be', 'an', 'insur', 'compani', 'that', 'is', 'not', 'the', 'case', 'it', 'is', 'instead', 'a', 'place', 'where', 'mani', 'memberinsur', 'transact', 'busi', 'just', 'as', 'they', 'did', 'centuri', 'ago', 'over', 'time', 'the', 'underwrit', 'solicit', 'passiv', 'investor', 'to', 'join', 'in', 'syndic', 'addit', 'the', 'busi', 'broaden', 'beyond', 'marin', 'risk', 'into', 'everi', 'imagin', 'form', 'of', 'insur', 'includ', 'exot', 'coverag', 'that', 'spread', 'the', 'fame', 'of', 'lloyd', 'far', 'and', 'wide', 'the', 'underwrit', 'left', 'the', 'coffe', 'hous', 'found', 'grander', 'quarter', 'and', 'formal', 'some', 'rule', 'of', 'associ', 'and', 'those', 'person', 'who', 'passiv', 'back', 'the', 'underwrit', 'becam', 'known', 'as', 'name', '8', 'eventu', 'the', 'name', 'came', 'to', 'includ', 'mani', 'thousand', 'of', 'peopl', 'from', 'around', 'the', 'world', 'who', 'join', 'expect', 'to', 'pick', 'up', 'some', 'extra', 'chang', 'without', 'effort', 'or', 'seriou', 'risk', 'true', 'prospect', 'name', 'were', 'alway', 'solemnli', 'told', 'that', 'they', 'would', 'have', 'unlimit', 'and', 'everlast', 'liabil', 'for', 'the', 'consequ', 'of', 'their', 'syndic', 'underwrit', 'down', 'to', 'the', 'last', 'cufflink', 'as', 'the', 'quaint', 'descript', 'went', 'but', 'that', 'warn', 'came', 'to', 'be', 'view', 'as', 'perfunctori', 'three', 'hundr', 'year', 'of', 'retain', 'cufflink', 'act', 'as', 'a', 'power', 'sed', 'to', 'the', 'name', 'pois', 'to', 'sign', 'up', 'then', 'came', 'asbesto', 'when', 'it', 'prospect', 'cost', 'were', 'ad', 'to', 'the', 'tidal', 'wave', 'of', 'environment', 'and', 'product', 'claim', 'that', 'surfac', 'in', 'the', '1980', 'lloyd', 'began', 'to', 'implod', 'polici', 'written', 'decad', 'earlier', 'and', 'larg', 'forgotten', 'about', 'were', 'develop', 'huge', 'loss', 'no', 'one', 'could', 'intellig', 'estim', 'their', 'total', 'but', 'it', 'wa', 'certain', 'to', 'be', 'mani', 'ten', 'of', 'billion', 'of', 'dollar', 'the', 'specter', 'of', 'unend', 'and', 'unlimit', 'loss', 'terrifi', 'exist', 'name', 'and', 'scare', 'away', 'prospect', 'mani', 'name', 'opt', 'for', 'bankruptci', 'some', 'even', 'chose', 'suicid', 'from', 'these', 'shambl', 'there', 'came', 'a', 'desper', 'effort', 'to', 'resuscit', 'lloyd', 'in', '1996', 'the', 'power', 'that', 'be', 'at', 'the', 'institut', 'allot', '111', 'billion', 'to', 'a', 'new', 'compani', 'equita', 'and', 'made', 'it', 'respons', 'for', 'pay', 'all', 'claim', 'on', 'polici', 'written', 'befor', '1993', 'in', 'effect', 'thi', 'plan', 'pool', 'the', 'miseri', 'of', 'the', 'mani', 'syndic', 'in', 'troubl', 'of', 'cours', 'the', 'money', 'allot', 'could', 'prove', 'to', 'be', 'insuffici', 'and', 'if', 'that', 'happen', 'the', 'name', 'remain', 'liabl', 'for', 'the', 'shortfal', 'but', 'the', 'new', 'plan', 'by', 'concentr', 'all', 'of', 'the', 'liabil', 'in', 'one', 'place', 'had', 'the', 'advantag', 'of', 'elimin', 'much', 'of', 'the', 'costli', 'intramur', 'squabbl', 'that', 'went', 'on', 'among', 'syndic', 'moreov', 'the', 'pool', 'allow', 'claim', 'evalu', 'negoti', 'and', 'litig', 'to', 'be', 'handl', 'more', 'intellig', 'than', 'had', 'been', 'the', 'case', 'previous', 'equita', 'embrac', 'ben', 'franklin', 'think', 'we', 'must', 'all', 'hang', 'togeth', 'or', 'assuredli', 'we', 'shall', 'hang', 'separ', 'from', 'the', 'start', 'mani', 'peopl', 'predict', 'equita', 'would', 'eventu', 'fail', 'but', 'as', 'ajit', 'and', 'i', 'review', 'the', 'fact', 'in', 'the', 'spring', 'of', '2006', '13', 'year', 'after', 'the', 'last', 'expos', 'polici', 'had', 'been', 'written', 'and', 'after', 'the', 'payment', 'of', '113', 'billion', 'in', 'claim', 'we', 'conclud', 'that', 'the', 'patient', 'wa', 'like', 'to', 'surviv', 'and', 'so', 'we', 'decid', 'to', 'offer', 'a', 'huge', 'reinsur', 'polici', 'to', 'equita', 'becaus', 'plenti', 'of', 'imponder', 'continu', 'to', 'exist', 'berkshir', 'could', 'not', 'provid', 'equita', 'and', 'it', '27972', 'name', 'unlimit', 'protect', 'but', 'we', 'said', 'and', 'im', 'simplifi', 'that', 'if', 'equita', 'would', 'give', 'us', '712', 'billion', 'in', 'cash', 'and', 'secur', 'thi', 'is', 'the', 'float', 'i', 'spoke', 'about', 'we', 'would', 'pay', 'all', 'of', 'it', 'futur', 'claim', 'and', 'expens', 'up', 'to', '139', 'billion', 'that', 'amount', 'wa', '57', 'billion', 'abov', 'what', 'equita', 'had', 'recent', 'guess', 'it', 'ultim', 'liabil', 'to', 'be', 'thu', 'the', 'name', 'receiv', 'a', 'huge', 'and', 'almost', 'certainli', 'suffici', 'amount', 'of', 'futur', 'protect', 'against', 'unpleas', 'surpris', 'inde', 'the', 'protect', 'is', 'so', 'larg', 'that', 'equita', 'plan', 'a', 'cash', 'payment', 'to', 'it', 'thousand', 'of', 'name', 'an', 'event', 'few', 'of', 'them', 'had', 'ever', 'dream', 'possibl', 'and', 'how', 'will', 'berkshir', 'fare', 'that', 'depend', 'on', 'how', 'much', 'known', 'claim', 'will', 'end', 'up', 'cost', 'us', 'how', 'mani', 'yettobepres', 'claim', 'will', 'surfac', 'and', 'what', 'they', 'will', 'cost', 'how', 'soon', 'claim', 'payment', 'will', 'be', 'made', 'and', 'how', 'much', 'we', 'earn', 'on', 'the', 'cash', 'we', 'receiv', 'befor', 'it', 'must', 'be', 'paid', 'out', 'ajit', 'and', 'i', 'think', 'the', 'odd', 'are', 'in', 'our', 'favor', 'and', 'should', 'we', 'be', 'wrong', 'berkshir', 'can', 'handl', 'it', 'scott', 'moser', 'the', 'ceo', 'of', 'equita', 'summar', 'the', 'transact', 'neatli', 'name', 'want', 'to', 'sleep', 'easi', 'at', 'night', 'and', 'we', 'think', 'weve', 'just', 'bought', 'them', 'the', 'world', 'best', 'mattress', 'warn', 'it', 'time', 'to', 'eat', 'your', 'broccoli', 'i', 'am', 'now', 'go', 'to', 'talk', 'about', 'account', 'matter', 'i', 'owe', 'thi', 'to', 'those', 'berkshir', 'sharehold', 'who', 'love', 'read', 'about', 'debit', 'and', 'credit', 'i', 'hope', 'both', 'of', 'you', 'find', 'thi', 'discuss', 'help', 'all', 'other', 'can', 'skip', 'thi', 'section', 'there', 'will', 'be', 'no', 'quiz', 'berkshir', 'ha', 'done', 'mani', 'retroact', 'transact', 'in', 'both', 'number', 'and', 'amount', 'a', 'multipl', 'of', 'such', 'polici', 'enter', 'into', 'by', 'ani', 'other', 'insur', 'we', 'are', 'the', 'reinsur', 'of', 'choic', 'for', 'these', 'coverag', 'becaus', 'the', 'oblig', 'that', 'are', 'transfer', 'to', 'us', 'for', 'exampl', 'lifetim', 'indemn', 'and', 'medic', 'payment', 'to', 'be', 'made', 'to', 'injur', 'worker', 'may', 'not', 'be', 'fulli', 'satisfi', 'for', '50', 'year', 'or', 'more', 'no', 'other', 'compani', 'can', 'offer', 'the', 'certainti', '9', 'that', 'berkshir', 'can', 'in', 'term', 'of', 'guarante', 'the', 'full', 'and', 'fair', 'settlement', 'of', 'these', 'oblig', 'thi', 'fact', 'is', 'import', 'to', 'the', 'origin', 'insur', 'policyhold', 'and', 'regul', 'the', 'account', 'procedur', 'for', 'retroact', 'transact', 'is', 'neither', 'well', 'known', 'nor', 'intuit', 'the', 'best', 'way', 'for', 'sharehold', 'to', 'understand', 'it', 'therefor', 'is', 'for', 'us', 'to', 'simpli', 'lay', 'out', 'the', 'debit', 'and', 'credit', 'charli', 'and', 'i', 'would', 'like', 'to', 'see', 'thi', 'done', 'more', 'often', 'we', 'sometim', 'encount', 'account', 'footnot', 'about', 'import', 'transact', 'that', 'leav', 'us', 'baffl', 'and', 'we', 'go', 'away', 'suspici', 'that', 'the', 'report', 'compani', 'wish', 'it', 'that', 'way', 'for', 'exampl', 'tri', 'comprehend', 'transact', 'describ', 'in', 'the', 'old', '10k', 'of', 'enron', 'even', 'after', 'you', 'know', 'how', 'the', 'movi', 'end', 'so', 'let', 'us', 'summar', 'our', 'account', 'for', 'the', 'equita', 'transact', 'the', 'major', 'debit', 'will', 'be', 'to', 'cash', 'and', 'invest', 'reinsur', 'recover', 'and', 'defer', 'charg', 'for', 'reinsur', 'assum', 'dcra', 'the', 'major', 'credit', 'will', 'be', 'to', 'reserv', 'for', 'loss', 'and', 'loss', 'adjust', 'expens', 'no', 'profit', 'or', 'loss', 'will', 'be', 'record', 'at', 'the', 'incept', 'of', 'the', 'transact', 'but', 'underwrit', 'loss', 'will', 'thereaft', 'be', 'incur', 'annual', 'as', 'the', 'dcra', 'asset', 'is', 'amort', 'downward', 'the', 'amount', 'of', 'the', 'annual', 'amort', 'charg', 'will', 'be', 'primarili', 'determin', 'by', 'how', 'our', 'endoftheyear', 'estim', 'as', 'to', 'the', 'time', 'and', 'amount', 'of', 'futur', 'loss', 'payment', 'compar', 'to', 'the', 'estim', 'made', 'at', 'the', 'begin', 'of', 'the', 'year', 'eventu', 'when', 'the', 'last', 'claim', 'ha', 'been', 'paid', 'the', 'dcra', 'account', 'will', 'be', 'reduc', 'to', 'zero', 'that', 'day', 'is', '50', 'year', 'or', 'more', 'away', 'what', 'import', 'to', 'rememb', 'is', 'that', 'retroact', 'insur', 'contract', 'alway', 'produc', 'underwrit', 'loss', 'for', 'us', 'whether', 'these', 'loss', 'are', 'worth', 'experienc', 'depend', 'on', 'whether', 'the', 'cash', 'we', 'have', 'receiv', 'produc', 'invest', 'incom', 'that', 'exce', 'the', 'loss', 'recent', 'our', 'dcra', 'charg', 'have', 'annual', 'deliv', '300', 'million', 'or', 'so', 'of', 'underwrit', 'loss', 'which', 'have', 'been', 'more', 'than', 'offset', 'by', 'the', 'incom', 'we', 'have', 'realiz', 'through', 'use', 'of', 'the', 'cash', 'we', 'receiv', 'as', 'a', 'premium', 'absent', 'new', 'retroact', 'contract', 'the', 'amount', 'of', 'the', 'annual', 'charg', 'would', 'normal', 'declin', 'over', 'time', 'after', 'the', 'equita', 'transact', 'howev', 'the', 'annual', 'dcra', 'cost', 'will', 'initi', 'increas', 'to', 'about', '450', 'million', 'a', 'year', 'thi', 'mean', 'that', 'our', 'other', 'insur', 'oper', 'must', 'gener', 'at', 'least', 'that', 'much', 'underwrit', 'gain', 'for', 'our', 'overal', 'float', 'to', 'be', 'costfre', 'that', 'amount', 'is', 'quit', 'a', 'hurdl', 'but', 'one', 'that', 'i', 'believ', 'we', 'will', 'clear', 'in', 'mani', 'if', 'not', 'most', 'year', 'arent', 'you', 'glad', 'that', 'i', 'promis', 'you', 'there', 'would', 'be', 'no', 'quiz', 'manufactur', 'servic', 'and', 'retail', 'oper', 'our', 'activ', 'in', 'thi', 'part', 'of', 'berkshir', 'cover', 'the', 'waterfront', 'let', 'look', 'though', 'at', 'a', 'summari', 'balanc', 'sheet', 'and', 'earn', 'statement', 'for', 'the', 'entir', 'group', 'balanc', 'sheet', '123106', 'in', 'million', 'asset', 'cash', 'and', 'equival', 'account', 'and', 'note', 'receiv', 'inventori', 'other', 'current', 'asset', 'total', 'current', 'asset', '1543', '3793', '5257', '363', '10956', 'liabil', 'and', 'equiti', 'note', 'payabl', '1468', 'other', 'current', 'liabil', '6635', 'total', 'current', 'liabil', '8103', 'goodwil', 'and', 'other', 'intang', '13314', 'defer', 'tax', '540', 'fix', 'asset', '8934', 'term', 'debt', 'and', 'other', 'liabil', '3014', 'other', 'asset', '1168', 'equiti', '22715', '34372', '34372', '10', 'earn', 'statement', 'in', 'million', '2006', '2005', '2004', 'revenu', '52660', '46896', '44142', 'oper', 'expens', 'includ', 'depreci', 'of', '823', 'in', '2006', '699', 'in', '2005', 'and', '676', 'in', '2004', '49002', '44190', '41604', 'interest', 'expens', '132', '83', '57', 'pretax', 'earn', '3526', '2623', '2481', 'incom', 'tax', 'and', 'minor', 'interest', '1395', '977', '941', 'net', 'incom', '2131', '1646', '1540', 'doe', 'not', 'includ', 'purchaseaccount', 'adjust', 'thi', 'motley', 'group', 'which', 'sell', 'product', 'rang', 'from', 'lollipop', 'to', 'motor', 'home', 'earn', 'a', 'pleas', '25', 'on', 'averag', 'tangibl', 'net', 'worth', 'last', 'year', 'it', 'noteworthi', 'also', 'that', 'these', 'oper', 'use', 'onli', 'minor', 'financi', 'leverag', 'in', 'achiev', 'that', 'return', 'clearli', 'we', 'own', 'some', 'terrif', 'busi', 'we', 'purchas', 'mani', 'of', 'them', 'howev', 'at', 'larg', 'premium', 'to', 'net', 'worth', 'a', 'point', 'reflect', 'in', 'the', 'goodwil', 'item', 'shown', 'on', 'the', 'balanc', 'sheet', 'and', 'that', 'fact', 'reduc', 'the', 'earn', 'on', 'our', 'averag', 'carri', 'valu', 'to', '108', 'here', 'are', 'a', 'few', 'newsworthi', 'item', 'about', 'compani', 'in', 'thi', 'sector', 'bob', 'shaw', 'a', 'remark', 'entrepreneur', 'who', 'from', 'a', 'stand', 'start', 'built', 'shaw', 'industri', 'into', 'the', 'countri', 'largest', 'carpet', 'produc', 'elect', 'last', 'year', 'at', 'age', '75', 'to', 'retir', 'to', 'succeed', 'him', 'bob', 'recommend', 'vanc', 'bell', 'a', '31year', 'veteran', 'at', 'shaw', 'and', 'bob', 'as', 'usual', 'made', 'the', 'right', 'call', 'weak', 'in', 'hous', 'ha', 'caus', 'the', 'carpet', 'busi', 'to', 'slow', 'shaw', 'howev', 'remain', 'a', 'powerhous', 'and', 'a', 'major', 'contributor', 'to', 'berkshir', 'earn', 'mitek', 'a', 'manufactur', 'of', 'connector', 'for', 'roof', 'truss', 'at', 'the', 'time', 'we', 'purchas', 'it', 'in', '2001', 'is', 'develop', 'into', 'a', 'miniconglomer', 'at', 'the', 'rate', 'it', 'is', 'grow', 'in', 'fact', 'mini', 'may', 'soon', 'be', 'inappropri', 'in', 'purchas', 'mitek', 'for', '420', 'million', 'we', 'lent', 'the', 'compani', '200', 'million', 'at', '9', 'and', 'bought', '198', 'million', 'of', 'stock', 'price', 'at', '10000', 'per', 'share', 'addit', '55', 'employe', 'bought', '2200', 'share', 'for', '22', 'million', 'each', 'employe', 'paid', 'exactli', 'the', 'same', 'price', 'that', 'we', 'did', 'in', 'most', 'case', 'borrow', 'money', 'to', 'do', 'so', 'and', 'are', 'they', 'ever', 'glad', 'they', 'did', 'five', 'year', 'later', 'mitek', 's', 'sale', 'have', 'tripl', 'and', 'the', 'stock', 'is', 'valu', 'at', '71699', 'per', 'share', 'despit', 'it', 'make', '14', 'acquisit', 'at', 'a', 'cost', 'of', '291', 'million', 'mitek', 'ha', 'paid', 'off', 'it', 'debt', 'to', 'berkshir', 'and', 'hold', '35', 'million', 'of', 'cash', 'we', 'celebr', 'the', 'fifth', 'anniversari', 'of', 'our', 'purchas', 'with', 'a', 'parti', 'in', 'juli', 'i', 'told', 'the', 'group', 'that', 'it', 'would', 'be', 'embarrass', 'if', 'mitek', 'stock', 'price', 'soar', 'beyond', 'that', 'of', 'berkshir', 'a', 'share', 'dont', 'be', 'surpris', 'howev', 'if', 'that', 'happen', 'though', 'charli', 'and', 'i', 'will', 'tri', 'to', 'make', 'our', 'share', 'a', 'move', 'target', 'not', 'all', 'of', 'our', 'busi', 'are', 'destin', 'to', 'increas', 'profit', 'when', 'an', 'industri', 'underli', 'econom', 'are', 'crumbl', 'talent', 'manag', 'may', 'slow', 'the', 'rate', 'of', 'declin', 'eventu', 'though', 'erod', 'fundament', 'will', 'overwhelm', 'manageri', 'brillianc', 'as', 'a', 'wise', 'friend', 'told', 'me', 'long', 'ago', 'if', 'you', 'want', 'to', 'get', 'a', 'reput', 'as', 'a', 'good', 'businessman', 'be', 'sure', 'to', 'get', 'into', 'a', 'good', 'busi', 'and', 'fundament', 'are', 'definit', 'erod', 'in', 'the', 'newspap', 'industri', 'a', 'trend', 'that', 'ha', 'caus', 'the', 'profit', 'of', 'our', 'buffalo', 'news', 'to', 'declin', 'the', 'skid', 'will', 'almost', 'certainli', 'continu', 'when', 'charli', 'and', 'i', 'were', 'young', 'the', 'newspap', 'busi', 'wa', 'as', 'easi', 'a', 'way', 'to', 'make', 'huge', 'return', 'as', 'exist', 'in', 'america', 'as', 'one', 'nottoobright', 'publish', 'famous', 'said', 'i', 'owe', 'my', 'fortun', 'to', 'two', 'great', 'american', 'institut', 'monopoli', 'and', 'nepot', 'no', 'paper', 'in', 'a', 'one', 'paper', 'citi', 'howev', 'bad', 'the', 'product', 'or', 'howev', 'inept', 'the', 'manag', 'could', 'avoid', 'gush', 'profit', 'the', 'industri', 'stagger', 'return', 'could', 'be', 'simpli', 'explain', 'for', 'most', 'of', 'the', '20', 'th', 'centuri', 'newspap', 'were', 'the', 'primari', 'sourc', 'of', 'inform', 'for', 'the', 'american', 'public', 'whether', 'the', 'subject', 'wa', 'sport', 'financ', 'or', 'polit', 'newspap', 'reign', 'suprem', 'just', 'as', 'import', 'their', 'ad', 'were', 'the', 'easiest', 'way', 'to', 'find', 'job', 'opportun', 'or', 'to', 'learn', 'the', 'price', 'of', 'groceri', 'at', 'your', 'town', 'supermarket', '11', 'the', 'great', 'major', 'of', 'famili', 'therefor', 'felt', 'the', 'need', 'for', 'a', 'paper', 'everi', 'day', 'but', 'understand', 'most', 'didnt', 'wish', 'to', 'pay', 'for', 'two', 'advertis', 'prefer', 'the', 'paper', 'with', 'the', 'most', 'circul', 'and', 'reader', 'tend', 'to', 'want', 'the', 'paper', 'with', 'the', 'most', 'ad', 'and', 'news', 'page', 'thi', 'circular', 'led', 'to', 'a', 'law', 'of', 'the', 'newspap', 'jungl', 'surviv', 'of', 'the', 'fattest', 'thu', 'when', 'two', 'or', 'more', 'paper', 'exist']\n",
      "\n",
      "***************************************************************************************************************************\n",
      "\n",
      "Prof's Model: berkshir hathaway inc to the sharehold of berkshir hathaway inc our gain in net worth dure 2006 wa 169 billion which increas the pershar book valu of both our class a and class b stock by 184 over the last 42 year that is sinc present manag took over book valu ha grown from 19 to 95453 a rate of 202 compound annual the highlight of 2010 wa our acquisit of burlington northern santa fe a purchas that work out even better than i expect it now appear that own thi railroad will increas berkshir normal earn power by nearli 40 pretax and by well over 30 aftertax make thi purchas increas our share count by 6 and use 22 billion of cash sinc weve quickli replenish the cash the sell in dividend from the privat or were about to their verifi incom oper expens if their own liquid will tell you an sharehold of b will have a manufactur that will not remain unchang to go to five charit foundat thu carri out part of my lifelong plan to eventu use all of my share for philanthrop purpos detail of the commit i made as well as the rational for them are post on our websit wwwberkshirehathawaycom tax i should note had noth to do with my decis or it time my feder and state incom tax in 2006 were exactli what they would have been had i not made my first contribut last summer and the same point will appli to my 2007 contribut in my will ive stipul that the proce from all berkshir share i still own at death are to be use for philanthrop purpos within ten year after my estat is close becaus my affair are not complic it should take three year at most for thi close to occur ad thi 13year period to my expect lifespan of about 12 year though natur im aim for more mean that proce from all of my berkshir share will like be distribut for societ purpos over the next 25 year or so ive set thi schedul becaus i want the money to be spent rel promptli by peopl i know to be capabl vigor and motiv these manageri attribut sometim wane as institut particularli those that are exempt from market forc age today there are terrif peopl in charg at the five foundat so at my death whi should they not move with dispatch to judici spend the money that remain 20 those peopl favor perpetu foundat argu that in the futur there will most certainli be larg and import societ problem that philanthropi will need to address i agre but there will then also be mani superrich individu and famili whose wealth will exceed that of today american and to whom philanthrop organ can make their case for fund these funder can then judg firsthand which oper have both the vital and the focu to best address the major societ problem that then exist in thi way a market test of idea and effect can be appli some organ will deserv major support while other will have outliv their use even if the peopl abov ground make their decis imperfectli they should be abl to alloc fund more ration than a deced six feet under will have ordain decad earlier will of cours can alway be rewritten but it veri unlik that my think will mortgag about 2 vi hour howev remain pretax than of such industri factor it appear i should be emphas or us have one have with unbeliev the hundr of earn from we deliv a larg other insur busi school teach it on smart offer it immedi began look thi thought slightli modifi seem appropri how as these price will be you may back on berkshir four criteria work hard of the annual meet we the annual meet thi year would be final decis are signific 13 show yearend thi invest more than had acquir stock today ibm with anyth close to our societ acquisit that counterproduct in 2009 our berkshir financ and after the the the acquisit they admir 101 cant buy oblig like to supplement more or by our four exposur your meet though it but took over our figur we are employ a dividend we now earn from which we own their futur result through 1978 have been restat to conform to the chang rule in all other respect the result are calcul use the number origin report the sp 500 number are pretax wherea the berkshir number are aftertax if a corpor such as berkshir were simpli to have own the sp 500 and accru the appropri tax it result would have lag the sp 500 in year when that index show a posit return but would have exceed the sp 500 in dividend as anyth advantag annual currenc for have ad at one must have readi so care a differ oper that were the lead author for cash the busi there wa billion increas thi crew 1 2 vi hour about becaus those ever not i will buyer befor in sever home in huge way but charli in million i treat mani and ash we receiv premium for variou though the director with which i have been requir in our report earn we reflect onli the dividend our portfolio compani pay us our share of the undistribut earn of these investe howev wa more than 2 billion last year these retain earn are import in our experi and manag the allstar of earn over those but last year i would have charg earn work 1993 we like serv as these geico will then abl invest project significantli last year and mani time in it will again be onli what they provid certainti geico these other will have the countri as a secur asset comer ceo must invest our but not own our present your meet credenti or a brokerag statement that show you are a berkshir holder enter with rhineston leav with diamond my daughter tell me that the more you buy the more you save kid say the darnedest thing on sunday in the mall outsid\n",
      "\n",
      "***************************************************************************************************************************\n",
      "\n",
      "Venky's Model: berkshir hathaway inc to the sharehold of berkshir hathaway inc our gain in net worth dure 2006 wa 169 billion which increas the pershar book valu of both our class a and class b stock by 198 over the last 45 year that is sinc present manag took over book valu ha grown from 19 to 84487 a rate of 203 compound annual berkshir recent acquisit of burlington northern santa fe bnsf ha ad at least 65000 sharehold to the 500000 or so alreadi on our book it import to charli munger my longtim partner and me that all of our owner understand berkshir oper goal limit and cultur in each annual report consequ we restat the econom principl that guid us thi year these principl appear on page 8994 and i urg all of you but particularli our new sharehold to read them berkshir ha adher to these principl for decad and will continu to do so long after im gone in thi letter we will also review some of the basic of our busi hope to provid both a freshman orient session for our bnsf newcom and a refresh cours for berkshir veteran how we measur ourselv our metric for evalu our manageri perform are display on the face page from the start charli and i have believ in have a ration and unbend standard for measur what we have or have not accomplish charli munger my live to do not befor that keep us from the temptat of see where the arrow of perform land and then paint the bull eye around it select the sp 500 as our bogey wa an easi choic becaus our sharehold at virtual no cost can match it perform by hold an index fund whi should they pay us for mere duplic that result a more difficult decis for us wa how to measur the progress of berkshir versu the sp there are good argument for simpli use the chang in our stock price over an extend period of time in fact that is the best test but yeartoyear market price can be extraordinarili errat even evalu cover as long as a decad can be greatli distort by foolishli high or low price at the outstand candid of board will see candi 8000 dairi queen 5 billion you money and audit on our websit simpli put float is money we hold that on price to get cost and the contract that our bid will be allow the uniniti from our could grew to about 800 million in group of option you wish to studi walter perform over our current valu capit expenditur or acquisit i didnt amount of famili we use of free money and better than we will came to us trade higher in part of their year our swiss re in addit brokerag oper we have never had dream possibl invest assumpt or invest great time for a oneyear gain or loss money too independ comp committe duti month and often fix decad from the luckiest moment prospect of all of which mention instead you els doe produc betterthanaverag collectnow paylat model leav us hold larg sum money we call float that will eventu go to zero histori precis balanc sheet and includ improv without the star ceo in 2004 49002 44190 41604 with 20300 agent though virtual all of the pc industri all of whose read from tonmil rail move 42 of america interc freight measur by tonmil and bnsf move more than ani other railroad about 28 of the industri total a littl math will tell you that more than 11 of all interc tonmil of freight in the us is transport by bnsf given the shift of popul to the west our share may well inch higher all of thi add up to a huge respons we find report or crew we were develop a major life of virtual all that it origin it wa close to three time and two point them in the year when you servic a sensibl percentag year of largest oper compani on ad in run exactli for intrins valu when i lose thi berkshir to cut the three test but i also get paid for the way you run your busi you origin no longer ceo when you get in the back seat of your car and it doesnt move at berkshir world headquart our annual rent is 270212 moreov the homeoffic invest in furnitur art coke dispens lunch room hightech equip you name it total 301363 as long as charli and i treat your money as if it were our own berkshir manag are like to be care with it as well our compens program our annual meet and even our annual report are all design with an eye to reinforc the berkshir cultur and make it one that will repel and expel manag of a differ bent thi cultur grow stronger everi year and it will remain intact long after charli and i have left the scene we will need all of the strength ive just describ to do reason well our manag will deliv you can count on that but whether charli and i can hold up our end in capit alloc depend in part on the competit environ for acquisit you will get our best effort geico now let me tell you a stori that will help you understand how the intrins valu of a busi can far exceed it book valu relat thi tale also give me a chanc to reliv some great memori sixti year ago last month geico enter my life destin to shape it in a huge way i wa then a 20yearold graduat student at columbia have elect to go there becaus my hero ben graham taught a onceaweek class at the school 8 one day at the librari i check out ben entri in who who who in america and found he wa chairman of govern employe insur co now call geico i knew noth of insur and had never heard of the compani the librarian howev steer me to a larg compendium of insur and after read the page on geico\n"
     ]
    }
   ],
   "source": [
    "# tokens=tokens.reshape(1,-1) #BxL\n",
    "tokens=tokens[0:32].reshape(1,-1) #BxL\n",
    "\n",
    "print(\"Input Tokens:\",input)\n",
    "print()\n",
    "print(\"***************************************************************************************************************************\")\n",
    "print()\n",
    "outputs = model.generate(context_token_ids=tokens,max_new_tokens=1000)\n",
    "print(\"Prof's Model:\",outputs)\n",
    "print()\n",
    "print(\"***************************************************************************************************************************\")\n",
    "print()\n",
    "venky_outputs= model_venky.generate(context_token_ids=tokens,max_new_tokens=1000)\n",
    "# outputs = model.generate(context_token_ids,max_new_tokens=1000)\n",
    "print(\"Venky's Model:\",venky_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most important thing to remember when investing is\n",
      "************************************************************************************************************************************************************************************************************************\n",
      "Input tokens: ['the', 'most', 'import', 'thing', 'to', 'rememb', 'when', 'invest', 'is']\n",
      "************************************************************************************************************************************************************************************************************************\n",
      "Prof. Model:  the most import thing to rememb when invest is much these long on a less lofti level sing a ceo to list for both prestig and busi that exactli what weve ad be extraordinarili rich in the movi we made last year berkshir onli becaus the combin though our origin loan but our custom satisfact midamerican acquir to at 2006 mark it to use all learn where i cant prove of cours expect that we will retain will tell you onli geico idea and at iscar way with the ceo lou and the market market market stock price when an opportun as executor to even as well that charli and i would like to meet you decid which i then 1991 254 153 101 1988 1992 256 158 98 1989 1993 244 145 99 1990 1994 186 87 99 1991 1995 256 165 no one have with what we do that we will almost certainli oper busi an individu who succeed hi offic i need to the sp we have arrang the sp 500 in year when that index show a posit return but would have exceed the sp 500 in year when the index show a neg return over the year the tax cost would have caus the aggreg lag to be substanti 2 berkshir hathaway inc to the sharehold of berkshir hathaway inc our gain in net worth dure 2009 wa 218 billion which increas the pershar book valu of both our class a and class b stock by 198 over the last 45 year that is sinc present manag took over book valu ha grown from 19 to 84487 a rate of 203 compound annual berkshir recent acquisit of burlington northern santa fe bnsf ha ad at least 65000 sharehold to the 500000 or so alreadi on our book it import to charli munger my longtim partner and me that all of our owner understand berkshir oper goal limit and cultur in each annual report consequ we restat the econom principl that guid us thi year these principl appear on page 8994 and i urg all of you but particularli our new sharehold to read them berkshir interest in support illustr the stock and to take the cours of our oper in 1910 american equal you work until becaus a ceo if chang significantli at the the size of our insur with our famili what that all of the other guy is do it to purchas you are debt hi inde the agre to buy the intim apparel busi midamerican ha happen at our corpor sector in ani geico will provid financi inform other opportun wa 110 of a lack of industri should put 1979 1983 316 173 143 gain 19642010 2011 that of the pursuit of mice and credit card im second largest i should have done far more big opportun come infrequ when it rain gold reach for a bucket not a thimbl we enter 2008 with 443 billion of cashequival and we have sinc retain oper earn of 17 billion nevertheless at yearend 2009 our cash wa down to 306 billion with 8 billion earmark for the bnsf acquisit weve put a lot of money to work dure the chao of the last two year it been an ideal period for investor a climat of fear is their best friend those who invest onli when comment are upbeat end up pay a heavi price for meaningless reassur in the end what count in invest is what you pay for a busi through the purchas of a small piec of it in the stock market and what that busi earn in the succeed decad or two last year i wrote extens about our deriv contract which were then the subject of both controversi and misunderstand for that discuss pleas go to wwwberkshirehathawaycom we have sinc chang onli a few of our posit some credit contract have run off the term of about 10 of our equiti put contract have also chang matur have been shorten and strike price materi reduc in these modif no money chang hand a few point from last year discuss are worth repeat 1 though it manag dave greg abel it mile discov evalu exactli i assign me explain mani comp committe duti and express 8007996634 in 1992 in 2011 2010 a materi three the custom of their syndic underwrit loss over a fleet to berkshir ha state part of you with these risk onli in mil 1970 claim payment of 2006 and piccolo giant root beer float in economi in resolv we go to be care is that local went to work at the wonder busi world ha the market easi to be mani intellig weve receiv though our bnsf car continu berkshir econom help john manvil insul mitek fasten carefre awn and nfm furnitur you will find that the home price at 139900 deliv excel valu last year a helper at the qwest bought one of two home on display well befor we open the door to sharehold flank the clayton home on the exhibit floor thi year will be an rv and pontoon boat from forest river geico will have a booth staf by a number of it top counselor from around the countri all of them readi to suppli you with auto insur quot in most case geico will be abl to give you a sharehold discount usual 8 thi special offer is permit by 44 of the 51 jurisdict in which we oper one supplement point the discount is not addit if you qualifi for anoth such as that given certain group bring the detail of your exist insur and check out whether we can save you money for at least 50 of you i believ we can and while your at it sign up for the new geico credit card it the one i now use sparingli of cours on saturday at the omaha airport we will have the usual array of aircraft from netjet avail for your inspect stop by the netjet booth\n",
      "************************************************************************************************************************************************************************************************************************\n",
      "Venky's Model:  the most import thing to rememb when invest is the point i detail of the demand bancorp equiti put togeth with ibmec up wall street pressur from the agenc forc and broker or simpli a refus by a testosteronedriven ceo to accept shrink volum ha led too mani insur to write busi at inadequ price the other guy is do it sometim these year the largest view offer limit achiev by the opportun to servic in run it wa inspir by the great prussian mathematician time is the right to call it industri or crew we alway know we will continu to go beyond marin on the level it convey 11 500 th edit of request inc 88 4330 3541 3947554 posco 52 768 2092 83128411 befor johnson johnson johnson 16 2749 2785 97214584 kraft food inc 56 3207 3063 19259600 munich re 105 2896 2924 3947555 posco 46 768 1706 72391036 the procter gambl compani 26 464 4657 25848838 sanofiaventi 20 2060 1656 242163773 tesco pic 30 1414 1608 78060769 us bancorp 41 2401 2105 39037142 walmart store inc 11 1893 2105 358936125 well fargo compani 68 8015 11123 other 3020 4956 total common stock carri at market 33733 61513 thi is our actual purchas price and also our tax basi gaap cost differ in a few case becaus of writeup or writedown that have been requir in addit we own posit in nontrad secur of dow chemic gener electr goldman sach swiss re and wrigley with an aggreg cost of 211 billion etc without offset thi posit in the futur of the same nine consecut year our float increas from 1295 in my friend the same period and say no matter colleg hi reason befor for the burden among the same firmli and were posit remain billion there is page thick on float for 12 billion includ assum debt and in decemb it agre to buy the intim apparel busi of vf corp dure when director berkshir histori tell them about sold hi wallet out most name came asbesto upon media inquiri too mani comp committe for newspap revenu in 2008 of great health it wa a lot of smaller compani stock salesman in cash natur we receiv in the compani year that the us certain sharehold commun do the bond in extrem manag howev is the independ by the independ comp committe member inde i were well as much hi offic jack up pay read or activ stem in earli in the last 18 lead supplier of small and best way out our exemplar own stock of combin overal return a for about 12 billion includ dividend of auto insur a tabl shown aggreg lag immedi knew we have instead a number of see a bit of page 9398 for our string are the market price of the chariti such polici charli and i had struggl be expect the journalist sometim my friend donna sheehan at berkshir and pay everhigh pay about 102 applic on expir home 2006 356 na pipelin 376 2006 356 na pipelin 376 309 homeservic 74 148 107 1972 anyth close to asid the open the 1990 74 393 million electr custom have warm we appli on sever of it 32 940 6427 229707000 tesco 29 1340 1820 31033800 us bancorp 18 969 1123 17072192 usg corp 190 536 936 19944300 walmart store inc 05 942 921 1727765 the washington post compani 180 11 1288 218169300 well fargo compani 65 3697 7758 1724200 white mountain insur 160 369 999 other 5866 8315 total common stock 22995 61533 thi is our actual purchas price and also our tax basi gaap cost differ in a few case becaus of writeup or writedown that have been requir in addit we own posit in nontrad secur of dow chemic gener electr goldman sach swiss re and wrigley with an aggreg cost of 211 billion and a carri valu of 260 billion we purchas these five posit in the last 18 month set asid the signific equiti potenti they provid us these hold deliv us an aggreg of 21 billion annual in dividend and interest final we own 76777029 share 225 of bnsf at yearend which we then carri at 8578 per share but which have subsequ been meld into our purchas of the entir compani in 2009 our largest sale were in conocophillip moodi procter gambl and johnson johnson sale of the latter occur after we had built our posit earlier in the year charli and i believ that all of these stock will like trade higher in the futur we made some sale earli in 2009 to rais cash for our dow and swiss re purchas and late in the year made other sale in anticip of the stake paid the sort that warn had been forc forward we pay for my successor as ceo i still do well see had job adjust and i have favorit dish at the thousand of midamerican ajit in resolv that sentenc while make no major compani were alway solemnli told that hurt us dure 201 1 a few year back i spent about 2 billion buy sever bond issu of energi futur hold deliv record sinc 1915 turn neg in 2006 foreign now earn more on their us invest than we do on our invest abroad in effect weve use up our bank account and turn to our credit card and like everyon who get in hock the us will now experi revers compound as we pay everincreas amount of interest on interest i want to emphas that even though our cours is unwis american will live better ten or twenti year from now than they do today percapita wealth will increas but our citizen will also be forc everi year for her israel year to arriv that we get now our geico s circumst here parallel of mani syndic underwrit profit on the net incom deriv contract most of the gift that situat joe tax on safeti and busi earn 3039 financ and intrins valu berkshir deriv\n"
     ]
    }
   ],
   "source": [
    "text =\"Best suggestion that warren buffet gave was\"\n",
    "text=\"Last year I told you that if you had a new son or grandson to be sure to name him\"\n",
    "text = \"In continuation of our investment philosophy, let's discuss\"\n",
    "text = \"In a world of short-term noise, Berkshire Hathaway believes in\"\n",
    "text=\"The most important thing to remember when investing is\"\n",
    "print(text)\n",
    "print(\"************************************************************************************************************************************************************************************************************************\")\n",
    "tokens,input=generate(text,vocab,\"cuda\")\n",
    "print(\"Input tokens:\", input)\n",
    "print(\"************************************************************************************************************************************************************************************************************************\")\n",
    "tokens=tokens.reshape(1,-1) #BxL\n",
    "outputs = model.generate(context_token_ids=tokens,max_new_tokens=1000)\n",
    "# outputs = model.generate(context_token_ids,max_new_tokens=1000)\n",
    "print(\"Prof. Model: \",outputs)\n",
    "\n",
    "print(\"************************************************************************************************************************************************************************************************************************\")\n",
    "\n",
    "# print(tokens.reshape(-1,1).shape)\n",
    "\n",
    "outputs = model_venky.generate(context_token_ids=tokens,max_new_tokens=1000)\n",
    "# outputs = model.generate(context_token_ids,max_new_tokens=1000)\n",
    "print(\"Venky's Model: \",outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discuss the most impressive text your model generated. What are the high impact design choices behind the generated text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input Text = \"The most important thing to remember when investing is\"**\n",
    "\n",
    "**Model Response** = \"the most import thing to rememb when invest is the point i detail of the demand bancorp equiti put togeth with ibmec up wall street pressur from the agenc forc and broker or simpli a refus by a testosteronedriven ceo to accept shrink volum ha led too mani insur to write busi at inadequ price the other guy is do it sometim these year the largest view offer limit achiev by the opportun\"\n",
    "\n",
    "\n",
    "**What could the output possibly mean ?**\n",
    "\n",
    "Ans. I asked the ChatGPT if it could understand the output words and rephrase them into meaningful sentences. It gave me this response \"The most important thing to remember when investing is the careful consideration of demand for Bancorp equities, combined with insights from IBMEC and awareness of Wall Street pressure exerted by agencies and brokers. Additionally, it's crucial not to succumb to pressure from a testosterone-driven CEO to accept reduced volumes, as this has led many insurers to write business at inadequate prices. Other investors have also faced challenges similar to this. Sometime this year, achieving the largest view offer limit will depend on seizing opportunities\"\n",
    "\n",
    "\n",
    "**Model Design Parameters**\n",
    "\n",
    "model_venky = TransformerBlockLM(batch_size=64, input_length=32, embed_size=128, sa_multihead_count=8, pos_embed=True, include_mlp=True)\n",
    "model_venky.fit(train_iters=20000, eval_iters=1000, lr=1e-4)\n",
    "\n",
    "The high impact design choices could be due positional encoding, skip connections, layer normalizations, use of multiple decoders, self-attention mechanism which understands relation of each word w.r.t the other words in the sentence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
